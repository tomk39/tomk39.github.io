<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2025-03-30T10:28:38+00:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Tomislav Kranjec’s Blog</title><subtitle>| Service Provider | Data Center | Cloud | Virtualization | SDN/NFV |
</subtitle><entry><title type="html">Designing with Cisco ACI</title><link href="http://0.0.0.0:4000/aci/designing-with-aci/" rel="alternate" type="text/html" title="Designing with Cisco ACI" /><published>2025-03-30T00:00:00+00:00</published><updated>2025-03-30T00:00:00+00:00</updated><id>http://0.0.0.0:4000/aci/designing-with-aci</id><content type="html" xml:base="http://0.0.0.0:4000/aci/designing-with-aci/"><![CDATA[<p align="center">
<img src="/images/designing_with_aci/1.gif" alt="ACI-top" title="ACI-top" /> 
</p>

<p>Welcome to “Designing with Cisco ACI,” where we will dive into the Cisco ACI principals of creating robust, scalable, and efficient network architectures using Cisco’s Application Centric Infrastructure (ACI).
<!-- excerpt --></p>

<p>
In today’s rapidly evolving IT landscape, the ability to design flexible and resilient networks is more critical than ever. This blog will provide you with ideas on what you can do with Cisco ACI fabric, covering various use cases and key design points. It will be especially valuable for those preparing to deploy Cisco ACI and who are still unsure which design to choose.
The following Cisco ACI design types will be the topics of discussion:  
</p>
<body id="top">  
</body>

<p><strong><a href="#section1">1. Cisco ACI Traditional Single Pod.</a></strong>  <br />
<strong><a href="#section2">2. Cisco ACI Remote leaf.</a></strong><br />
<strong><a href="#section3">3. Cisco ACI Mini.</a></strong><br />
<strong><a href="#section4">4. Cisco ACI Multi-Pod Interconnect via IPN.</a></strong><br />
<strong><a href="#section5">5. Cisco ACI Multi-Pod Interconnect Spine back to back.</a></strong><br />
<strong><a href="#section6">6. Cisco ACI Multi-Site.</a></strong><br />
<strong><a href="#section7">7. Multi-Pod - Multi-Site hybrid deployment.</a></strong></p>

<p><strong>So let’s start!</strong></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section1">1. Cisco ACI Traditional Single Pod.</span></strong></p>
<p align="center">
<img src="/images/designing_with_aci/single-pod.png" alt="ACI-SinglePod" title="ACI-SinglePod" /> 
</p>
<p>Figure 1. Cisco ACI Single Pod Fabric.</p>

<p><strong>Main Characteristics:</strong></p>

<ul>
  <li><strong>Architecture:</strong> Consists of a single APIC (Application Policy Infrastructure Controller) cluster managing one ACI fabric.</li>
  <li><strong>Management:</strong> Centralized management with a single point of control.</li>
  <li><strong>Scalability:</strong> Limited to the capacity of one fabric.</li>
  <li><strong>Latency:</strong> Lower latency within the single fabric.</li>
  <li><strong>Failure Domain:</strong> Single failure domain, meaning any issue can affect the entire fabric.</li>
</ul>

<p><strong>Key Differences between Cisco ACI Single and Multi Pod Fabric:</strong></p>
<ul>
  <li><strong>Scalability:</strong> Multi-Pod offers greater scalability by allowing multiple Pods to be managed under one APIC cluster.</li>
  <li><strong>Resiliency:</strong> Multi-Pod improves resiliency by isolating failure domains.</li>
  <li><strong>Latency:</strong> Single Pod has lower latency within the fabric, while Multi-Pod may have slightly higher latency due to inter-Pod communication</li>
</ul>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt" id="section2">2. Cisco ACI Remote leaf.</span></strong></p>
<p align="center">
<img src="/images/designing_with_aci/remote-leaf.png" alt="ACI-Remote-leaf" title="ACI-Remote-leaf" /> 
</p>
<p>Figure 2. Cisco ACI Extending fabric with remote leaf to remote location.</p>

<p><strong>Main Characteristics:</strong></p>
<ul>
  <li><strong>VxLAN Tunnels:</strong> Unicast traffic is forwarded through VXLAN tunnels over Layer 3. This ensures that traffic between the remote leaf and the main fabric is encapsulated and securely transmitted.</li>
  <li><strong>Head End Replication (HER):</strong> For Layer 2 broadcast, unknown unicast, and multicast (BUM) traffic, Head End Replication (HER) tunnels are used. This method avoids the need for multicast in the WAN.</li>
  <li><strong>Policy Consistency:</strong> All policies defined in the main data center are consistently applied to the remote leaf switches, ensuring uniform policy enforcement across the entire fabric.</li>
  <li><strong>Local Switching:</strong> Traffic between endpoints connected to the remote leaf is switched locally, while traffic requiring spine proxy services is forwarded to the main fabric.</li>
</ul>

<p><strong>Inter-Pod Network Requirements for Remote Leaf deployment:</strong></p>
<ul>
  <li>L3 network should support 50 Bytes overhead for VxLAN if we have default MTU 1500 Bytes.</li>
  <li>The latency between the ACI main DC and the remote Leaf should be less than 300 msec Round Trip Time (RTT)</li>
  <li>A minimum of 100 Mbps bandwidth in the IP Network. Starting from Cisco ACI Release 4.2.4, the requirement has been lowered to 10 Mbps of bandwidth.</li>
  <li>The remote leaf solution requires the /32 Tunnel End Point (TEP) IP addresses of the remote leaf switches and main data center leaf/spine switches to be advertised across the main data center and remote leaf switches without summarization.</li>
</ul>

<p><strong>Use Cases:</strong><br />
<strong>Satellite/Co-Location Data Centers:</strong>  <br />
Enterprises or service providers often have a main data center and several smaller satellite data centers distributed across multiple locations.<br />
<strong>Disaster Recovery Sites:</strong>  <br />
Organizations need to maintain disaster recovery sites to ensure business continuity in case of a failure at the primary site.<br />
<strong>Branch Offices:</strong><br />
Companies with multiple branch offices require consistent network policies and centralized management.<br />
<strong>Edge Computing:</strong><br />
With the rise of edge computing, data processing is moved closer to the source of data generation example telco 5G.</p>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt" id="section3">3. Cisco ACI Mini.</span></strong></p>
<p align="center">
<img src="/images/designing_with_aci/aci-mini.png" alt="ACI-Remote-leaf" title="ACI-Remote-leaf" /> 
</p>
<p>Figure 3. Cisco ACI mini - connecting virtual APICs to fabric.</p>

<p><strong>Main Characteristics</strong></p>
<ul>
  <li><strong>APIC Cluster:</strong>
Consists of one physical APIC and two virtual APICs (vAPICs) running in virtual machines.
Reduces the physical footprint and cost of the APIC cluster, making it suitable for environments with limited rack space or budget.</li>
  <li><strong>Deployment Scenarios:</strong>
Ideal for colocation facilities, single-room data centers, or any scenario where full-scale ACI installations are impractical.</li>
  <li><strong>Management and Policy:</strong>
Provides centralized management and policy enforcement similar to full-scale ACI deployments.
Does not support features like Multi-Pod, Virtual Pod, or Remote Leaf switches.</li>
  <li><strong>Integration:</strong>
Multi-Site Support: Can be integrated with Cisco ACI Multi-Site for broader deployment scenarios.</li>
  <li><strong>Time Synchronization:</strong>
Requires the physical APIC and the ESXi server hosting the virtual APICs to be time-synced with the same NTP server to avoid synchronization issues.</li>
</ul>

<p><strong>Use Cases:</strong></p>
<ul>
  <li><strong>Colocation Facilities:</strong>
Businesses that use colocation facilities to host their IT infrastructure.</li>
  <li><strong>Single-Room Data Centers:</strong>
Small businesses or branch offices with a single-room data center.</li>
  <li><strong>Development and Test Environments:</strong>
Organizations that need a dedicated environment for development and testing.</li>
  <li><strong>Edge Computing:</strong>
Deployments at the edge of the network, closer to data sources.</li>
</ul>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt" id="section4">4. Cisco ACI Multi-Pod Interconnect via IPN.</span></strong></p>
<p align="center">
<img src="/images/designing_with_aci/multi-pod.png" alt="ACI-MultiPod" title="ACI-MultiPod" /> 
</p>
<p>Figure 4. Cisco ACI Multi-Pod Interconnected via Inter-Pod Network.</p>

<p><strong>Main Characteristics</strong></p>
<ul>
  <li><strong>Centralized Management:</strong>
Managed by a single APIC cluster, providing a unified management and policy domain across all Pods1.</li>
  <li><strong>Fault Isolation:</strong>
Control plane protocols (e.g., IS-IS, COOP, MP-BGP) operate independently within each Pod, enhancing fault isolation.</li>
  <li><strong>Scalability:</strong>
Supports the interconnection of multiple Pods, allowing for greater scalability and flexibility in data center design.</li>
  <li><strong>Inter-Pod Network (IPN):</strong>
Pods are connected via a Layer 3 Inter-Pod Network (IPN), which uses VXLAN encapsulation for data plane traffic and MP-BGP EVPN for control plane communication.</li>
  <li><strong>End-to-End Policy Enforcement:</strong>
Ensures consistent policy enforcement across all Pods, maintaining uniform security and network policies.</li>
  <li><strong>Resiliency:</strong>
Isolated failure domains within each Pod reduce the impact of failures, enhancing overall resiliency.</li>
  <li><strong>Geographical Flexibility:</strong>
Pods can be located in different physical data centers, including geographically dispersed locations, with up to 50 ms RTT latency.</li>
</ul>

<p><strong>Inter-Pod Network (IPN) Requirements:</strong></p>
<ul>
  <li><strong>OSPF:</strong> Required for routing between Pods.</li>
  <li><strong>PIM Bi-Dir:</strong> Used for handling unknown unicast, multicast, and broadcast traffic.</li>
  <li><strong>MTU Size:</strong> The IPN must support jumbo frames with an MTU of 9150 to accommodate VXLAN encapsulated traffic.
In newer APIC version there is possibility to perform tunning of the MTU size of all the control plane packets generated by ACI nodes (leaf and spines), including inter-Pod MP-BGP control plane traffic.</li>
  <li><strong>DHCP Relay Functionality:</strong> Needed to allow Spine and Leaf nodes to be discovered across the IPN.</li>
  <li><strong>Prioritization:</strong> Ensures that APIC-to-APIC communication is prioritized over the IPN.</li>
  <li><strong>Interface Support:</strong> 10/40/100G</li>
  <li><strong>VLAN 4:</strong> Used for 802.1Q encapsulation between the spine and IPN devices.</li>
  <li><strong>VRF Context:</strong> Recommended to isolate IPN traffic from other services, ensuring stable IPN connectivity.</li>
</ul>

<p><strong>Use Cases:</strong></p>
<ul>
  <li><strong>Disaster Recovery and Business Continuity:</strong>
Organizations need to ensure business continuity and disaster recovery across multiple data centers.</li>
  <li><strong>Geographically Dispersed Data Centers:</strong>
Enterprises with data centers in different geographical locations require a unified network fabric.</li>
  <li><strong>Active/Active Data Centers:</strong>
Businesses need to deploy applications across multiple active data centers for load balancing and high availability.</li>
  <li><strong>Scalability and Flexibility:</strong>
Large enterprises or service providers need to scale their data center infrastructure.</li>
</ul>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt" id="section5">5. Cisco ACI Multi-Pod Interconnect Spine back to back.</span></strong></p>
<p align="center">
<img src="/images/designing_with_aci/multi-pod-b2b.png" alt="ACI-MultiPod" title="ACI-MultiPod" /> 
</p>
<p>Figure 5. Cisco ACI Multi-Pod, pods connected directly via Spines.</p>

<p>The characteristics and use cases for Cisco ACI Multi-Pod with spine back-to-back interconnect are essentially the same as those for Cisco ACI Multi-Pod interconnected via an Inter-Pod Network (IPN). The primary distinction is the absence of an IPN. However, it is crucial to ensure direct connectivity between remote spines.</p>

<p><strong>Advantages:</strong></p>
<ul>
  <li><strong>Simplified Network Design:</strong>
Eliminates the need for an Inter-Pod Network (IPN), simplifying the overall network design and reducing complexity.</li>
  <li><strong>Reduced Latency:</strong><br />
Provides direct connectivity between remote spines, which can reduce latency compared to traversing an IPN.</li>
  <li><strong>Cost Efficiency:</strong><br />
Reduces the need for additional IPN devices and infrastructure, leading to cost savings.</li>
  <li><strong>Improved Performance:</strong><br />
Direct spine-to-spine connections can optimize traffic flow and improve overall network performance.</li>
  <li><strong>Enhanced Resiliency:</strong><br />
Offers redundant paths between Pods, enhancing network resiliency and fault tolerance.</li>
</ul>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt" id="section6">6. Cisco ACI Multi-Site.</span></strong></p>
<p align="center">
<img src="/images/designing_with_aci/multi-site.png" alt="ACI-MultiSite" title="ACI-MultiSite" /> 
</p>
<p>Figure 6. Cisco ACI Multi-Site managed by Nexus Dashboard Orchestrator.</p>

<p><strong>Key Differences between Multi Site and Multi Pod deployment:</strong></p>
<ul>
  <li><strong>Management:</strong> Multi-Pod uses a single APIC cluster, while Multi-Site uses multiple APIC clusters managed by NDO.</li>
  <li><strong>Latency:</strong> Multi-Pod typically has lower latency between pods, whereas Multi-Site can handle higher latency across sites.</li>
  <li><strong>Scalability:</strong> Multi-Site is better suited for large-scale, geographically dispersed deployments.</li>
</ul>

<p><strong>Main Characteristics:</strong></p>
<ul>
  <li><strong>Geographical Flexibility:</strong>
Supports the interconnection of geographically dispersed data centers, extending Layer 2 and Layer 3 connectivity between sites.</li>
  <li><strong>Unified Policy Management:</strong>
Utilizes the Nexus Dashboard Orchestrator (formerly Multi-Site Orchestrator) to manage and enforce policies across multiple sites.</li>
  <li><strong>Fault Isolation:</strong>
Each site operates with its own APIC cluster, ensuring fault isolation and independent control planes.</li>
  <li><strong>Scalability:</strong>
Allows for the integration of multiple ACI fabrics, providing scalability for large and distributed environments.</li>
  <li><strong>Consistent Policy Enforcement:</strong>
Ensures consistent policy enforcement across all interconnected sites, maintaining uniform security and network policies.</li>
  <li><strong>Data Plane Communication:</strong>
VXLAN and MP-BGP EVPN: Uses VXLAN for data-plane communication and MP-BGP EVPN for control-plane communication between sites.</li>
</ul>

<p><strong>Inter-Site Network (ISN) Requirements:</strong>
ACI fabrics are connected via ISN routed network which will be used for VxLAN communications between endpoints which they belong to different fabrics.
I would like to highlight that there is no latency limit between the different ACI fabrics part of the same Multi-Site domain.<br />
The only latency considerations are:</p>

<ul>
  <li>Up to 150 ms, RTT is the latency supported between the Nexus Dashboard cluster nodes where the Orchestrator service is enabled.</li>
  <li>Up to 500 msec RTT is the latency between each Nexus Dashboard Orchestrator node and the APIC controller nodes that are added to the Multi-Site domain. This means that the Multi-Site architecture has been designed from the ground up to be able to manage ACI fabrics that can be geographically dispersed around the world.</li>
  <li>No Need for Multicast.</li>
  <li>MTU between ISN routers and spines should be increases to minimum 1600 Bytes, in this case should be changed default MTU size 9000 Bytes in global configuration.</li>
  <li>IP connectivity can be achieved using OSPF, BGP or static routing.</li>
</ul>

<p><strong>Use Cases:</strong><br />
<strong>Centralized Data Center with Separate Availability Zones:</strong><br />
Allows for the creation of distinct availability zones within a centralized data center, enhancing fault isolation and resource management.<br />
<strong>Disaster Recovery:</strong><br />
Facilitates disaster recovery by enabling data replication and failover between sites, ensuring business continuity in case of site failures.<br />
<strong>Geographically Distributed Data Centers:</strong><br />
Supports the management of data centers spread across different cities, countries, or continents from a single pane of glass, simplifying provisioning, monitoring, and management.<br />
<strong>Stretched Bridge Domain with Layer 2 Broadcast Extension:</strong><br />
Extends Layer 2 domains across sites, allowing for live VM migration and active/active high availability.<br />
<strong>Stretched VRF with Inter-Site Contracts:</strong><br />
Enables consistent policy enforcement across sites by stretching VRFs and applying inter-site contracts.<br />
<strong>Shared Services with Stretched Provider EPGs:</strong><br />
Allows shared services to be accessed across multiple sites using stretched Endpoint Groups (EPGs).</p>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt" id="section7">7. Multi-Pod - Multi-Site hybrid deployment.</span></strong></p>

<p><a href="/images/designing_with_aci/multi-pod-multi-site.png" target="new" title="click here to see the full sized image"><img src="/images/designing_with_aci/multi-pod-multi-site.png" alt="your alt description here" /></a>
Figure 7. Extending Cisco ACI Multi-Pod to Multi-Site.</p>

<p>Using Cisco ACI Multi-Site and Multi-Pod as a single fabric offers several benefits, enhancing the overall network architecture. Here are some key advantages:</p>

<ul>
  <li><strong>Centralized Management:</strong><br />
Both Multi-Site and Multi-Pod architectures allow for centralized management through a single APIC cluster, simplifying policy enforcement and configuration.</li>
  <li><strong>Scalability:</strong><br />
Multi-Pod enables the expansion of a single ACI fabric across multiple locations without the need for separate fabrics, making it easier to scale the network.<br />
Multi-Site allows for the interconnection of multiple ACI fabrics, providing scalability across geographically dispersed data centers.</li>
  <li><strong>Resiliency and Fault Isolation:</strong><br />
Multi-Pod architecture offers full resiliency at the network level across pods while maintaining a single fabric, ensuring minimal impact from failures.<br />
Multi-Site architecture isolates failure domains between sites, enhancing overall network resiliency.</li>
  <li><strong>Disaster Recovery and Business Continuity:</strong><br />
Both architectures support disaster recovery by enabling data replication and failover between sites or pods, ensuring business continuity in case of site failures.</li>
  <li><strong>Consistent Policy Enforcement:</strong> 
Policies and configurations can be consistently applied across all sites and pods, ensuring uniform security and compliance.</li>
  <li><strong>Operational Efficiency:</strong>  <br />
Centralized management and automation reduce administrative overhead and operational complexity, making it easier to manage large-scale networks.</li>
</ul>

<p><a href="#top">Back to Top</a></p>]]></content><author><name></name></author><category term="ACI" /><summary type="html"><![CDATA[Welcome to “Designing with Cisco ACI,” where we will dive into the Cisco ACI principals of creating robust, scalable, and efficient network architectures using Cisco’s Application Centric Infrastructure (ACI).]]></summary></entry><entry><title type="html">Unlocking the Power of Cisco ACI 6.0</title><link href="http://0.0.0.0:4000/aci/aci-6-new-features/" rel="alternate" type="text/html" title="Unlocking the Power of Cisco ACI 6.0" /><published>2024-09-27T00:00:00+00:00</published><updated>2024-09-27T00:00:00+00:00</updated><id>http://0.0.0.0:4000/aci/aci-6-new-features</id><content type="html" xml:base="http://0.0.0.0:4000/aci/aci-6-new-features/"><![CDATA[<p align="center">
<img src="/images/aci_6_new_features/1.png" alt="ACI_60" title="ACI_60" />   
</p>
<p style="text-align: justify;">
The Cisco ACI 6.0 release has been available for a couple of months now. Let’s check out what it brings to network engineers and IT professionals. Experience a new era of enhanced performance, security, and automation capabilities. In this blog, we’ll dive into the key enhancements of Cisco ACI 6.0 and explore how they can empower your network infrastructure.
</p>

<p><strong><span style="color:#074080">Main three features and enhancements in Cisco ACI 6.0.2 and 6.0.3</span></strong></p>

<ol>
  <li>Virtual APIC on VMware ESXi and Cloud</li>
  <li>VMM Integration with Nutanix AHV</li>
  <li>Nexus Dashboard - Connectivity analysis</li>
</ol>

<p><strong><span style="color:#074080">1. Virtual APIC  6.0(2)F</span></strong></p>

<ul>
  <li>No physical APICs.</li>
  <li>No configuration limitations you can administer it like a regular APICs.</li>
  <li>Scale is pretty much same as for physical APIC.</li>
  <li>Limitations not able to run applications such as ELAM, policy explorer.</li>
  <li>APICs can be connected to leaf switches directly.</li>
  <li>Or they can be connected remotely via L3 network.
During the initial setup, the wizard will ask you to choose how the APIC is connected to the network. Figure 1 below is an example.</li>
  <li>Virtual APIC Cluster Fully functional APIC that can manage up to 200 switches.</li>
  <li>6.0(2)F Cloud hosted APIC Cluster can only be deployed within AWS Cloud.</li>
</ul>
<p align="center">
<img src="/images/aci_6_new_features/2.png" alt="ACI 60" title="ACI 60" /> 
</p>
<p><strong>Figure 1</strong> ACI setup wizard</p>

<p><strong><span style="color:#074080">2. Integration with Nutanix AHV hypervisor  6.0(3)F</span></strong></p>
<ul>
  <li>APICs connect and authenticate to Prism Manager.</li>
  <li>Integration of Nutanix Acropolis Hypervisor (AHV) with Cisco Application Centric Infrastructure (ACI).</li>
  <li>This integration provides virtual and physical network automation and VM endpoints visibility in Cisco ACI.</li>
  <li>End to end network automation and end to end visibility.</li>
</ul>

<p><strong><span style="color:#074080">3. Nexus Dashboard - Connectivity Analysis 6.0(2)F</span></strong></p>

<ul>
  <li>Troubleshooting between two End points, src and dst IP addresses or src and dst MAC addresses.</li>
  <li>Visibility of what happening between control and data plane.</li>
  <li>Verification of control and data plane.</li>
  <li>Traces all possible forwarding paths between source and destinations.</li>
  <li>Identifies if there are problems in the path and what is causing issues.</li>
  <li>Helps to highlight real root cause, perform forwarding checks.</li>
</ul>

<p><strong><span style="color:#074080">4.Interface policy - simplify configuration</span></strong></p>
<ul>
  <li>This type of configuration is optional there is no need to change if you already master polices configuration. On that way you will keep everything under your control.</li>
  <li>This is more for new guys which is starting to use ACI Fabric.</li>
  <li>Guys with experience will not have so many benefits from this feature.</li>
  <li>It is compatible with previous manual configuration of polices.</li>
  <li>Not recommended for brown field deployments, I would like to recommend only for green field deployment.</li>
</ul>

<p><strong><span style="color:#074080">5.Weight-Based Symmetric PBR</span></strong></p>

<p>In 6.0 it was introduced Weight-based PBR it means that we can send more traffic to specific node.
Use case:
•	On specific node we have connected Load Balancer.
•	On specific node we have more powerful firewall in the group.
•	Introducing new firewall, we can redirect small amount of traffic to new firewall for test purposes.</p>

<p><strong><span style="color:#074080">6.Stretched L3out SVI across remote leaves.</span></strong></p>

<ul>
  <li>Supported for stretching a non-vPC L3Out SVI across remote leaf switches
Use cases:</li>
  <li>Mobile packet core.</li>
  <li>Containerized workloads special at the Edge.</li>
  <li>Cloud native network functions CNF.</li>
</ul>

<p><strong><span style="color:#074080">7. Multi-Site support for vzAny PBR and L3Out-to-L3Out PBR use cases.</span></strong>
From ACI 6.0(4)F and NDO 4.2(3) following uses cases are supported:</p>
<ul>
  <li>vzAny-to-vzAny (Intra VRF).</li>
  <li>vzAny-to-EPG (Intra VRF).</li>
  <li>vzAny-to-L3out (Intra VRF).</li>
  <li>L3out-to-L3out ((Intra VRF) and inter-VRF).</li>
  <li>Standard EPG-to-EPG subnet under consumer still is required.
If you have plan to implement vzAny inside multi-site design, please check all details on link below:
<a href="https://www.cisco.com/c/en/us/td/docs/dcn/ndo/4x/configuration/cisco-nexus-dashboard-orchestrator-configuration-guide-aci-421/ndo-configuration-aci-use-case-vzany-pbr-42x.html" target="_blank">Cisco Nexus Dashboard Orchestrator Configuration Guide for ACI Fabrics</a>.</li>
</ul>

<p><strong><span style="color:#074080">8. Main Routing improvements.ulti-Site support for vzAny PBR and L3Out-to-L3Out PBR use cases.</span></strong></p>

<ul>
  <li>BGP AS - Remove private AS from AS_PATH.</li>
  <li>Support for BFD on Secondary subnet IPv4/IPv6.</li>
  <li>BGP Additional Paths - Receive multiple paths for the same prefix without the new paths replacing any previous paths.</li>
</ul>

<p><strong><span style="color:#074080">9. Operating and Administering improvements.</span></strong></p>

<p>ACI ver.6.0(1)F</p>
<ul>
  <li>Simplifying interface configuration.</li>
  <li>Secure Erase (RMA).</li>
</ul>

<p>ACI ver.6.0(2)F</p>
<ul>
  <li>Auto firmware update for Cisco APIC on discovery - APIC will be automatically upgrade on version same as fabric.</li>
  <li>Installing switch SMU without reload.</li>
  <li>Troubleshooting of QoS Polices.</li>
</ul>

<p>ACI ver.6.0(3)F</p>
<ul>
  <li>Reduce of time of APIC upgrade.</li>
  <li>Transaction-based Logging.</li>
  <li>Memory-based Automatic Switch Image Selection 32/64 bit.</li>
</ul>

<p>ACI ver.6.0(4)F</p>
<ul>
  <li>Streamlined navigation and editing of the stat collection threshold from a TCA fault.</li>
</ul>

<p><strong><span style="color:#074080">10. Main Security improvements</span></strong></p>

<ul>
  <li>Cisco Nexus 9K switch secure erase.</li>
  <li>Support for a user group map rule for SAML and OAuth2.</li>
  <li>Support for TLS ver 1.3</li>
  <li>First hop security FHS support for VMM.</li>
  <li>TACACS external logging for switches.</li>
  <li>RSA Key 307 and 4096 support on X.509 certificates.</li>
</ul>

<p><strong><span style="color:#074080">11. Integrations.</span></strong></p>

<ul>
  <li>Enhanced LLDP TLVs for Azure Stack HCI.</li>
  <li>Policy mode for Cisco ACI and VMware NSX-T integration.</li>
  <li>VMM Domain support for VMware vSphere 8.0.</li>
</ul>

<p><strong><span style="color:#074080">12. Scale improvements.</span></strong></p>

<p align="center">
<img src="/images/aci_6_new_features/3.png" alt="ACI 60" title="ACI 60" /> 
</p>

<p align="center">
<img src="/images/aci_6_new_features/4.png" alt="ACI 60" title="ACI 60" /> 
</p>

<p align="center">
<img src="/images/aci_6_new_features/5.png" alt="ACI 60" title="ACI 60" /> 
</p>

<p>References: <br />
<a href="https://www.cisco.com/c/en/us/td/docs/dcn/aci/apic/6x/release-notes/cisco-apic-release-notes-611.html" target="_blank">Cisco Application Policy Infrastructure Controller</a> <br />
<a href="https://www.cisco.com/c/en/us/td/docs/dcn/aci/apic/all/cisco-aci-releases-changes-in-behavior.html" target="_blank">Cisco ACI Releases Changes in Behavior</a></p>]]></content><author><name></name></author><category term="ACI" /><summary type="html"><![CDATA[The Cisco ACI 6.0 release has been available for a couple of months now. Let’s check out what it brings to network engineers and IT professionals. Experience a new era of enhanced performance, security, and automation capabilities. In this blog, we’ll dive into the key enhancements of Cisco ACI 6.0 and explore how they can empower your network infrastructure.]]></summary></entry><entry><title type="html">Mastering Cisco ACI Physical Design</title><link href="http://0.0.0.0:4000/aci/physical-design/" rel="alternate" type="text/html" title="Mastering Cisco ACI Physical Design" /><published>2024-07-07T00:00:00+00:00</published><updated>2024-07-07T00:00:00+00:00</updated><id>http://0.0.0.0:4000/aci/physical-design</id><content type="html" xml:base="http://0.0.0.0:4000/aci/physical-design/"><![CDATA[<p align="center">
<img src="/images/phy_design/2.png" alt="Switch SFP Cables" title="ACI Phy Design" /> 
</p>
<p style="text-align: justify;">
Let's dive into the world of Cisco ACI physical design. After all, we all know that without hardware, there is no cloud or networking.
</p>

<p style="text-align: justify;">
Dive into the world of Clos Topology, a concept born from a 1952 paper by Charles Clos, a researcher at Bell Laboratories. His revolutionary ideas on multistage telephone switching systems have transcended time, now serving as the foundation for building highly scalable data centers with cost-effective switches.
The Cisco ACI fabric is based on either a two-tier (spine and leaf switch) or three-tier (spine switch, tier-1 leaf switch, and tier-2 leaf switch) architecture.
In a Cisco Application Centric Infrastructure (ACI), each of the leaf, Fabric Extenders and spine switches has different functions.
</p>

<p><strong><span style="color:#074080">Leaf Switches</span></strong></p>

<ul>
  <li>Leaf switches are the ingress/egress points for traffic into and out of an ACI fabric.</li>
  <li>They connect directly to servers and other devices, facilitating rapid data transfer.</li>
  <li>Each leaf switch is linked to every spine switch, creating a mesh of high-bandwidth connections.</li>
  <li>This setup ensures that any data packet is only a couple of hops away from its destination, significantly reducing latency.</li>
</ul>

<p><strong><span style="color:#074080">Fabric Extenders (FEX)</span></strong></p>

<p>Fabric Extenders (FEX) in Cisco’s Application Centric Infrastructure (ACI) have several key functions:</p>
<ul>
  <li>Port Extension - FEX extends the ports of a Nexus switch, acting as a remote line card⁶. This allows for the use of more cost-effective devices with a reduced set of features that are connected to the Leaf Switches.</li>
  <li>Agile Connectivity - FEX offers agile connectivity for rack and blade servers and for converged fabric deployments⁵. It delivers innovation to the data center, reduces total cost of ownership, and gains architectural flexibility.</li>
  <li>Configuration and Management - All management is done on the parent switch; there is no console/vty access on the FEX⁶. This simplifies the management of the network infrastructure.</li>
  <li>Cisco ACI leaf switch models with -G or -Tin the product name, such as Cisco Nexus N9K-C9348GC-FXP, N9K-C93108TC-FX, N9K-C93108TC-FX-24, N9K-C93108TC-EX, N9K-C93108TC-EX-24, N9K-C93216TC-FX2, and N9K-93108TC-FX3P.</li>
</ul>

<p>Discover the steps to configure Fabric Extenders (FEX) in Cisco ACI in Cisco comprehensive guide available <a href="https://www.cisco.com/c/en/us/support/docs/cloud-systems-management/application-policy-infrastructure-controller-apic/200529-Configure-a-Fabric-Extender-with-Applica.html" target="_blank">here</a>.</p>

<p><strong><span style="color:#074080">Spine Switches</span></strong></p>

<p>Spine switches are Clos intermediary switches that have a number of key functions:</p>

<ul>
  <li>They are responsible for high-capacity data transmission across the network.</li>
  <li>They ensure that data packets travel efficiently from one leaf switch to another.</li>
  <li>The spine layer is crucial for maintaining the performance of the network as it scales, providing a robust framework that can handle increasing data demands without bottlenecks.</li>
  <li>Spine switches interconnect all leaf switches in a full-mesh topology.</li>
</ul>

<p>Together, they create an architecture that is highly standardized across deployments. This design allows for a highly scalable and efficient network infrastructure.</p>

<h5 id="two-tier-or-multi-tier-design"><strong><span style="color:#074080">Two-Tier or Multi-Tier Design?</span></strong></h5>

<p style="text-align: justify;">
Below Figure 1 illustrates the necessary components and cabling for an two-tier ACI fabric. It’s important to note that no cables should be connected between ACI leaf switches, as this is characteristic of Clos network design. Similarly, cross-cabling ACI spines will lead to the disabling of the cross-connected ports. The topology depicts a full mesh of cabling between the spine and leaf layers. 
However, it’s important to note that while the Spine-Leaf architecture can be implemented without ACI, with the switches working in NX-OS mode. I would like to highlight that ASICs are different for Leaf and Spine switches, so a switch like the Nexus N9K-93180YC-EX cannot run as Spine switches.  
</p>

<p align="center">
<img class="center" style="float" width="350" height="350" src="/images/phy_design/aci-PhysicalDesign.jpg" alt="phy-design" title="PHY" />  
</p>

<p><strong>Figure 1</strong> Two-Tier Topology Spine-Leaf classic interconnect between leaf and spine switches</p>

<p>In summary, I would like to recommend to check switch compatibility on the links below:</p>

<ul>
<li><a href="https://www.cisco.com/c/dam/en/us/products/switches/nexus-9000-series-switches/nexus-9300-10GbaseT-switches-comparison.html" target="_blank">Cisco Nexus 9300 1/10GBaseT Switches</a></li>
<li><a href="https://www.cisco.com/c/en/us/products/switches/nexus-9000-series-switches/nexus-9300-10ge-fiber-switches-comparison.html" target="_blank">Cisco Nexus 9300 1/10/25/40/50/100GE Fiber Switches</a></li>
<li><a href="https://www.cisco.com/c/dam/en/us/products/switches/nexus-9000-series-switches/nexus-9300-40GE-switches-comparison.html" target="_blank">Cisco Nexus 9300 40/100 GE Switches</a></li>
<li><a href="https://www.cisco.com/c/en/us/products/switches/nexus-9000-series-switches/nexus-9300-400-ge-switches.html" target="_blank">Cisco Nexus 9300 400 GE Switches</a></li>
<li><a href="https://www.cisco.com/c/en/us/products/collateral/switches/nexus-9000-series-switches/datasheet-c78-731792.html" target="_blank"> Cisco Nexus 9300 ACI Fixed Spine Switches Data Sheet</a></li>
</ul>

<p>If you plan to use Cisco Nexus modular switches for Cisco ACI fabric technical details you can find on below links:</p>

<ul>
  <li><a href="https://www.cisco.com/c/en/us/products/collateral/switches/nexus-9000-series-switches/datasheet-c78-729404.html" target="_blank"> Cisco Nexus 9500 Series Switches Data Sheet</a></li>
  <li><a href="https://www.cisco.com/c/en/us/products/switches/nexus-9000-series-switches/nexus-9500-chassis-comparison.html" target="_blank"> Nexus 9500 Chassis Comparison</a></li>
  <li><a href="https://www.cisco.com/c/en/us/products/switches/nexus-9000-series-switches/nexus-9800-models-comparison.html" target="_blank"> Nexus 9800 Compare Models</a></li>
  <li><a href="https://www.cisco.com/c/en/us/products/switches/nexus-9000-series-switches/nexus-9800-line-cards-comparison.html" target="_blank"> Nexus 9800 Line Cards comparison</a></li>
</ul>

<p style="text-align: justify;">
Starting from Cisco ACI 4.1, the fabric also supports a multi-tier (three-tiers) topology. This includes two tiers of leaf switches, allowing for vertical expansion of the Cisco ACI fabric. This is particularly useful for migrating a traditional three-tier architecture (core-aggregation-access), a common design model for many enterprise networks.
The ACI Multi-Tier is a beneficial solution for data centers aiming to reduce inter-row cabling and have low-bandwidth needs for top-of-rack switches. In this setup, Tier 1 leaf switches are placed at the end of each row, while Tier 2 leaf switches are positioned at the top of each rack. This strategy is particularly effective in managing cables and meeting bandwidth requirements.
</p>

<p align="center">
<img class="center" style="float" width="450" height="400" src="/images/phy_design/aci-PhysicalDesign 2.jpg" alt="phy-design" title="PHY" />  
</p>

<p><strong>Figure 2</strong> Multi -Tier Topology Spine- Tier 1 Leaf and Tier 2 Leaf</p>

<p align="center">
<img class="center" style="float" width="400" height="300" src="/images/phy_design/aci-Sec PhysicalDesign 2.jpg" alt="phy-design" title="PHY" />  
</p>

<p><strong>Figure 3</strong> Multi -Tier Topology Spine- Tier 1 Leaf and Tier 2 Leaf option between two data center buildings</p>

<p>The topology requirements may include various models of spine and leaf switches, please check below:</p>

<ul>
  <li>
    <p><strong>Spine:</strong> EX/FX/C/GX/GX2 spines (For example, Cisco Nexus 9332C, 9316D-GX, 9348D-GX2A, 9408 and 9500 with EX/FX/GX linecards).</p>
  </li>
  <li>
    <p><strong>Tier-1</strong> Leaf: EX/FX/FX2/FX3/GX/GX2 except Cisco Nexus 93180LC-EX.</p>
  </li>
  <li>
    <p><strong>Tier-2</strong> All EX/FX/FX2/FX3/GX/GX2.</p>
  </li>
</ul>

<p>In the scenario where server access is at 1G and there is not need for high bandwidth, it’s a viable choice to use the Cisco Nexus 93180FX3 or Nexus 9364C-GX as the tier-1 leaf and the Cisco Nexus 9348FXP as the tier-2 leaf.</p>

<p>Before you start to build and design a multi-tier topology in a Cisco ACI environment, please take into account the following:</p>

<p><strong><span style="color:#074080">1.Fabric Port Configuration:</span></strong> All inter-switch connections must be configured as fabric ports. This includes the connections between Tier-2 leaf switch fabric ports and Tier-1 leaf switch fabric ports.</p>

<p><strong><span style="color:#074080">2.Tier-2 Leaf Switch Connectivity:</span></strong> A Tier-2 leaf switch has the capability to connect to more than two Tier-1 leaf switches. This is in contrast to a traditional double-sided vPC design, which only supports two upstream switches. The maximum number of ECMP links supported by a Tier-2 leaf switch to a Tier-1 leaf switch is 18.</p>

<p><strong><span style="color:#074080">3.Endpoint Group (EPG) and Layer 3 Outside (L3Out) Connections:</span></strong> EPGs, L3Outs, Cisco APICs, or Fabric Extenders (FEX) can be connected to either Tier-1 or Tier-2 leaf switches.</p>

<p><strong><span style="color:#074080">4.Tier-1 Leaf Switch Connections:</span></strong> Tier-1 leaf switches can have both hosts and Tier-2 leaf switches connected to them.</p>

<p><strong><span style="color:#074080">5.Switch Tier Transition:</span></strong> Transitioning from a Tier-1 to a Tier-2 leaf switch (and vice versa) requires decommissioning and recommissioning the switch.</p>

<p><strong><span style="color:#074080">6.Compatibility with Cisco ACI Multi-Pod and Multi-Site:</span></strong> Multi-tier architectures are compatible with both Cisco ACI Multi-Pod and Cisco ACI Multi-Site.</p>

<p><strong><span style="color:#074080">7.Tier-2 Leaf Switch Connectivity Limitations:</span></strong> Tier-2 leaf switches cannot be connected to remote leaf switches (Tier-1 leaf switches).</p>

<p><strong><span style="color:#074080">8.Scale Considerations:</span></strong> The combined number of Tier-1 and Tier-2 leaf switches must not exceed the maximum number of leaf switches validated for a given release (400 per pod; 500 per Cisco ACI Multi-Pod as of Cisco ACI release 6.0(1)).</p>

<p>Reference for <a href="https://www.cisco.com/c/en/us/products/switches/nexus-9000-series-switches/nexus-9800-line-cards-comparison.html" target="_blank"> Cisco ACI Multi-tier Architecture</a>.</p>

<p><strong><em><span style="color:#074080">Fabric Port</span></em></strong></p>

<p style="text-align: justify;">
In Cisco's Application Centric Infrastructure (ACI), a fabric port is a type of port that is used for uplink connectivity within the fabric. These ports are typically used to connect different switches within the ACI fabric.
For example, in a multi-tier topology, all switch-to-switch links must be configured as fabric ports. This includes the connections between Tier-2 leaf switch fabric ports and Tier-1 leaf switch fabric ports.
</p>

<p><strong><span style="color:#074080">Fiber optics modules</span></strong></p>

<p style="text-align: justify;">  
Don't forget that based on specific switch model and uplinks design you will need to choose proper optical modules.
All details about compatibility matrix for Cisco Optics and devices you can find on below links:
</p>
<ul>
  <li><a href="https://tmgmatrix.cisco.com/">Cisco Optics-to-Device Compatibility Matrix</a></li>
  <li><a href="https://tmgmatrix.cisco.com/iop">Cisco Optics-to-Optics Interoperability Matrix</a></li>
</ul>
<p style="text-align: justify;"> 
If you struggling or you have some doubts about compatibility between switches and optical modules SFP SFP+ QSFPs I would like to recommend:
</p>

<p>
<iframe width="420" height="315" src="https://www.youtube.com/embed/5I19ePqzNhc?controls=0">
</iframe>
</p>

<p>
<iframe width="420" height="315" src="https://www.youtube.com/embed/FFhNGxmniR4?controls=0">
</iframe>  
</p>

<p><strong><span style="color:#074080">Taxonomy for Cisco Nexus 9000 Series Part Numbers</span></strong></p>

<p style="text-align: justify;">
For a comprehensive understanding and decoding of all Nexus Part Numbers names, kindly refer to Figure 4.
</p>

<p align="center">
<img class="center" style="float" width="800" height="400" src="/images/phy_design/taxonomy_nk9.png" alt="taxonomynk9" title="PHY" />  
</p>

<p><strong>Figure 4</strong> Cheet-sheet for decoding Cisco Nexus 9000 Series Part Numbers</p>]]></content><author><name></name></author><category term="ACI" /><summary type="html"><![CDATA[Let's dive into the world of Cisco ACI physical design. After all, we all know that without hardware, there is no cloud or networking.]]></summary></entry><entry><title type="html">Cisco ACI Fabric - APIC Day 1 configuration</title><link href="http://0.0.0.0:4000/aci/aci-apic-initial-setup/" rel="alternate" type="text/html" title="Cisco ACI Fabric - APIC Day 1 configuration" /><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>http://0.0.0.0:4000/aci/aci-apic-initial-setup</id><content type="html" xml:base="http://0.0.0.0:4000/aci/aci-apic-initial-setup/"><![CDATA[<p align="center">
<img src="/images/apic_day_0_config/apic.jpg" alt="Cisco ACI APIC" title="Cisco APIC interconnects" />  
</p>
<p style="text-align: justify;">
This time I will go through configuration steps for provisioning switches on Cisco ACI Fabric, NTP, DNS and BGP Route Reflectors. Steps described in this post are applicable for Cisco ACI Fabric and Cisco ACI simulator. 
</p>

<p><strong>0.</strong> In your favorite browser address bar add <strong><em>https://oob.ip.address</em></strong>. Use IP address what you set during APIC initial configuration.</p>

<p><img class="center" width="800" height="400" src="/images/acisim_init_conf/login_screen2.jpg" alt="APIC Login" title="APIC Login" /></p>

<p><strong>Cisco ACI Fabric APIC ver 5.2 supported browsers:</strong></p>
<ul>
  <li>Chrome version 59 (at minimum)</li>
  <li>Firefox version 54 (at minimum)</li>
  <li>Internet Explorer version 11 (at minimum)</li>
  <li>Safari version 10 (at minimum)</li>
</ul>

<p><strong>1.</strong> After what you will login it will appear Cisco APIC welcome screen, looks nice. Just press <strong>Let’s go</strong> as this white button suggesting.
<img class="center" width="800" height="400" src="/images/apic_day_0_config/1.jpg" alt="APIC Welcome Screen" title="APIC Welcome Screen" /></p>

<p><strong>2.</strong> One more screen from APIC this one is important as it will provided you information about:</p>
<ul>
  <li>New features for installed APIC version of APIC.</li>
  <li>Updated scalability numbers.</li>
  <li>Very good resources and knowledge base for ACI fabric and APIC clusters.<br />
After what you reviewed all interesting things pres <strong>Start ACI Fabric Setup</strong> button.</li>
</ul>

<p><img class="center" width="800" height="470" src="/images/apic_day_0_config/2.jpg" alt="APIC Release notes" title="APIC Release notes" /></p>

<p><strong>3.</strong> Below we can see the most important sections which should be configured before any service configuration on ACI.</p>

<p>I would like to suggest following order:</p>
<ol>
  <li>Add leaf and spine to Cisco ACI Fabric.</li>
  <li>NTP server configuration.</li>
  <li>DNS server configuration.</li>
  <li>BGP RR configuration for Spine.</li>
</ol>

<p><img class="center" width="800" height="450" src="/images/apic_day_0_config/3.jpg" alt="APIC Basic service setup" title="APIC Basic service setup" /></p>

<p><strong>4.</strong> Add first leaf to Cisco ACI Fabric, for serial number we will use TEP-1-102, 101 and 103 as this is Cisco ACI simulator on real Cisco ACI Fabric you will have serial numbers in following format F1234A768. For node ID choose unique number, once when you tied node ID to serial number you will not be able to change on the fly. If you would like to change you will need to do decommissioning and commissioning with new node ID. Switch name you can change anytime if you will not like assigned during initial switch provisioning.</p>

<p><img class="center" width="600" height="s 200" src="/images/apic_day_0_config/4.jpg" alt="APIC spine leaf membership" title="APIC spine leaf membership" /></p>

<p><strong>5.</strong> Here you should pay attention  and check is it switch in <strong>Registering</strong> phase, if you notice some other messages please stop and perform tshoot.</p>

<p><img class="center" width="800" height="400" src="/images/apic_day_0_config/5.jpg" alt="APIC add leaf to membership" title="APIC add leaf to membership" /></p>

<p><strong>6.</strong> Once when is done we should see that switch is successfully <strong>Register</strong> as we can see below for leaf with node ID 101.</p>

<p><img class="center" width="800" height="350" src="/images/apic_day_0_config/6.jpg" alt="APIC add leaf to membership" title="APIC add leaf to membership" /></p>

<p><strong>7.</strong> Same you should repeat for all Spine switches too, below you can see <strong>Registering</strong> of spine with node ID 103.</p>

<p><img class="center" width="800" height="350" src="/images/apic_day_0_config/7.jpg" alt="APIC add spine to membership" title="APIC add spine to membership" /></p>

<p><strong>8.</strong> And Spine with node ID 103 has been successfully <strong>Register</strong>.</p>

<p><img class="center" width="800" height="350" src="/images/apic_day_0_config/8.jpg" alt="APIC add spine to membership" title="APIC add spine to membership" /></p>

<p><strong>9.</strong> Based on workflow from point #3 I will proceed with NTP configuration. You can choose time format from <strong>local</strong> or <strong>utc</strong>. Use <strong>plus</strong> sign for adding one or more NTP servers, one of them you can set like preferable.</p>

<p><img class="center" width="800" height="400" src="/images/apic_day_0_config/9.jpg" alt="APIC NTP Configuration" title="APIC NTP Configuration" /></p>

<p><strong>10.</strong> In the lab for NTP server I use public Google NTP server just for test.
<img class="center" width="800" height="450" src="/images/apic_day_0_config/10.jpg" alt="APIC NTP Configuration" title="APIC NTP Configuration" /></p>

<p><strong>11.</strong> Here I will add Google DNS server just for test. Usually you will use internal DNS. Don’t forget to add DNS records for leaf, spines and APICs.</p>

<p><img class="center" width="800" height="400" src="/images/apic_day_0_config/11.jpg" alt="APIC DNS Configuration" title="APIC DNS Configuration" /></p>

<p><strong>12.</strong> Here we can check status about what we done already. Below you can notice that still we have to configure BGP route reflectors.  To configure BGP just press button <strong>Begin</strong> which will redirect you to configuration page.</p>

<p><img class="center" width="800" height="430" src="/images/apic_day_0_config/12.jpg" alt="APIC setup overview" title="APIC setup overview" /></p>

<p><strong>13.</strong> Press <strong>plus</strong> sign and add minimu two spines per Cisco ACI fabric site.</p>

<p><img class="center" width="800" height="400" src="/images/apic_day_0_config/13.jpg" alt="APIC BGP RR config" title="APIC BGP RR config" /></p>

<p><strong>14.</strong> From drop down choose node id of your spine. As I am using Cisco ACI Simulator I have only one spine with node ID 103. For test I am using default AS number.</p>

<p><img class="center" width="600" height="250" src="/images/apic_day_0_config/14.jpg" alt="APIC BGP RR config" title="APIC BGP RR config" /></p>

<p><strong>15.</strong> Below you can see that spine with node ID 103 was successful registered like RR in Pod 1.</p>

<p><img class="center" width="800" height="300" src="/images/apic_day_0_config/15.jpg" alt="APIC BGP RR config" title="APIC BGP RR config" /></p>

<p><strong>16.</strong> Short overview all looks good as Proxy won’t be used.</p>

<p><img class="center" width="800" height="400" src="/images/apic_day_0_config/16.jpg" alt="APIC setup overview" title="APIC setup overview" /></p>

<p><strong>17.</strong> Summary page, last step for configuration setup.
<img class="center" width="800" height="430" src="/images/apic_day_0_config/17.jpg" alt="APIC setup summary" title="APIC setup summary" /></p>

<p><strong>18.</strong> I would like to recommend you to check <strong>Dashboard</strong> sections like below on print screen</p>
<ul>
  <li>Switches should be <strong>Healthy</strong>.</li>
  <li>Controllers should be <strong>In service</strong> and <strong>FullyFit</strong>.</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Note
This is only zero day configuration for Cisco APIC. For MultiPod, Multisite, 
Remote Leaf and other Cisco ACI Fabric designs should be configured other parameters
and network objects.
</code></pre></div></div>
<p><img class="center" width="800" height="400" src="/images/apic_day_0_config/24.jpg" alt="" title="" /><br />
<br />
<br />
<strong>Bonus steps</strong> for those how like to have clean ACI fabric without lot of system notifications.<br />
Next few steps will describe how we can disable  messages from Cisco Smart Software Manage, this is mostly applicable for lab deployments and ACI Simulator as after 90 days should be reconfigured.</p>

<p><strong>19.</strong> From APIC notification center please choose Cisco Smart Software Manager (CSSM).</p>

<p><img class="center" width="800" height="400" src="/images/apic_day_0_config/18.jpg" alt="APIC notifications" title="APIC notification" /></p>

<p><strong>20.</strong> Press on fault notification F3057.</p>

<p><img class="center" width="800" height="400" src="/images/apic_day_0_config/19.jpg" alt="APIC notification" title="APIC notification" /></p>

<p><strong>21.</strong> As below on print screen click on <strong>Configure Network Settings</strong>.</p>

<p><img class="center" width="800" height="350" src="/images/apic_day_0_config/20.jpg" alt="APIC notification" title="APIC notification" /></p>

<p><strong>22.</strong> From drop down menu choose <strong>Offline</strong> click on OK and you are ready.</p>

<p><img class="center" width="500" height="400" src="/images/apic_day_0_config/21.jpg" alt="APIC notification" title="APIC notification" /></p>

<p>I have walked through most important parameters for Cisco ACI Fabric. Now you can start to play with Cisco ACI Fabric network objects or you can proceed with fine tunning for the production environment. 😎</p>]]></content><author><name></name></author><category term="ACI" /><summary type="html"><![CDATA[This time I will go through configuration steps for provisioning switches on Cisco ACI Fabric, NTP, DNS and BGP Route Reflectors. Steps described in this post are applicable for Cisco ACI Fabric and Cisco ACI simulator.]]></summary></entry><entry><title type="html">Cisco ACI Simulator APIC Initial setup</title><link href="http://0.0.0.0:4000/aci/acisim-init-conf/" rel="alternate" type="text/html" title="Cisco ACI Simulator APIC Initial setup" /><published>2022-11-10T00:00:00+00:00</published><updated>2022-11-10T00:00:00+00:00</updated><id>http://0.0.0.0:4000/aci/acisim-init-conf</id><content type="html" xml:base="http://0.0.0.0:4000/aci/acisim-init-conf/"><![CDATA[<p align="center">
<img src="/images/acisim_init_conf/subject.jpg" alt="Cisco ACI Simulator" title="Cisco ACI Initial Configuration" />  
</p>
<p style="text-align: justify;">
If you already read my last post <a href="/aci/acisim-on-kvm/index.html" target="_blank">How to run Cisco ACI Simulator on KVM host</a> you know that now we need to do first step and go through Cisco APIC initial configuration setup. If you need help this post is right place for you. So let' go and see what we need to do.
</p>

<p><img class="center" style="float" width="400" height="400" src="/images/acisim_init_conf/net_diagram.jpg" alt="ACISIM Diagram" title="ACISIM internal connectivity" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Note:
Cisco ACI simulator doesn't support data plane and you won't be able to connect any  
other VM which can represent End Point. Simple you will not be able to issue ping  
command and send ICMP request and see any ICMP reply or use any other protocol for testing purpose.
</code></pre></div></div>

<p>1.VM is booting
<img class="center" width="800" height="400" src="/images/acisim_init_conf/1.jpg" alt="acisim booting" title="acisim booting" /></p>

<p>2.If you have recommended HW resource use large topology otherwise your choice is “N”.
<img class="center" width="800" height="280" src="/images/acisim_init_conf/2.jpg" alt="Acisim topology size" title="Acisim topology size" /></p>

<p>3.After few seconds it will appear this nice logo.
<img class="center" width="800" height="280" src="/images/acisim_init_conf/3.jpg" alt="APIC Welcome screen" title="APIC Welcome screen" /></p>

<p>4.For ACISIM I would like to recommend to accept all default parameters. All parameters you can change but in that case there is no guaranteed that Cisco ACI simulator will work properly.
<img class="center" width="800" height="s 350" src="/images/acisim_init_conf/7.jpg" alt="APIC ACISIM setup" title="APIC ACISIM setup" /></p>

<p>5.One of parameters what actually you should change is default OOB subnet and gateway. Choose IP address/Subnet mask from your OOB lab network and set in this step. Later you will use this IP address for access APIC UI via HTTPS.
<img class="center" width="600" height="100" src="/images/acisim_init_conf/8.jpg" alt="APIC ACISIM OOB" title="APIC ACISIM OOB" /></p>

<p>6.Please set admin password and don’t forget, later you will need for login to APIC UI 😉
<img class="center" width="600" height="100" src="/images/acisim_init_conf/9.jpg" alt="APIC ACISIM admin pass" title="APIC ACISIM admin pass" /></p>

<p>7.Here you have chance to change any configuration parameter what you set during initial configuration.
If you are self confident and you put password on the sticky note on your desk you just press “ENTER”
<img class="center" width="600" height="120" src="/images/acisim_init_conf/10.jpg" alt="APIC ACISIM modification" title="APIC ACISIM modification" /></p>

<p>8.In your favorite browser type https://oob.ip.address what you set during initial configuration.
When APIC login page appear for User ID use <strong>admin</strong> and for password use note from your sticky note from your the desk.
<img class="center" width="800" height="400" src="/images/acisim_init_conf/login_screen1.jpg" alt="Login" title="Login" /></p>

<p>Final Part 2 coming soon! Are you enjoyed reading? If yes please subscribe <strong><a href="/subscribe/index.html" target="_blank">Here</a></strong></p>

<head>
<style>
a:link {
  text-decoration: underline;
}

a:visited {
  text-decoration: none;
}

a:hover {
  text-decoration: none;
}

a:active {
  text-decoration: none;
}
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
</style>
</head>]]></content><author><name></name></author><category term="ACI" /><summary type="html"><![CDATA[If you already read my last post How to run Cisco ACI Simulator on KVM host you know that now we need to do first step and go through Cisco APIC initial configuration setup. If you need help this post is right place for you. So let' go and see what we need to do.]]></summary></entry><entry><title type="html">How to run Cisco ACI Simulator on KVM host</title><link href="http://0.0.0.0:4000/aci/acisim-on-kvm/" rel="alternate" type="text/html" title="How to run Cisco ACI Simulator on KVM host" /><published>2022-06-13T00:00:00+00:00</published><updated>2022-06-13T00:00:00+00:00</updated><id>http://0.0.0.0:4000/aci/acisim-on-kvm</id><content type="html" xml:base="http://0.0.0.0:4000/aci/acisim-on-kvm/"><![CDATA[<p align="center">
<img src="/images/acisim_on_kvm/subject.jpg" alt="Cisco ACI Simulator" title="Cisco ACI Simulator Spine leaf and APIC" />  
</p>
<p style="text-align: justify;">
As officially ACI Simulator is not supported on KVM host and I didn't have ESXi in my lab I have decided to do experiment. 
Moreover I have played with amount of hardware resource necessary for this experiment. If you would like to know result of this experiment, you can proceed and read rest of the post.
</p>

<p style="text-align: justify;">
At the beginning lets say couple of words about Cisco ACI Simulator or ACISIM. So this is Cisco virtual machine, inside which we can find simulation of two leaf switches, one spine and one Cisco Application Policy Infrastructure Controller (APIC). Actually this is simulation of Data Center leaf spine topology with SDN controller. 
</p>

<p><strong><span style="color:#074080">Official Cisco ACI Simulator Virtual Machine HW requirements:</span></strong></p>

<p><img style="center" src="/images/acisim_on_kvm/hw_resurces.jpg" alt="HW requirements" title="Official HW requirements" /></p>
<p style="text-align: justify;">
To make things more interesting I have reduced HW resources just from the beginning of experiment.&#128521;
</p>

<p><strong><span style="color:#074080">And I created setup as below:</span></strong></p>

<p><img style="float" width="350" height="100" src="/images/acisim_on_kvm/hw_resurces2.jpg" alt="HW requirements" title="HW requirements tested in the lab" /></p>

<p style="text-align: justify;">
So, are you ready to spin ACI Simulator with less than minimum HW resource for release 5.2(4d)? If yes, let's go and see what will happen in the end.&#128526;
</p>
<p><strong><span style="color:#074080">Step 1. Download ACI Simulator</span></strong></p>

<p>Go to <a href="https://software.cisco.com/download/home/286283149/type/286283168/release/5.2(4d)" target="_blank">Cisco Download Page</a>, please don’t forget that you should have valid Cisco account, otherwise no luck.</p>

<p>Download all <strong>eight</strong> pieces:</p>
<ul>
<li>acisim-5.2-4d_part1.ova</li>
<li>acisim-5.2-4d_part2.ova</li>
<li>acisim-5.2-4d_part3.ova</li>
<li>acisim-5.2-4d_part4.ova</li>
<li>acisim-5.2-4d_part5.ova</li>
<li>acisim-5.2-4d_part6.ova</li>
<li>acisim-5.2-4d_part7.ova</li>
<li>acisim-5.2-4d_part8.ova</li>
</ul>
<p><strong><span style="color:#074080">Step 2. Concatenate downloaded parts and create QCOW2 image from VMDK</span></strong></p>

<p>Use Linux <strong>cat</strong> command to concatenate all 8 parts.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat acisim-5.2-4d_part1.ova \ 
    acisim-5.2-4d_part2.ova \
    acisim-5.2-4d_part3.ova \
    acisim-5.2-4d_part4.ova \
    acisim-5.2-4d_part5.ova \
    acisim-5.2-4d_part6.ova \
    acisim-5.2-4d_part7.ova \
    acisim-5.2-4d_part8.ova &gt; acisim.ova
</code></pre></div></div>

<p>Optional check MD5 of acisim.ova VM once when concatenations is done.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>md5sum -c aci.md5
</code></pre></div></div>

<p>Now we have virtual machine with vmdk vHD but we need qocow2 image. So what’s next, we need to untar acisim.ova and we have to convert acisim-5.2-4d-disk1.vmdk to acisim-5.2-4d-disk1.qcow2.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tar -xvf aci.ova
</code></pre></div></div>
<p>voila finally we have acisim-5.2-4d-disk1.vmdk let’s run one more command and we will have acisim-5.2-4d-disk1.qcow2</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>qemu-img convert -O qcow2 acisim-5.2-4d-disk1.vmdk path/to/images_folder/acisim-5.2-4d-disk1.qcow2
</code></pre></div></div>

<p><strong><span style="color:#074080">Step 3. Run ACISIM VM on Ubuntu with KVM hypervisor</span></strong></p>

<p style="text-align: justify;">
Run virt-manger application from desktop UI or If you are using headless server first ssh to server (don't forget to enable x11 forwarding).
</p>
<ul>
<li>Once when you run <b>virt-manager command</b> it will appear creation VM setup dialog</li>
</ul>

<p><img style="center" src="/images/acisim_on_kvm/Untitled0.jpg" alt="Virt-Manager1" title="Open VM setup dialog" /></p>

<ul>
<li>Please choose like below on print screens.</li>
</ul>

<p><img style="float" width="350" height="275" src="/images/acisim_on_kvm/Untitled.jpg" alt="Virt-Manager2" title="Create new VM" />  <img style="float" width="350" height="275" src="/images/acisim_on_kvm/Untitled 2.jpg" alt="Virt-Manager3" title="Choose qcow2 image" /></p>

<ul>
<li>Here you will need to point to folder where you saved acisim.qcow2 image.</li>
</ul>

<p><img style="float" width="350" height="275" src="/images/acisim_on_kvm/Untitled 3.jpg" alt="Virt-Manager4" title="Choose qcow2 image" /> <img style="float" width="350" height="275" src="/images/acisim_on_kvm/Untitled 4.jpg" alt="Virt-Manager5" title="Customize VM parameters" /></p>

<ul>
<li>Let use only 8 vCPUs and 32G of RAM, this is far from recommended by Cisco. </li>
</ul>

<p><img style="float" width="350" height="275" src="/images/acisim_on_kvm/Untitled 5.jpg" alt="Virt-Manager6" title="Increase vCPU" /> <img style="float" width="350" height="275" src="/images/acisim_on_kvm/Untitled 6.jpg" alt="Virt-Manager7" title="Increase Memory" /></p>

<ul>
<li>Mandatory, please select network adapter type <b>e1000</b> otherwise later you will not be able to connect to your ACISIM via HTTPS or SSH.</li>
<li>To start VM creation press "Begin installation".</li>
</ul>

<p><img style="float" width="350" height="275" src="/images/acisim_on_kvm/Untitled 7.jpg" alt="Virt-Manager8" title="Set network" /> <img style="float" width="350" height="275" src="/images/acisim_on_kvm/Untitled 9.jpg" alt="Virt-Manager9" title="Begin installation" /></p>

<ul>
<li>Between all VMs choose acisim and run.</li>
</ul>

<p><img style="center" src="/images/acisim_on_kvm/Untitled 10.jpg" alt="Virt-Manager10" title="Select VM and Run" /></p>

<ul>
<li>Press OPEN button to access acisim VM console.</li>
</ul>

<p><img style="center" src="/images/acisim_on_kvm/Untitled 11.jpg" alt="Virt-Manager11" title="Open Console" /></p>

<ul>
<li>After 2-3 min (depend about power of your host) it will appear APIC initial screen.</li>
</ul>

<p><img style="center" src="/images/acisim_on_kvm/Untitled 12.jpg" alt="Virt-Manager12" title="APIC initial config" /></p>

<ul>
<li>Below we can see that we are running Cisco ACI simulator on KVM host and successful start of initial configuration on Cisco APIC.</li>
</ul>

<p><img style="center" src="/images/acisim_on_kvm/Untitled 13.jpg" alt="Virt-Manager13" title="APIC initial config" /></p>

<p><strong><span style="color:#074080">Short Outcome:</span></strong></p>
<ul>
<li>If we have insufficient HW resource already in the lab we can reduce number of vCPUs and amount of memory
for ACISIM VM.</li> 
<li>In this Blog post I have used 8x vCPS and 32 GB of RAM below this it will not work.</li>
<li>We can use Cisco ACI simulator on KVM host, even if it is not officially supported by vendor.</li>
</ul>
<p><strong><span style="color:#074080">Enjoy your ACI simulator and happy testing!!!</span></strong></p>]]></content><author><name></name></author><category term="ACI" /><summary type="html"><![CDATA[As officially ACI Simulator is not supported on KVM host and I didn't have ESXi in my lab I have decided to do experiment. Moreover I have played with amount of hardware resource necessary for this experiment. If you would like to know result of this experiment, you can proceed and read rest of the post.]]></summary></entry><entry><title type="html">Five easy steps to configure static VxLAN Part 4</title><link href="http://0.0.0.0:4000/vxlan/static-vxlan-part4/" rel="alternate" type="text/html" title="Five easy steps to configure static VxLAN Part 4" /><published>2021-12-24T00:00:00+00:00</published><updated>2021-12-24T00:00:00+00:00</updated><id>http://0.0.0.0:4000/vxlan/static-vxlan-part4</id><content type="html" xml:base="http://0.0.0.0:4000/vxlan/static-vxlan-part4/"><![CDATA[<style>
p1 {
  font-size: 11px;
}
p2 {
  font-size: 12px;
}
</style>

<p align="center">
<img src="/images/static_vxlan/vxlan_tunnel4.jpg" alt="Overlay VxLAN Cumulus Linux" title="VxLAN between Cumulus VX and Juniper vMX" />  
</p>
<p style="text-align: justify;">
Part 4 final countdown with static VxLAN. In this post I will cover the case when we need to extend L2 domain between Data centers. According to this VxLAN tunnel will be established between two DC gateways and DC gateways will represent VTEPs.
</p>

<p style="text-align: justify;">
Data center gateways will be simulated with Cumulus Linux VX vSwitch and Juniper vMX virtual router. This time I will skip installation and configuration of the following:  
<ul>
<li>namespaces for simulation of VM1 and VM3</li>
<li>Download VPCS and configuration</li>
<li>Configuration of VxLAN tunnels from srv and srv1 to Cumulus VX switch as it will be the same like in Part 3.</li>
</ul>
All details related to the above configuration and deployment you can find in blogposts <a href="/vxlan/static-vxlan-part2/index.html">Part 2</a> and <a href="/vxlan/static-vxlan-part3/index.html">Part 3</a>.
</p>

<p align="center"> 
  <img src="/images/static_vxlan/net_diag4.jpg" alt="VxLAN, VTEP, Overlay Network" title="VxLAN Network diagram" /> 
</p>

<p><strong>Virtual router, switch and linux host machines:</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">VM name</th>
      <th style="text-align: left">OS Version</th>
      <th style="text-align: left">Extra installed SW</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">srv</td>
      <td style="text-align: left">Ubuntu 20.04</td>
      <td style="text-align: left">bridge-utils</td>
    </tr>
    <tr>
      <td style="text-align: left">srv1</td>
      <td style="text-align: left">Ubuntu 20.04</td>
      <td style="text-align: left">bridge-utils</td>
    </tr>
    <tr>
      <td style="text-align: left">virtual switch</td>
      <td style="text-align: left">Cumulus Linux 4.4.1 VX (Nvidia)</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: left">virtual router</td>
      <td style="text-align: left">Juniper vMX  JUNOS 18.4R1.8</td>
      <td style="text-align: left"> </td>
    </tr>
  </tbody>
</table>

<p><strong><span style="color:#074080">Step 1. Configure interfaces on dcgw1 and dcgw2</span></strong></p>

<p>On dcgw1 (Cumulus Linux) configure interface for OSPF peering with dc-interconnect router</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add interface swp3 ip address 172.21.100.1/30
</code></pre></div></div>
<p>add loopback which will be used for VTEP termination point.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add loopback lo ip address 192.168.1.4/32
</code></pre></div></div>
<p>Above we should repeat for dcgw2 (Juniper vMX)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set interfaces xe-0/0/4 unit 0 family inet address 172.21.101.1/30  
</code></pre></div></div>
<p>add loopback which will be used for VTEP termination point.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set interfaces lo0 unit 5 family inet address 192.168.1.5/32  
</code></pre></div></div>
<p><strong><span style="color:#074080">Step 2. Configure dc-interconnect L3 switch interfaces(Cumulus Linux VX)</span></strong></p>

<p>Add loopback</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add loopback lo ip address 2.2.2.2/32
</code></pre></div></div>
<p>Add interface for peering with dcgw1</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add interface swp3 ip address 172.21.100.2/30
</code></pre></div></div>
<p>Add interface for peering with dcgw2</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add interface swp1 ip address 172.21.101.2/30
</code></pre></div></div>
<p><strong><span style="color:#074080">Step 3. Configuration of  OSPF peering</span></strong></p>

<p><u>Apply on dcgw1.</u></p>

<p>Set router ID.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add ospf router-id 192.168.1.4
</code></pre></div></div>
<p>Add specific networks to OSPF.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add ospf network 192.168.1.4/32 area 1
net add ospf network 172.21.100.0/30 area 1
</code></pre></div></div>
<p>Change network type on link between dcgw1 and dc-interconnect.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add interface swp3 ospf network point-to-point
</code></pre></div></div>
<p>On unnecessary interfaces stop sending OSPF messages.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add ospf passive-interface swp1
net add ospf passive-interface swp2
net add ospf passive-interface swp4
net add ospf passive-interface swp5
net add ospf passive-interface swp6
net add ospf passive-interface swp7
</code></pre></div></div>
<p><u>Apply on dc-interconnect.</u></p>

<p>Set router ID.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add ospf router-id 2.2.2.2
</code></pre></div></div>
<p>Add specific networks to OSPF.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add ospf network 2.2.2.2/32 area 1
net add ospf network 172.21.100.0/30 area 1
net add ospf network 172.21.101.0/30 area 1
</code></pre></div></div>
<p>Change network type on links between dc-interconnect dcgw1 and dcgw2.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add interface swp3 ospf network point-to-point
net add interface swp1 ospf network point-to-point
</code></pre></div></div>
<p>On unnecessary interfaces stop sending OSPF messages.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add ospf passive-interface swp2
net add ospf passive-interface swp4
net add ospf passive-interface swp5
net add ospf passive-interface swp6
net add ospf passive-interface swp67
</code></pre></div></div>
<p><u>Apply on dcgw2.</u></p>

<p>Set router ID.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set routing-options router-id 192.168.1.5
</code></pre></div></div>
<p>Change network type on links between dc-interconnect and dcgw2 and enable OSPF on xe-0/0/4 (172.21.101.0/30).</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set protocols ospf area 0.0.0.1 interface xe-0/0/4.0 interface-type p2p
</code></pre></div></div>
<p>Enable OSPF on loopback</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set protocols ospf area 0.0.0.1 interface lo0.5
</code></pre></div></div>
<p>Check OSPF routes on dc-interconnect L3 switch.</p>

<p><img style="center" src="/images/static_vxlan/ospf_check_dcintcon.jpg" alt="Show OSPF routes" title="OSPF Routing" /></p>

<p><strong><span style="color:#074080">Step 4. Add static VxLAN tunnels between dcgw1 and dcgw2</span></strong><br />
<u>Apply on dcgw1.</u><br />
Create VxLAN tunnel interface.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add vxlan vni-103 vxlan id 103
</code></pre></div></div>
<p>Add vni-103 to specific bridge.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add bridge br-22 ports vni-103
</code></pre></div></div>
<p>Configure Loopback primary address for VTEP source.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add vxlan vni-103 vxlan local-tunnelip 192.168.1.4
</code></pre></div></div>
<p>Configure remote VTEP on dcgw2.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add vxlan vni-103 vxlan remoteip 192.168.1.5
</code></pre></div></div>
<p><u>Apply on dcgw2.</u><br />
Create bridge domain.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set bridge-domains br-vl22 vlan-id 22
set bridge-domains br-vl22 vxlan vni 103
</code></pre></div></div>
<p>Enable “ingress-node-replication” this will handle incoming broadcast, unknown unicast, or multicast (BUM) traffic.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set bridge-domains br-vl22 vxlan ingress-node-replication
</code></pre></div></div>
<p>Configure Loopback lo0.5 for VTEP source.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set switch-options vtep-source-interface lo0.5
</code></pre></div></div>
<p>Configure remote VTEP on dcgw1.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set switch-options remote-vtep-list 192.168.1.4
</code></pre></div></div>
<p><strong><span style="color:#074080">Step 5. Connectivity check </span></strong></p>

<p>Run PING command from VM1, VM3 and DB server (VPCS)
<img style="center" src="/images/static_vxlan/vm1_vm3_ping_db_part4.jpg" alt="Ping from namespace" title="Connectivity check with PING command" /></p>

<p><u>Issue ping from VM3 (namespace)</u></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo ip netns exec vm3 ping 192.168.22.4 -- &gt; remote IP on DB server.
</code></pre></div></div>
<p><img style="float" width="650" height="150" src="/images/static_vxlan/vm3_ping_db_part4.jpg" alt=" PING ICMP remote DB Server" title="VM3 Ping remote DB server" /></p>

<p><img style="float" width="750" height="250" src="/images/static_vxlan/vm3_ping_db_part4_v2.jpg" alt="L2 frame, VxLAN, wireshark, packet capture, pcap" title="Wireshark VxLAN packet capture" /></p>

<p><u>Issue ping from VM1 (namespace)</u></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo ip netns exec vm1 ping 192.168.22.4 -- &gt; remote IP on DB server.
</code></pre></div></div>
<p><img style="float" width="650" height="150" src="/images/static_vxlan/vm1_ping_db_part4.jpg" alt=" PING ICMP remote DB Server" title="VM3 Ping remote DB server" /></p>

<p><img style="float" width="750" height="250" src="/images/static_vxlan/vm1_ping_db_part4_v2.jpg" alt="L2 frame, VxLAN, wireshark, packet capture, pcap" title="Wireshark VxLAN packet capture" /></p>

<p>From the capture you can see that two VxLAN tunnels were used for interconnection with Cumulus VX virtual switch. Both tunnels landed on to one bridge on Cumulus VX virtual switch. That way we achieve communication between two VMs and DB server (VPCS).</p>

<p><u>Issue ping from DB server (VPCS) to check connectivity with VM1 and VM3</u></p>

<p><img style="center" width="750" height="250" src="/images/static_vxlan/db_srv_ping_vm3_pcap_part3.jpg" alt="Ping from VPCS" title="Connectivity check with PING command" /></p>

<p>Based on the ping and packet captures we can conclude that we have L2 connectivity between two data centers. It means that we build overlay network on top of classical L3 network connectivity.</p>

<p><br />
<strong><a href="/vxlan/static_vxlan_part1/index.html" target="_blank">Part 1 Static VxLAN between Ubuntu hosts</a></strong><br />
<strong><a href="/vxlan/static-vxlan-part2/index.html" target="_blank">Part 2 Static VxLAN between Ubuntu and Cumulus VX vSwicth</a></strong><br />
<strong><a href="/vxlan/static-vxlan-part3/index.html" target="_blank">Part 3 Static VxLAN between Ubuntu Hosts and Cumulus VX vSwitch DC Gateway</a></strong><br />
<br /></p>]]></content><author><name></name></author><category term="VxLAN" /><summary type="html"><![CDATA[Part 4 final countdown with static VxLAN. In this post I will cover the case when we need to extend L2 domain between Data centers. According to this VxLAN tunnel will be established between two DC gateways and DC gateways will represent VTEPs.]]></summary></entry><entry><title type="html">Five easy steps to configure static VxLAN Part 3</title><link href="http://0.0.0.0:4000/vxlan/static-vxlan-part3/" rel="alternate" type="text/html" title="Five easy steps to configure static VxLAN Part 3" /><published>2021-11-12T00:00:00+00:00</published><updated>2021-11-12T00:00:00+00:00</updated><id>http://0.0.0.0:4000/vxlan/static-vxlan-part3</id><content type="html" xml:base="http://0.0.0.0:4000/vxlan/static-vxlan-part3/"><![CDATA[<style>
p1 {
  font-size: 11px;
}
p2 {
  font-size: 12px;
}
</style>

<p align="center">
<img src="/images/static_vxlan/vxlan_tunnel3.jpg" alt="Overlay VxLAN Cumulus Linux" title="VxLAN between Ubuntu and Cumulus VX" />  
</p>
<p style="text-align: justify;">
In Part 3 I will cover the case when Cumulus VX virtual switch is playing the role of DC edge gateway as well as interconnect between underlay and overlay network. On Cumulus virtual switch two VxLAN tunnels will be terminated from remote servers.  
</p>

<p style="text-align: justify;">
Details on how to download Cumulus Linux and VPCS (Virtual PC Simulator) can be found in 
<a href="/vxlan/static-vxlan-part2/index.html">Part 2</a>.
Like in Part 1 and 2, physical server with VMs will be deployed as virtual machine on baremetal KVM host and will be represented as "srv" and "srv1". Nested VM (application) will be simulated with Linux network namespace. On Cumulus VX virtual switch will be used bridge, which is not VLAN aware, as both tunnels from srv and srv1 will be terminated inside that bridge. Details on how everything is interconnected you can find below on the network diagram.
</p>

<p align="center"> 
  <img src="/images/static_vxlan/net_diag3.jpg" alt="VxLAN, VTEP, Overlay Network" title="VxLAN Network diagram" /> 
</p>

<p><strong>Prerequisites for servers (virtual machines):</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">VM name</th>
      <th style="text-align: left">OS Version</th>
      <th style="text-align: left">Extra installed SW</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">srv</td>
      <td style="text-align: left">Ubuntu 20.04</td>
      <td style="text-align: left">bridge-utils</td>
    </tr>
    <tr>
      <td style="text-align: left">srv1</td>
      <td style="text-align: left">Ubuntu 20.04</td>
      <td style="text-align: left">bridge-utils</td>
    </tr>
    <tr>
      <td style="text-align: left">virtual switch</td>
      <td style="text-align: left">Cumulus Linux 4.4.1 VX (Nvidia)</td>
      <td style="text-align: left"> </td>
    </tr>
  </tbody>
</table>

<p><strong><span style="color:#074080">Step 1. Run VMs and install bridge-utils</span></strong></p>

<p>Let’s assume that virtual machine “srv” and “srv1” (Ubuntu), Cumulus VX vSwitch and VPCS are successfully deployed on KVM host.</p>

<p>Bridge-utils were installed on Ubuntu virtual machine using the command below:</p>

<p><code>apt install bridge-utils</code></p>

<p>version of bridge-utils we can check from CLI from srv and srv1 with the following command:</p>

<p><code>apt list | grep bridge-utils</code></p>

<p>output should be</p>

<p><strong><code>bridge-utils/focal,now 1.6-2ubuntu1 amd64 [installed]</code></strong></p>

<p>version can vary.</p>

<p><strong><span style="color:#074080">Step 2. Initial configuration of Linux srv, srv1  and Nvidia Cumulus VX virtual switch</span></strong></p>

<p><u>Apply configuration on srv</u></p>

<p>Creation of namespaces.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip netns add vm3 
</code></pre></div></div>
<p><u>Apply configuration on srv1.</u></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip netns add vm1
</code></pre></div></div>

<p>Creation of bridges on both hosts srv and srv1.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link add br-vxlan type bridge
ip link set br-vxlan up
ip link set mtu 9000 dev br-vxlan
</code></pre></div></div>
<p>Add static routes for Loopback IP on Cumulus VX switch on both hosts srv and srv1.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip route add 192.168.1.4/32 via 192.168.100.24 dev ens0
</code></pre></div></div>
<p>Disable spanning-tree on the bridges</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brctl stp br-vxlan off
</code></pre></div></div>
<p>Creation of veth interfaces on both host VMs.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link add veth0 type veth peer veth1
ip link set up veth0
ip link set veth0 master br-vxlan
</code></pre></div></div>
<p><u>Apply configuration on srv.</u><br />
Adding IP address on veth1 and bring interface up</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip netns exec vm3 ip a a 192.168.22.1/24 dev veth1
ip netns exec vm3 ip link set up veth1
</code></pre></div></div>
<p><u>Apply configuration on srv1.</u><br />
Adding IP address on veth1 and bring interface up.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip netns exec vm1 ip a a 192.168.22.2/24 dev veth1
ip netns exec vm1 ip link set up veth1
</code></pre></div></div>
<p><u>Configure Cumulus VX virtual switch.</u></p>

<p>When you connect for the first time to Cumulus VX console it will request you to change default password.</p>

<p><strong><p1><u>Default User and Password for Cumulus VX:</u></p1></strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>user: cumulus
pass: cumulus
</code></pre></div></div>

<p>Set hostname of Cumulus Linux virtual switch.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add hostname leaf1
</code></pre></div></div>
<p>Configure L3 interface for connection with underlay network.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add interface swp1 ip address 192.168.100.24/24
</code></pre></div></div>
<p>Configuration of Loopback interface, which will be used for VTEP termination point.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add loopback lo ip address 192.168.1.4/32
</code></pre></div></div>
<p>Add swp2 interface (please note interface will be used without VLAN id).</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add interface swp2 
</code></pre></div></div>
<p>Create bridge not VLAN aware.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add bridge br-12
</code></pre></div></div>
<p>Add interface swp2 to bridge.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add bridge br-12 ports swp2
</code></pre></div></div>

<p><strong><span style="color:#074080">Step 3. Configuration of  VxLAN Tunnel</span></strong></p>

<p><u>Apply on srv.</u></p>

<p>Create VxLAN tunnel interface with VNI 101.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link add vx1 type vxlan id 101 local 192.168.100.20 remote 192.168.1.4 dev ens0 dstport 4789
</code></pre></div></div>
<p>Bring up tunnel interface.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link set vx1 up
</code></pre></div></div>
<p>Add VxLAN tunnel interface to specific bridge.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link set vx1 master br-vxlan
</code></pre></div></div>
<p><u>Apply on srv1.</u></p>

<p>Create VxLAN tunnel interface with VNI 102.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link add vx1 type vxlan id 102 local 192.168.100.21 remote 192.168.1.4 dev ens0 dstport 4789
</code></pre></div></div>
<p>Bring up tunnel interface.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link set vx1 up
</code></pre></div></div>
<p>Add VxLAN tunnel interface to specific bridge.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link set vx1 master br-vxlan
</code></pre></div></div>
<p><u>Configure Cumulus VX virtual switch.</u></p>

<p><strong><p1><u>Default User and Password for Cumulus VX:</u></p1></strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>user: cumulus
pass: cumulus
</code></pre></div></div>
<p>Add vni-101 and vni-102 interfaces.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add vxlan vni-101 vxlan id 101
net add vxlan vni-102 vxlan id 102
</code></pre></div></div>
<p>Add VxLAN interfaces to the bridge</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add bridge br-12 ports vni-101
net add bridge br-12 ports vni-102
</code></pre></div></div>
<p>For both tunnels set local and remote VETP address</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add vxlan vni-101 vxlan local-tunnelip 192.168.1.4
net add vxlan vni-101 vxlan remoteip 192.168.100.20

net add vxlan vni-102 vxlan local-tunnelip 192.168.1.4
net add vxlan vni-102 vxlan remoteip 192.168.100.21
</code></pre></div></div>

<p><strong><span style="color:#074080">Step 4. Add static MAC record to forwarding database</span></strong></p>

<p><u>Apply on srv and srv1.</u></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bridge fdb append 00:00:00:00:00:00 dev vx1 dst 192.168.1.4
</code></pre></div></div>
<p>This step we can skip on Cumulus Linux virtual switch as it will be applied automatically.</p>

<p><strong><span style="color:#074080">Step 5. Connectivity check </span></strong></p>

<p>Run PING command from VM1, VM3 and DB server (VPCS)
<img style="center" src="/images/static_vxlan/con_check_vm1_vm3_db_cumulus.jpg" alt="Ping from namespace" title="Connectivity check with PING command" /></p>

<p><u>Issue ping from VM3 (namespace)</u></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo ip netns exec vm3 ping 192.168.22.4 -- &gt; remote IP on DB server.
</code></pre></div></div>

<p><img style="float" width="750" height="250" src="/images/static_vxlan/vm3_ping_db_part3.jpg" alt="L2 frame, VxLAN, wireshark, packet capture, pcap" title="Wireshark VxLAN packet capture" /></p>

<p><u>Issue ping from VM1 (namespace)</u></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo ip netns exec vm1 ping 192.168.22.4 -- &gt; remote IP on DB server.
</code></pre></div></div>
<p><img style="float" width="750" height="250" src="/images/static_vxlan/vm1_ping_db.jpg" alt="L2 frame, VxLAN, wireshark, packet capture, pcap" title="Wireshark VxLAN packet capture" /></p>

<p>From the capture you can see that two VxLAN tunnels have been used for interconnection with Cumulus VX virtual switch. Both tunnels landed on to one bridge on Cumulus VX virtual switch. That way we achieve communication between two VMs and DB server (VPCS).</p>

<p><u>Issue ping from DB server (VPCS) to check connectivity with VM1 and VM3</u></p>

<p><img style="center" width="750" height="250" src="/images/static_vxlan/db_srv_ping_vm3_pcap_part3.jpg" alt="Ping from VPCS" title="Connectivity check with PING command" /></p>

<p><br />
<strong><a href="/vxlan/static_vxlan_part1/index.html" target="_blank">Part 1 Static VxLAN between Ubuntu hosts</a></strong><br />
<strong><a href="/vxlan/static-vxlan-part2/index.html" target="_blank">Part 2 Static VxLAN between Ubuntu and Cumulus VX vSwicth</a></strong><br />
<strong><a href="/vxlan/static-vxlan-part4/index.html" target="_blank">Part 4 Static VxLAN Data Center Interconnect</a></strong><br />
<br /></p>]]></content><author><name></name></author><category term="VxLAN" /><summary type="html"><![CDATA[In Part 3 I will cover the case when Cumulus VX virtual switch is playing the role of DC edge gateway as well as interconnect between underlay and overlay network. On Cumulus virtual switch two VxLAN tunnels will be terminated from remote servers.]]></summary></entry><entry><title type="html">Five easy steps to configure static VxLAN Part 2</title><link href="http://0.0.0.0:4000/vxlan/static-vxlan-part2/" rel="alternate" type="text/html" title="Five easy steps to configure static VxLAN Part 2" /><published>2021-10-31T00:00:00+00:00</published><updated>2021-10-31T00:00:00+00:00</updated><id>http://0.0.0.0:4000/vxlan/static-vxlan-part2</id><content type="html" xml:base="http://0.0.0.0:4000/vxlan/static-vxlan-part2/"><![CDATA[<style>
p1 {
  font-size: 11px;
}
p2 {
  font-size: 12px;
}
</style>

<p align="center">
<img src="/images/static_vxlan/vxlan_tunnel2.jpg" alt="Overlay VxLAN Cumulus Linux" title="VxLAN between Ubuntu and Cumulus VX" />  
</p>
<p style="text-align: justify;">
In Part 2 will be covered the case when we have physical DB server connected to network via L2/L3 switch and it should communicate with application running inside virtual machine. Nvidia Cumulus VX virtual switch will play the role of L2/L3 switch. On Cumulus virtual switch VxLAN tunnel be terminated from remote server.
</p>

<p style="text-align: justify;">
To download Nvidia Cumulus VX virtual switch first you need to register. The procedure is very friendly and corporate email address is not required. Nvidia didn't limit bandwidth or some features of virtual switch. Cumulus Linux VX (virtual switch) does not require any license, which is really great for labs, PoC and tests.
<a href="https://www.nvidia.com/en-us/networking/ethernet-switching/cumulus-vx/download/" target="_blank">Click here to download Nvidia Cumulus VX virtual switch</a>
DB server will be simulated with VPCS (Virtual PC Simulator). VPCS is actually application with few command line utilities such as ping, ip interface configuration etc. It can be run on Linux or Windows OS. Instructions on how to use and run VPCS you can find on the following link  <a href="https://wiki.freecode.com.cn/doku.php?id=wiki:vpcs" target="_blank">click here</a>.
Like in Part 1, physical server with VMs will be deployed as virtual machine on baremetal KVM host and will be represented as "srv". Nested VM (application) will be simulated with Linux network namespace. Details on how everything is interconnected you can find below on the network diagram.
</p>

<p align="center">
  <img src="/images/static_vxlan/net_diag2.jpg" alt="VxLAN, VTEP, Overlay Network" title="VxLAN Network diagram" /> 
</p>

<p><strong>Prerequisites for servers (virtual machines):</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">VM name</th>
      <th style="text-align: left">OS Version</th>
      <th style="text-align: left">Extra installed SW</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">srv</td>
      <td style="text-align: left">Ubuntu 20.04</td>
      <td style="text-align: left">bridge-utils</td>
    </tr>
    <tr>
      <td style="text-align: left">switch</td>
      <td style="text-align: left">Cumulus Linux 4.4.1 VX (Nvidia)</td>
      <td style="text-align: left"> </td>
    </tr>
  </tbody>
</table>

<p><strong><span style="color:#074080">Step 1. Run VMs and install bridge-utils</span></strong></p>

<p>Let’s assume that virtual machine “srv” (Ubuntu), Cumulus VX vSwitch and VPCS are successfully deployed on KVM host.</p>

<p>Bridge-utils were installed on Ubuntu virtual machine using the command below:</p>

<p><code>apt install bridge-utils</code></p>

<p>version of bridge-utils we can check from CLI from srv and srv1 with the following command:</p>

<p><code>apt list | grep bridge-utils</code></p>

<p>output should be</p>

<p><strong><code>bridge-utils/focal,now 1.6-2ubuntu1 amd64 [installed]</code></strong></p>

<p>version can vary.</p>

<p><strong><span style="color:#074080">Step 2. Initial configuration of Linux srv and Nvidia Cumulus VX virtual switch</span></strong></p>

<p><u>Apply configuration on srv</u></p>

<p>Creation of namespaces.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip netns add vm3 
</code></pre></div></div>
<p>Creation of bridges on host srv.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link add br-vxlan type bridge
ip link set br-vxlan up
ip link set mtu 9000 dev br-vxlan
</code></pre></div></div>
<p>Disable spanning-tree on the bridges</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brctl stp br-vxlan off
</code></pre></div></div>
<p>Creation of veth interfaces on host srv.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link add veth0 type veth peer veth1
ip link set up veth0
ip link set veth0 master br-vxlan
</code></pre></div></div>
<p>Adding IP address on veth1 and bring interface up</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip netns exec vm3 ip a a 192.168.22.1/24 dev veth1
ip netns exec vm3 ip link set up veth1
</code></pre></div></div>
<p><u>Configure Cumulus VX virtual switch.</u></p>

<p>When you connect for the first time to Cumulus VX console it will request you to change default password.</p>

<p><strong><p1><u>Default User and Password for Cumulus VX:</u></p1></strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>user: cumulus
pass: cumulus
</code></pre></div></div>
<p align="left">
  <img src="/images/static_vxlan/cumulus_login1.jpg" width="450" height="200 " alt="VxLAN, VTEP, Overlay Network" title="Cumulus ZTP" /> 
</p>
<p>When you log in for the first time on Cumulus VX virtual switch, you will need to stop ZTP - zero touch provisioning process. Please check below:</p>

<p align="left">
  <img src="/images/static_vxlan/cumulus_login2.jpg" width="450" height="150 " alt="VxLAN, VTEP, Overlay Network" title="Disable Cumulus ZTP" /> 
</p>

<p>Set hostname of Cumulus Linux virtual switch.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add hostname leaf1
</code></pre></div></div>
<p>Configure L3 interface for connection with underlay network.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add interface swp1 ip address 192.168.100.24/24
</code></pre></div></div>
<p>Configuration of Loopback interface, which will be used for VTEP termination point.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add loopback lo ip address 192.168.1.4/32
</code></pre></div></div>
<p>Add swp2 interface to VLAN 101.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add interface swp2 bridge access 101
</code></pre></div></div>
<p>Create bridge VLAN aware (Recommended by Cumulus network team).</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add bridge bridge vlan-aware
</code></pre></div></div>
<p>Add port swp2 to bridge.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add bridge bridge ports swp2
</code></pre></div></div>
<p>Configure bridge VLAN id.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add bridge bridge vids 101
</code></pre></div></div>

<p><strong><span style="color:#074080">Step 3. Configuration of  VxLAN Tunnel</span></strong></p>

<p><u>Apply on srv.</u></p>

<p>Create VxLAN tunnel interface with VNI 101.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link add vx1 type vxlan id 101 local 192.168.100.20 remote 192.168.1.4 dev ens0 dstport 4789
</code></pre></div></div>
<p>Bring up tunnel interface.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link set vx1 up
</code></pre></div></div>
<p>Add VxLAN tunnel interface to specific bridge.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link set vx1 master br-vxlan
</code></pre></div></div>
<p><u>Apply on Cumulus Linux virtual switch.</u></p>

<p>Create VxLAN tunnel interface with VNI 101.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add vxlan vni-101 vxlan id 101
</code></pre></div></div>
<p>Allow VLAN id 101 for vni-101 interface.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add vxlan vni-101 bridge access 101
</code></pre></div></div>
<p>BPDU guard and port BPDU filter will be added automatically.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add vxlan vni-101 stp bpduguard
net add vxlan vni-101 stp portbpdufilter
</code></pre></div></div>
<p>Configure local VTEP, local Loopback0.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add vxlan vni-101 vxlan local-tunnelip 192.168.1.4
</code></pre></div></div>
<p>Configure remote VTEP, IP address of remote server.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add vxlan vni-101 vxlan remoteip 192.168.100.20
</code></pre></div></div>
<p>Add VxLAN interface to the bridge.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add bridge bridge ports vni-101
</code></pre></div></div>

<p><strong><span style="color:#074080">Step 4. Add static MAC record to forwarding database</span></strong></p>

<p><u>Apply on srv.</u></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bridge fdb append 00:00:00:00:00:00 dev vx0 dst 192.168.1.4
</code></pre></div></div>
<p>This step we can skip on Cumulus Linux virtual switch as it will be applied automatically.</p>

<p><strong><span style="color:#074080">Step 5. Connectivity check </span></strong></p>

<p>Run PING command from both side (namespace VM3 and VPCS DB server).
<img style="center" src="/images/static_vxlan/vm3_ping_db_srv.jpg" alt="Ping from namespace" title="Connectivity check with PING command" /></p>

<p><u>Issue ping from VM3 (namespace)</u></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo ip netns exec vm3 ping 192.168.22.4 -- &gt; remote IP on DB server.
</code></pre></div></div>
<p><u>Issue ping from DB server (VPCS)</u></p>

<p><img style="center" src="/images/static_vxlan/ping_db_srv_vm3.jpg" alt="Ping from VPCS" title="Connectivity check with PING command" /></p>

<p>So, let’s check packet capture:
<img style="float" width="750" height="400" src="/images/static_vxlan/db_srv_ping_vm3_pcap.jpg" alt="L2 frame, VxLAN, wireshark, packet capture, pcap" title="Wireshark VxLAN packet capture" /></p>

<p><br />
<strong><a href="/vxlan/static_vxlan_part1/index.html" target="_blank">Part 1 Static VxLAN between Ubuntu hosts</a></strong><br />
<strong><a href="/vxlan/static-vxlan-part3/index.html" target="_blank">Part 3 Static VxLAN between Ubuntu Hosts and Cumulus VX vSwitch DC Gateway</a></strong><br />
<strong><a href="/vxlan/static-vxlan-part4/index.html" target="_blank">Part 4 Static VxLAN Data Center Interconnect</a></strong><br />
<br /></p>]]></content><author><name></name></author><category term="VxLAN" /><summary type="html"><![CDATA[In Part 2 will be covered the case when we have physical DB server connected to network via L2/L3 switch and it should communicate with application running inside virtual machine. Nvidia Cumulus VX virtual switch will play the role of L2/L3 switch. On Cumulus virtual switch VxLAN tunnel be terminated from remote server.]]></summary></entry><entry><title type="html">Five easy steps to configure static VxLAN Part 1</title><link href="http://0.0.0.0:4000/vxlan/static_vxlan_part1/" rel="alternate" type="text/html" title="Five easy steps to configure static VxLAN Part 1" /><published>2021-10-01T00:00:00+00:00</published><updated>2021-10-01T00:00:00+00:00</updated><id>http://0.0.0.0:4000/vxlan/static_vxlan_part1</id><content type="html" xml:base="http://0.0.0.0:4000/vxlan/static_vxlan_part1/"><![CDATA[<p align="center">
<img src="/images/static_vxlan/vxlan_tunnel.jpg" alt="VxLAN Overlay Ubuntu Intro" title="Introduction to VxLAN" /> 
</p>
<p>In this blog post I will cover static VxLAN tunnels, it means no multicast flood and learning or BGP with EVPN signalling.
Part 1 will cover “Static VxLAN tunnels between two Linux hosts” and you could have guessed already that there will be Part 2 and Part 3, or even more, who knows.
Future subjects of blog posts will be kept secret for now.😉</p>

<h3 id="introduction-to-vxlan---virtual-extensible-lan"><span style="color:#074080">Introduction to VxLAN - Virtual Extensible LAN</span></h3>
<p style="text-align: justify;">
I will try not to bother you with heaps of theory that you can find in the books or on the Internet. However I will mention some important facts about VxLAN.
So, What is VxLAN? It is just yet another encapsulation, which allows you transporting L2 frames over L3 network and that way extending L2 domain. Moreover, it provides flexibility and scalability and solve limitations of 4096 vlans. Also it introduces overhead of 16 bytes (UDP 8 bytes + VxLAN header 8 Bytes). So because of that the underlay network should support MTU size 1600 bytes, not less.
VxLAN adds header to original L2 frame and encapsulates all to UDP and transport via underlay IP network.
VTEPs (VxLAN Tunnel Endpoint) are the points on network (server) nodes where traffic will be packed to encapsulation and on remote point unpack and send to final destination. 
</p>
<p>Below you can see L2 frame before and after encapsulation to VxLAN.</p>
<p align="center">
  <img src="/images/static_vxlan/vxlan.jpg" alt="L2 frame, VxLAN" title="L2 Frame inside VxLAN" />
</p>

<p>It’s time to stop talking and start lab configuration.</p>

<h3 id="part-1-static-vxlan-between-two-linux-hosts"><span style="color:#074080">Part 1 Static VxLAN between two Linux hosts</span></h3>
<p>In Part 1 I will cover the case when we have two physical servers and the requirement is to connect virtual machines to one subnet.
For the sake of simplicity, physical servers will be deployed as virtual machines on baremetal KVM host and they will represented as “srv” and “srv1”. Nested VMs will be simulated with Linux network namespaces. This way we can save hardware resources on KVM host. Details on how everything is interconnected you can find below on the network diagram.</p>

<p align="center">
  <img src="/images/static_vxlan/net_diag.jpg" alt="VxLAN, VTEP, Overlay Network" title="Network diagram VxLAN between Ubuntu KVM hosts" />
</p>

<p><strong>Prerequisites for servers (virtual machines):</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">VM name</th>
      <th style="text-align: left">OS Version</th>
      <th style="text-align: left">Extra installed SW</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">srv</td>
      <td style="text-align: left">Ubuntu 20.04</td>
      <td style="text-align: left">bridge-utils</td>
    </tr>
    <tr>
      <td style="text-align: left">srv1</td>
      <td style="text-align: left">Ubuntu 20.04</td>
      <td style="text-align: left">bridge-utils</td>
    </tr>
  </tbody>
</table>

<p><strong><span style="color:#074080">Step 1. Run VMs and install bridge-utils</span></strong></p>

<p>Let’s assume that virtual machines “srv” and “srv1” are successfully deployed on KVM host and bridge-utils were installed on both VMs using the command below:</p>

<p><code>apt install bridge-utils</code></p>

<p>version of bridge-utils we can check from CLI from srv and srv1 with the following command:</p>

<p><code>apt list | grep bridge-utils</code></p>

<p>output should be</p>

<p><strong><code>bridge-utils/focal,now 1.6-2ubuntu1 amd64 [installed]</code></strong></p>

<p>version can vary.</p>

<p><strong><span style="color:#074080">Step 2. Create and configure namespaces, bridges and veth interfaces pairs </span></strong></p>

<p>Creation of namespaces.</p>

<p><u>Apply configuration on srv.</u></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip netns add vm3 
</code></pre></div></div>
<p><u>Apply configuration on srv1.</u></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip netns add vm1
</code></pre></div></div>

<p>Creation of bridges on both host VMs.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link add br-vxlan type bridge
ip link set br-vxlan up
ip link set mtu 9000 dev br-vxlan
</code></pre></div></div>
<p>Disable spanning-tree on the bridges</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brctl stp br-vxlan off
</code></pre></div></div>
<p>Creation of veth interfaces on both host VMs.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link add veth0 type veth peer veth1
ip link set up veth0
ip link set veth0 master br-vxlan
</code></pre></div></div>
<p><u>Apply configuration on srv.</u><br />
Adding IP address on veth1 and bring interface up</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip netns exec vm3 ip a a 192.168.22.1/24 dev veth1
ip netns exec vm3 ip link set up veth1
</code></pre></div></div>
<p><u>Apply configuration on srv1.</u><br />
Adding IP address on veth1 and bring interface up.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip netns exec vm1 ip a a 192.168.22.2/24 dev veth1
ip netns exec vm1 ip link set up veth1
</code></pre></div></div>

<p><strong><span style="color:#074080">Step 3. Configuration of  VxLAN Tunnel</span></strong></p>

<p><u>Creation of VxLAN tunnel interface vx0 on srv and srv1.</u></p>

<p>Create VxLAN tunnel interface with VNI 100.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link add vx0 type vxlan id 100 local 192.168.100.20 remote 192.168.100.21 dev ens0 dstport 4789
</code></pre></div></div>
<p>Add IP address on tunnel and bring up interface.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip a a 192.168.1.1/24 dev vx0
ip link set vx0 up
</code></pre></div></div>
<p>Add VxLAN tunnel interface to specific bridge.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link set vx0 master br-vxlan
</code></pre></div></div>
<p><u>Apply configuration on srv1.</u></p>

<p>Create VxLAN tunnel interface with VNI 100.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link add vx0 type vxlan id 100 local 192.168.100.21 remote 192.168.100.20 dev ens0 dstport 4789
</code></pre></div></div>
<p>Add IP address on tunnel and bring up interface.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip a a 192.168.1.2/24 dev vx0
ip link set vx0 up
</code></pre></div></div>
<p>Add VxLAN tunnel interface to specific bridge.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip link set vx0 master br-vxlan
</code></pre></div></div>

<p><strong><span style="color:#074080">Step 4. Add static MAC record to forwarding database</span></strong></p>

<p><u>Apply configuration on srv.</u></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bridge fdb append 00:00:00:00:00:00 dev vx0 dst 192.168.100.21
</code></pre></div></div>

<p><u>Apply configuration on srv1.</u></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bridge fdb append 00:00:00:00:00:00 dev vx0 dst 192.168.100.20
</code></pre></div></div>

<p><strong><span style="color:#074080">Step 5. Connectivity check </span></strong></p>

<p>Run PING command from both VMs (namespaces).
<img style="center" src="/images/static_vxlan/vm1_ping_vm3.jpg" alt="Ping from namespace" title="Connectivity check with PING command" /></p>

<p><u>Issue ping from VM3 (namespace)</u></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo ip netns exec vm3 ping 192.168.22.2 -- &gt; remote IP inside namespace VM1
</code></pre></div></div>
<p><u>Issue ping from VM1 (namespace)</u></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo ip netns exec vm1 ping 192.168.22.1 -- &gt; remote IP inside namespace VM3
</code></pre></div></div>
<p>Below is capture which is prove that all ICMP packets (request, reply) were encapsulated to VxLAN.
<img style="float" width="750" height="400" src="/images/static_vxlan/vm3_ping_vm1_pcap.jpg" alt="L2 frame, VxLAN, wireshark, packet capture, pcap" title="Wireshark VxLAN packet capture" /></p>

<p><br />
<strong><a href="/vxlan/static-vxlan-part2/index.html" target="_blank">Part 2 Static VxLAN between Ubuntu and Cumulus VX vSwicth</a></strong><br />
<strong><a href="/vxlan/static-vxlan-part3/index.html" target="_blank">Part 3 Static VxLAN between Ubuntu Hosts and Cumulus VX vSwitch DC Gateway</a></strong><br />
<strong><a href="/vxlan/static-vxlan-part4/index.html" target="_blank">Part 4 Static VxLAN Data Center Interconnect</a></strong><br />
<br /></p>]]></content><author><name></name></author><category term="VxLAN" /><summary type="html"><![CDATA[In this blog post I will cover static VxLAN tunnels, it means no multicast flood and learning or BGP with EVPN signalling. Part 1 will cover “Static VxLAN tunnels between two Linux hosts” and you could have guessed already that there will be Part 2 and Part 3, or even more, who knows. Future subjects of blog posts will be kept secret for now.😉]]></summary></entry></feed>