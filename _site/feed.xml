<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2025-10-19T09:19:53+00:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Tomislav Kranjec’s Blog</title><subtitle>| Service Provider | Data Center | Cloud | Virtualization | SDN/NFV |
</subtitle><entry><title type="html">Understanding Contracts in Cisco ACI: The Backbone of Network Security</title><link href="http://0.0.0.0:4000/cisco%20aci/aci-contracts/" rel="alternate" type="text/html" title="Understanding Contracts in Cisco ACI: The Backbone of Network Security" /><published>2025-10-05T00:00:00+00:00</published><updated>2025-10-05T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cisco%20aci/aci-contracts</id><content type="html" xml:base="http://0.0.0.0:4000/cisco%20aci/aci-contracts/"><![CDATA[<p align="center">
<img src="/images/aci_contracts/1.png" alt="ACI-top" title="ACI-top" /> 
</p>

<p>We recently explored the foundational elements of Cisco ACI, diving into how Tenants, VRFs, 
Bridge Domains, and EPGs serve as the logical building blocks for organizing your network. 
We saw how these components create a structured environment for applications, but that raises a critical question, how do these EPGs actually talk to each other securely?
<!-- excerpt --></p>
<p>
How do you define precisely what traffic is allowed, and what is denied, between different application tiers or external networks? This is where Cisco ACI Contracts come in. They are the core enforcement mechanism for defining communication policies and building a 
zero-trust security model within your ACI fabric.
</p>

<p>In this post, we’ll cover the different types of contracts and policy enforcement options:</p>

<p><strong><a href="#section1">1. Standard Contracts: Building the ACI Security Whitelist</a></strong>  <br />
<strong><a href="#section2">2. Taboo Contracts: ACI’s Blacklist for Overriding Security Policies.</a></strong><br />
<strong><a href="#section3">3. vzAny Contracts: The Shortcut to allow communication inside VRF.</a></strong><br />
<strong><a href="#section4">4. Preferred Groups: When to Unenforce Policy in ACI.</a></strong><br />
<strong><a href="#section5">5. Unenforced VRFs: The Full Open Communication inside VRF.</a></strong></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section1">1. Standard Contracts: Building the ACI Security Whitelist.</span></strong></p>
<div style="text-align: justify;">
The Contract is the core policy object in Cisco ACI that enables inter-EPG communication based on a provider-consumer model. It defines the exact, permitted traffic flows, serving as the explicit allow-list that overrides the fabric's default deny-all. A standard contract is composed of several key components:  
</div>

<ol>
  <li>
    <p><strong>Filters</strong> are the most granular rules, specifying permitted traffic based on network attributes such as EtherType, IP, ICMPv4 and v6 Source and Destination Port/Range.</p>
  </li>
  <li>
    <p><strong>Subjects</strong> are Logical containers for Filters that define the directionality of the traffic flow. It also defines the directionality of the communication using options like “Apply Both Directions” and “Reverse Filter Ports” these “Subject” options are responsible for automatically creating the necessary return path TCAM entry (ACL rule) for a connection.</p>
  </li>
  <li><strong>Scopes</strong> Settings (Application Profile, VRF, Tenant, and Global) that establish the contract’s domain, controlling how widely it can be shared and reused across the ACI environment.
    <ul>
      <li><strong>Application Profile scope -</strong> communication policies that are strictly internal to a single application, ensuring that the contract is not accidentally used by other applications, even in the same VRF.</li>
      <li><strong>VRF scope -</strong> The default and most common scope for defining communication policies between EPGs that share a Layer 3 forwarding domain. This scope is used for most intra-VRF contracts.</li>
      <li><strong>Tenant Scope -</strong> Primarily for Inter-VRF (route leaking) communication when both VRFs are in the same tenant. It allows EPGs in one VRF to consume a contract provided by an EPG in a different VRF, as long as they are in the same tenant.</li>
      <li><strong>Global scope -</strong> Inter-Tenant contracts, where the provider and consumer are in different tenants. For contracts defined in the Common Tenant that need to be universally available (e.g., contracts for shared services like DNS/NTP). or Inter-VRF communication where the VRFs are in different tenants.</li>
    </ul>
  </li>
  <li><strong>Action</strong> specifies whether to permit, deny, or log the traffic (deny actions are supported in APIC Release 3.2(x) and later with EX/FX switches).</li>
</ol>

<p>As many are still struggling with ACI contract direction (Consumer to Provider and Provider to Consumer), let me reveal one secret:</p>
<ul>
  <li><strong>Consumer is the SOURCE</strong></li>
  <li><strong>Provider is the DESTINATION</strong></li>
</ul>

<p>Apply your policies using this simple traffic flow rule, and you’ll get it right every time!</p>

<p>Below figures 1 through 3 illustrate the operational behavior of the “Apply Both Directions” and “Reverse Filter Ports” settings.</p>

<p><img src="/images/aci_contracts/1.gif" alt="ACI-Contracts1" title="ACI-Contracts1" /></p>

<p>Figure 1. Applying contract between two EPGs.</p>

<p>The Consumer EPG can only initiate a new session with the Provider EPG on the specified port, which in this case is TCP 22 for SSH.<br />
However, communication won’t be established because the contract is currently applied only in the <strong>Consumer-to-Provider</strong> direction, causing the necessary return traffic to be dropped. Therefore, we have two options:</p>
<ul>
  <li>either create a second contract in the <strong>Provider-to-Consumer</strong> direction or</li>
  <li>use the “Apply Both Directions” option with “Reverse Filter Ports.”</li>
</ul>

<p>Since contracts utilize hardware resources on the leaf switches, I recommend using “Apply Both Directions” with “Reverse Filter Ports.”</p>

<p><img src="/images/aci_contracts/2.gif" alt="ACI-Contracts1" title="ACI-Contracts1" /></p>

<p>Figure 2. Feature “Apply Both Direction” Enabled.</p>

<p>With “Apply Both Directions” (Option ON):</p>
<ul>
  <li>The Consumer EPG can initiate a connection to the Provider EPG.</li>
  <li>The Provider EPG can also initiate a new, separate connection back to the Consumer EPG on the same port(s).</li>
</ul>

<p>Based on Figure 2. SRV1 and SRV2 can open SSH session to DB1 and DB2 and vice versa DB1 and DB2 can open SSH session to SRV1 and SRV2 but communication won’t be established.</p>

<p><img src="/images/aci_contracts/3.gif" alt="ACI-Contracts1" title="ACI-Contracts1" /></p>

<p>Figure 3. Representing establishment of SSH session from Jumphost (SRV1 and SRV2) to DB servers.</p>

<p><strong>Note:</strong><br />
Contracts can be applied only on unicast traffic. Broadcast, Unknown unicast and Multicast Protocols (BUM) traffic and below listed protocols are implicitly permitted.</p>
<ul>
  <li>ARP reply (unicast)</li>
  <li>DHCP v4 (prot 0x11, sport 0x44, dport 0x43)</li>
  <li>DHCP v4 (prot 0x11, sport 0x43, dport 0x44)</li>
  <li>DHCP v6 (prot 0x11, sport 0x222, dport 0x223)</li>
  <li>OSPF (prot 0x59)</li>
  <li>EIGRP (prot 0x58)</li>
  <li>PIM (prot 0x67)</li>
  <li>IGMP (prot 0x2)</li>
  <li>ND-Sol ICMPv6 (prot 0x3a dport 0x0087)</li>
  <li>ND-Advt ICMPv6 (prot 0x3a dport 0x0088)</li>
</ul>

<p>Moreover, in case of Multi-Site deployment, Cisco Multi-Site Orchestrator (MSO) creates one contract for each subject.</p>

<div style="text-align: justify;">
</div>
<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section2">2. Taboo Contracts: ACI’s Blacklist for Overriding Security Policies.</span></strong></p>

<p>Taboo contracts are specialized contracts used to explicitly deny specific traffic for a single EPG. Unlike standard contracts, they are:</p>
<ul>
  <li><strong>Unidirectional -</strong> Always provided by the EPG, with no consumer relationship.</li>
  <li><strong>Higher Priority -</strong> Evaluated before standard contracts in the hardware’s Policy CAM.</li>
  <li><strong>Limited Scope -</strong> Applied to a single EPG to block specific traffic, such as insecure protocols.</li>
  <li><strong>Taboo contracts</strong> are not supported with Endpoint Security Groups (ESGs).</li>
  <li><strong>They cannot be applied to vzAny.</strong></li>
  <li><strong>Cisco best practices</strong> generally advise against its regular use in production environments and suggest to use standard contracts with deny action if it is needed.</li>
</ul>

<p><strong><ins>Example Use Case:</ins></strong></p>

<ol>
  <li>You want to block HTTP (port 80) traffic to Web_EPG while allowing other traffic via standard contracts.</li>
  <li>Create a taboo contract Deny_HTTP_Taboo with a filter for TCP port 80 and apply it as provided by Web_EPG.</li>
</ol>

<p><a href="/images/aci_contracts/5.png" target="new" title="click here to see the full sized image"><img src="/images/aci_contracts/5.png" alt="ACI-Taboo" /></a></p>

<p>Figure 4. A Taboo-Contract blocking access to HTTP (insecure TCP port 80).</p>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section3">3. vzAny Contracts: The Shortcut to allow communication inside VRF and more.</span></strong></p>

<p>vzAny is a powerful construct that simplifies the application of security policies across all Endpoint Groups (EPGs) and Endpoint Security Groups (ESGs) within a Virtual Routing and Forwarding (VRF) instance.</p>

<p><strong>Key characteristics of vzAny:</strong></p>

<p><strong>1. VRF-Specific -</strong> vzAny is scoped to a single VRF and cannot directly apply to EPGs/ESGs in other VRFs.<br />
<strong>2. Contract Simplification -</strong> Instead of applying a contract to each EPG/ESG individually, vzAny applies it to all EPGs/ESGs in the VRF simultaneously. <strong>Remember vzAny representing all EPGs inside VRF.</strong><br />
<strong>3. Provider or Consumer Role -</strong> vzAny can act as a contract provider (offering services) or consumer (accessing services), depending on the configuration.<br />
<strong>4. Standard Contracts Only -</strong> vzAny supports standard contracts but not taboo contracts, which are limited to individual EPGs.</p>

<p><img src="/images/aci_contracts/2.png" alt="ACI-vzAny2" title="ACI-vzAny2" /></p>

<p>Figure 5. Representation of applying the contract per EPG.</p>

<p><img src="/images/aci_contracts/3.png" alt="ACI-vzAny3" title="ACI-vzAny3" /></p>

<p>Figure 6. Representing of applying the contract to vzAny.</p>

<p>Under the “Application Profile” and “Topology” tabs, you can view the graphical representation of the relationship between EPGs and Contracts.
Below Figure 6 illustrates how DNS and NTP services are shared for all EPGs within the VRF using vzAny.</p>

<p><strong>Note</strong> the checkbox on the right, “Show VRF in EPG,” which helps us identify which VRF a particular EPG belongs to.</p>

<p><a href="/images/aci_contracts/4.png" target="new" title="click here to see the full sized image"><img src="/images/aci_contracts/4.png" alt="ACI-vzAny4" /></a></p>

<p>Figure 7. Contract relationship: vzAny consumes services provided by DNS and NTP EPGs.</p>

<p><strong>Please Note:</strong>
External EPGs that are associated with L3Outs and are part of a VRF are also included in the vzAny logical group.</p>

<p><strong>Limitations of vzAny:</strong><br />
<strong>1. VRF Scope -</strong> vzAny is limited to a single VRF. It cannot directly apply contracts to EPGs/ESGs in other VRFs without additional configuration (e.g., contract export or shared services).<br />
<strong>2. No Taboo Contracts -</strong> vzAny does not support taboo contracts, which are used to deny specific traffic for individual EPGs.<br />
<strong>3. Over-Permissiveness Risk -</strong> Applying contracts to vzAny affects all EPGs/ESGs in the VRF, which may lead to unintended traffic flows if not carefully planned.<br />
<strong>4. Not for Granular Control -</strong> For specific EPG-to-EPG policies, individual contracts are preferred over vzAny.</p>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section4">4. Preferred Groups: When to Unenforce Policy in ACI.</span></strong></p>

<p>A Preferred Group is a VRF-level configuration that designates a subset of EPGs or ESGs within a VRF as members of a group that can communicate with each other without contracts. When an EPG or ESG is added to the Preferred Group, it can send and receive traffic to/from other Preferred Group members within the same VRF, bypassing the need for standard contracts, provided the VRF is in enforced mode. I would like to mentioned that only one group can exist per VRF.</p>

<p><strong>Limitations of Preferred Groups:</strong></p>
<ol>
  <li>Preferred Groups are limited to a single VRF.</li>
  <li>No Taboo Contract Support: Taboo contracts, which explicitly deny traffic for individual EPGs, cannot be overridden by Preferred Groups. If a taboo contract is applied to an EPG, it takes precedence.</li>
  <li>All-or-Nothing Communication, preferred Group members can communicate freely with each other for all traffic types.</li>
  <li>Not for External Connectivity: Preferred Groups do not apply to L3Out external EPGs for external connectivity; contracts are still required.</li>
  <li>Policy CAM Usage: Preferred Group rules consume Policy CAM on leaf switches, which can impact scalability in large deployments.</li>
  <li>If the VRF is unenforced, contracts are bypassed, and the Preferred Group has no effect (all EPGs/ESGs can communicate freely).</li>
</ol>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section5">5. Unenforced VRFs: The Full Open Communication inside VRF.</span></strong></p>

<p>An unenforced VRF is a VRF configuration in Cisco ACI where the policy enforcement is disabled.</p>

<p><strong>In this mode:</strong></p>
<ol>
  <li>All EPGs and ESGs within the VRF can communicate with each other without requiring contracts.</li>
  <li>The unenforced setting applies only to intra-VRF traffic; inter-VRF communication still requires contracts and route leaking.</li>
  <li>Traffic flows freely between endpoints within the VRF.</li>
  <li>Contracts, including standard and taboo contracts, are bypassed for intra-VRF communication.</li>
  <li>Features like Preferred Groups and vzAny become irrelevant in an unenforced VRF, as all traffic is permitted.</li>
</ol>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Feature</th>
      <th style="text-align: left">Preferred group</th>
      <th style="text-align: left">vzAny</th>
      <th>Unenforced VRF</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Scope</td>
      <td style="text-align: left">Specific EPGs/ESGs in a VRF</td>
      <td style="text-align: left">All EPGs/ESGs in a VRF</td>
      <td>All EPGs/ESGs in a VRF</td>
    </tr>
    <tr>
      <td style="text-align: left">Contract Requirement</td>
      <td style="text-align: left">No contracts needed between members.</td>
      <td style="text-align: left">Contracts required (provider/consumer)</td>
      <td>No contracts needed for any traffic</td>
    </tr>
    <tr>
      <td style="text-align: left">Granularity</td>
      <td style="text-align: left">Selective (only members communicate freely).</td>
      <td style="text-align: left">Applies to all EPGs/ESGs via contracts.</td>
      <td>No control (all traffic allowed)</td>
    </tr>
    <tr>
      <td style="text-align: left">Inter-VRF Support</td>
      <td style="text-align: left">No (requires contracts for inter-VRF).</td>
      <td style="text-align: left">Supports inter-VRF with contract export.</td>
      <td>No (VRF isolation still applies)</td>
    </tr>
    <tr>
      <td style="text-align: left">Use Case</td>
      <td style="text-align: left">Controlled open communication within VRF.</td>
      <td style="text-align: left">Simplified contract application.</td>
      <td>Fully open VRF (less secure)</td>
    </tr>
  </tbody>
</table>

<p>Table 1. Representing comparison Preferred Groups vs. vzAny vs. Unenforced VRF.</p>

<p><a href="#top">Back to Top</a></p>]]></content><author><name></name></author><category term="Cisco ACI" /><summary type="html"><![CDATA[We recently explored the foundational elements of Cisco ACI, diving into how Tenants, VRFs, Bridge Domains, and EPGs serve as the logical building blocks for organizing your network. We saw how these components create a structured environment for applications, but that raises a critical question, how do these EPGs actually talk to each other securely?]]></summary></entry><entry><title type="html">Cisco ACI’s Core Building Blocks: Tenants to EPGs Explained</title><link href="http://0.0.0.0:4000/cisco%20aci/aci-core-build/" rel="alternate" type="text/html" title="Cisco ACI’s Core Building Blocks: Tenants to EPGs Explained" /><published>2025-06-16T00:00:00+00:00</published><updated>2025-06-16T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cisco%20aci/aci-core-build</id><content type="html" xml:base="http://0.0.0.0:4000/cisco%20aci/aci-core-build/"><![CDATA[<p align="center">
<img src="/images/aci_building_blocks/1.png" alt="ACI-top" title="ACI-top" /> 
</p>

<p>In the world of modern data centers, Cisco Application Centric Infrastructure (ACI) stands out as a powerful solution for network automation, policy enforcement, and application agility.
<!-- excerpt --></p>

<p>
This post will demystify the core building blocks of Cisco ACI, guiding you through the concepts of Tenants, VRFs (Virtual Routing and Forwarding), Bridge Domains, Application Profiles, and Endpoint Groups (EPGs). By the end of this explanation, you'll have a clear understanding of how these elements work together to form a robust and flexible network fabric, enabling you to design and implement ACI solutions with confidence. Let's dive in!
</p>

<p><a href="/images/aci_building_blocks/2-0.png" target="new" title="click here to see the full sized image"><img src="/images/aci_building_blocks/2-0.png" alt="ACI-Tenant" /></a></p>

<p>Figure 1. Workflow of Tenant to EPG deployment.</p>

<p><strong><a href="#section1">1. Understanding the Tenant: The Administrative Boundary.</a></strong>  <br />
<strong><a href="#section2">2. VRF (Virtual Routing and Forwarding): The Layer 3 Isolation Domain.</a></strong><br />
<strong><a href="#section3">3. Bridge Domain (BD): The Layer 2 Forwarding Domain.</a></strong><br />
<strong><a href="#section4">3.1. Subnet Configuration within a Bridge Domain: Defining IP Address Space &amp; Reachability.</a></strong><br />
<strong><a href="#section5">4. Application Profile (AP): The Application Logical Container.</a></strong><br />
<strong><a href="#section6">5. EPG (Endpoint Group): The Granular Policy Enforcement Point.</a></strong><br />
<strong><a href="#section7">6. Endpoint Security Groups (ESGs): Decoupling Security from Forwarding</a></strong></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section1">1. Understanding the Tenant: The Administrative Boundary.</span></strong></p>

<p>In the ACI object model, the Tenant (fvTenant) is the root of a policy tree for a given set of configurations. All subsequent networking objects (VRF, BD, EPG, etc.) are children of a Tenant.</p>

<p><strong>Default Tenants:</strong> When you initialize an ACI fabric, three default tenants are automatically created:</p>

<p><strong>common:</strong> This tenant is for shared resources that can be accessed by all other tenants. Common uses include shared Layer 3 Out (L3Out) connections to external networks, common services like DNS or DHCP, and shared Layer 4-7 services (e.g., firewalls, load balancers). Objects in the common tenant can be consumed by other tenants via contracts.</p>

<p><strong>infra (Infrastructure):</strong> This tenant is used for internal ACI fabric communications. It contains policies related to fabric discovery, image management, VXLAN overlay, and other underlying infrastructure functions. It’s generally not directly configured by users unless specifically instructed by Cisco for advanced scenarios like Multi-Pod or Multi-Site deployments.</p>

<p><strong>mgmt (Management):</strong> This tenant is dedicated to managing the ACI fabric itself. It defines policies for in-band and out-of-band management access to the APIC controllers and fabric switches. No Direct Network Construct: A Tenant itself doesn’t map directly to a traditional network construct like a VLAN, VRF, or physical interface.</p>

<p><strong>Globally Unique Names:</strong> Tenant names must be globally unique across the ACI fabric.</p>

<p><a href="/images/aci_building_blocks/2.png" target="new" title="click here to see the full sized image"><img src="/images/aci_building_blocks/2.png" alt="ACI-Tenant" /></a></p>

<p>Figure 2. Tenant deployment workflow.</p>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section2">2. VRF (Virtual Routing and Forwarding): The Layer 3 Isolation Domain.</span></strong></p>

<p>Layer 3 Isolation: The fundamental purpose of a VRF in ACI, just like in traditional networking, is to provide Layer 3 isolation. Each VRF has its own independent routing table, IP address space, and set of routing protocols. This means that traffic within one VRF cannot directly communicate with traffic in another VRF unless explicitly configured to do so (e.g., via a firewall or inter-VRF routing policies).</p>

<p><strong>Routing Instance:</strong> A VRF is essentially a virtual routing instance. It holds the routing information (routes, ARP/ND entries) for the subnets and endpoints associated with it.</p>

<p><strong>Tenant Scoped:</strong> A VRF is always contained within a Tenant. It’s a child object of a Tenant (fvCtx in the ACI object model). This means that a VRF belongs to a specific administrative domain and its policies are managed under that tenant.</p>

<p><strong>Policy Control Enforcement Preference:</strong></p>

<ul>
  <li>“Enforced” (Default and Recommended): This means that all communication between EPGs within this VRF (or between an EPG in this VRF and an external network) requires a Contract.</li>
  <li>“Unenforced”: This setting implies that all EPGs within this VRF can communicate freely with each other by default, without the need for explicit Contracts. While it simplifies initial setup, it bypasses ACI’s micro-segmentation capabilities and is generally discouraged for production environments where granular security is a requirement.</li>
</ul>

<p><strong>Preferred Group:</strong> setting, enabled at the VRF level, allows all EPGs within that VRF to communicate with each other by default if they are part of the preferred group.</p>

<p><a href="/images/aci_building_blocks/3.png" target="new" title="click here to see the full sized image"><img src="/images/aci_building_blocks/3.png" alt="ACI-VRF" /></a></p>

<p>Figure 3. VRF deployment workflow</p>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section3">3. Bridge Domain (BD): The Layer 2 Forwarding Domain.</span></strong></p>

<p>In Cisco ACI, a Bridge Domain (BD) is a critical logical construct that represents a Layer 2 forwarding domain. It’s the ACI equivalent of a traditional VLAN or broadcast domain, but with significantly enhanced capabilities and policy integration.</p>

<p><a href="/images/aci_building_blocks/4.png" target="new" title="click here to see the full sized image"><img src="/images/aci_building_blocks/4.png" alt="ACI-BD" /></a></p>

<p>Figure 4. First step in Bridge Domain deployment.</p>

<p><a href="/images/aci_building_blocks/4-1.png" target="new" title="click here to see the full sized image"><img src="/images/aci_building_blocks/4-1.png" alt="ACI-BD" /></a></p>

<p>Figure 5. Second step in Bridge Domain deployment.</p>

<p><a href="/images/aci_building_blocks/4-2.png" target="new" title="click here to see the full sized image"><img src="/images/aci_building_blocks/4-2.png" alt="ACI-BD" /></a></p>

<p>Figure 6. Third step in Bridge Domain deployment.</p>

<p><a href="/images/aci_building_blocks/4-3.png" target="new" title="click here to see the full sized image"><img src="/images/aci_building_blocks/4-3.png" alt="ACI-BD" /></a></p>

<p>Figure 7. Bridge Domain L2 features.</p>

<p><strong>Advertise Host Routes:</strong> By advertising a precise /32 (or /128) route for each individual endpoint, external routers learn the exact leaf switch (and pod/site) where the endpoint resides. This directs traffic directly to the optimal entry point into the ACI fabric.</p>

<p><strong>L2 Unknown Unicast Handling:</strong> This setting defines how the ACI fabric handles unknown unicast traffic within the Bridge Domain:</p>
<ul>
  <li>
    <p>Hardware Proxy (Default and Recommended): This is the most efficient and scalable method. When a leaf switch receives an unknown unicast frame, it doesn’t flood it everywhere. Instead, it sends the frame to a spine proxy. The spine acts as a central lookup point, and if it knows the destination MAC, it forwards the frame directly to the correct leaf. If the MAC is unknown to the spine, only then does the spine flood it to relevant leaves. This significantly reduces flood domains.</p>
  </li>
  <li>
    <p>Flood: This behaves like traditional Ethernet. Unknown unicast frames are flooded to all leaf switches that have an endpoint associated with this Bridge Domain. This can be inefficient and generate unnecessary traffic, especially in large environments.</p>
  </li>
</ul>

<p><strong>Optimize ARP Flooding:</strong> This option, when enabled, means that the ACI fabric will proxy ARP requests for known MAC addresses instead of flooding them. This reduces broadcast traffic.</p>

<p><strong>Limit local IP learning to BD/EPG subnets:</strong> Restrict IP Learning: The ACI leaf switches connected to this Bridge Domain will only learn IP addresses for endpoints that fall within the explicitly defined IP subnets configured under that particular Bridge Domain, or the subnets defined within the EPGs that reside within that BD. It will ignore Out-of-Subnet IPs.</p>

<p><a href="/images/aci_building_blocks/4-4.png" target="new" title="click here to see the full sized image"><img src="/images/aci_building_blocks/4-4.png" alt="ACI-BD" /></a></p>

<p>Figure 8. Bridge Domain L3 features.</p>

<p><strong>IP Unicast Routing (Enabled/Disabled):</strong></p>
<ul>
  <li>If enabled, the ACI fabric will provide a distributed default gateway (Anycast Gateway) for any IP subnets defined within this Bridge Domain. This means the Layer 3 routing for endpoints in this BD is handled directly by the ACI leaf switches. This is the typical configuration for routed networks.</li>
  <li>If disabled, the ACI fabric will not provide a default gateway for the subnets in this BD. This is often used for: Networks where an external router or firewall provides the default gateway. In such cases, the ACI fabric simply acts as a transparent Layer 2 forwarding plane.</li>
</ul>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:16.0pt;" id="section4">3.1 Subnet Configuration within a Bridge Domain: Defining IP Address Space &amp; Reachability.</span></strong></p>

<p>Each Bridge Domain can have one or more IP subnets associated with it. These subnets represent the IP address space for the endpoints connected to the Bridge Domain. Additionally, subnets can also be created directly under an EPG, but their purpose in that context is different: it facilitates Route Leaking and Shared Services between VRFs/Tenants with policy granularity.</p>

<p><a href="/images/aci_building_blocks/4-6.png" target="new" title="click here to see the full sized image"><img src="/images/aci_building_blocks/4-6.png" alt="ACI-Sub" /></a></p>

<p>Table 1. Comparing Subnets: Bridge Domain vs. EPG.</p>

<p><a href="/images/aci_building_blocks/4-5.png" target="new" title="click here to see the full sized image"><img src="/images/aci_building_blocks/4-5.png" alt="ACI-Sub" /></a></p>

<p>Figure 9. Subnet deployment within specific Bridge Domain.</p>

<p><strong>Treat as Virtual IP address:</strong> Special handling for HA (Active/Standby) external device VIPs, ensuring proper learning and failover during HA events.</p>

<p><strong>Make this IP address primary</strong> Designates a preferred source IP for the BD’s own control plane traffic when multiple subnets exist within this BD.</p>

<p><strong>Advertised Externally:</strong> Advertises individual /32 (or /128) host routes for learned endpoints to external networks via L3Outs, optimizing traffic paths and VM mobility.</p>

<p><strong>Shared Between VRFs:</strong> This option allows the subnet to be “leaked” and reachable by other VRFs within the same or different Tenants, usually in conjunction with Contracts for inter-VRF communication.</p>

<p><strong>No Default SVI Gateway:</strong> Disables ACI’s pervasive gateway for this specific subnet (useful for secondary IPs or external gateways).</p>

<p><strong>IP Data Plane Learning</strong> Enables dynamic learning of endpoint IP addresses by observing network traffic (typically enabled for general endpoints).</p>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section5">4. Application Profile (AP): The Application Logical Container.</span></strong></p>

<p>In Cisco ACI, an Application Profile (AP) is a logical container that groups together all the Endpoint Groups (EPGs) and Contracts needed to deploy a specific application or service. It provides a higher-level abstraction for managing application-centric policies within the ACI fabric. The primary purpose of an Application Profile is to shift the focus from network-centric configuration to application-centric configuration. Instead of configuring network policies based on VLANs or subnets, you define policies based on the application components and their communication requirements.</p>

<p><a href="/images/aci_building_blocks/5.png" target="new" title="click here to see the full sized image"><img src="/images/aci_building_blocks/5.png" alt="ACI-AppProf" /></a></p>

<p>Figure 10. Deployment of Application profile.</p>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section6">5. EPG (Endpoint Group): The Granular Policy Enforcement Point.</span></strong></p>

<p>An EPG is the smallest and most critical unit to which policy (Contracts) is applied. If two endpoints are in the same EPG, they can communicate by default (unless otherwise restricted by other policies like Preferred Group or micro-segmentation). If they are in different EPGs, communication between them is blocked by default, and a Contract is required to permit specific traffic flows. This is ACI’s core “whitelist” security model. EPGs support micro-segmentation through features like intra-EPG isolation, which prevents communication between endpoints within the same EPG if required.</p>

<p><strong>Type Of EPGs:</strong></p>
<ul>
  <li>“Classic” EPG which is representing endpoints, they can be physical servers, virtual machines, network-attached storage devices, or even clients on the internet.</li>
  <li>Layer 2 external outside network instance endpoint groups (l2extInstP).</li>
  <li>Layer 3 external outside network instance endpoint groups (l3extInstP).</li>
  <li>Management Endpoints groups (mgmtOoB, mgmtInB) are used for securing access to the ACI fabric’s management plane, either through a dedicated out-of-band management network or via in-band access within the fabric itself.</li>
</ul>

<p><a href="/images/aci_building_blocks/5.png" target="new" title="click here to see the full sized image"><img src="/images/aci_building_blocks/5.png" alt="ACI-EPG" /></a></p>

<p>Figure 11. Deployment of EPG under application profile.</p>

<p>To provision EPG on the port/switch we have to methods:</p>
<ul>
  <li><strong>Static Ports:</strong> Explicitly links an EPG to a specific physical interface.</li>
  <li><strong>Static Leafs (via AAEP):</strong> Links an EPG to a logical group of interfaces defined by an AAEP, effectively pushing the EPG’s VLAN to all ports associated with that AAEP.</li>
</ul>

<p><a href="/images/aci_building_blocks/5-1.png" target="new" title="click here to see the full sized image"><img src="/images/aci_building_blocks/5-1.png" alt="ACI-EPG" /></a></p>

<p>Figure 12. EPG to Leaf Port Static Assignment.</p>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section7">6. Endpoint Security Groups (ESGs): Decoupling Security from Forwarding</span></strong></p>

<p>Decoupling Security from Forwarding: This is the core architectural shift and primary benefit of ESGs compared to EPGs. EPGs combine both forwarding (VLAN/VXLAN binding, BD association) and security, whereas ESGs aim to provide security policy based on attributes, independent of the underlying network forwarding.</p>

<p><a href="/images/aci_building_blocks/6.png" target="new" title="click here to see the full sized image"><img src="/images/aci_building_blocks/6.png" alt="ACI-ESG" /></a></p>

<p>Figure 13. Defining Endpoint Security Groups (ESGs) for Policy Decoupling.</p>

<p>After defining your ESGs, you will create Contracts as usual.
Associate the ESG as a Provider or Consumer of the contract, just like you would with EPGs.</p>

<p><strong>Remember:</strong> Contracts typically apply ESG-to-ESG, ESG-to-L3Out External EPG, ESG-to-L2Out External EPG or ESG-to-vzAny.</p>

<p>By using ESGs, ACI provides an additional layer of flexibility and powerful tools for defining and enforcing security policies, especially beneficial in complex and evolving data center environments.</p>

<p><a href="#top">Back to Top</a></p>]]></content><author><name></name></author><category term="Cisco ACI" /><summary type="html"><![CDATA[In the world of modern data centers, Cisco Application Centric Infrastructure (ACI) stands out as a powerful solution for network automation, policy enforcement, and application agility.]]></summary></entry><entry><title type="html">Understanding Cisco ACI Access Policies: A Practical Guide</title><link href="http://0.0.0.0:4000/cisco%20aci/aci-access-polices/" rel="alternate" type="text/html" title="Understanding Cisco ACI Access Policies: A Practical Guide" /><published>2025-05-28T00:00:00+00:00</published><updated>2025-05-28T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cisco%20aci/aci-access-polices</id><content type="html" xml:base="http://0.0.0.0:4000/cisco%20aci/aci-access-polices/"><![CDATA[<p align="center">
<img src="/images/aci_access_polices/1.gif" alt="ACI-top" title="ACI-top" /> 
</p>

<p>Understanding Cisco ACI Access Policies doesn’t have to be complicated. This practical guide simplifies the core concepts and clearly shows you what you need to know.
<!-- excerpt --></p>

<p>
Secure and controlled access is fundamental to any network. This guide walks you through both the essentials and more advanced topics of Cisco ACI Access Policies, demonstrating how to apply them effectively in your environment and designs.
</p>

<p><strong><a href="#section1">1. The End-to-End Workflow for Cisco ACI Access Policy Deployment.</a></strong>  <br />
<strong><a href="#section2">1.1. Interface Policies Configuration Workflow.</a></strong><br />
<strong><a href="#section3">1.2. Leaf Switches Policies Configuration Workflow.</a></strong><br />
<strong><a href="#section4">2. Leaf Access Port configuration.</a></strong><br />
<strong><a href="#section5">3. Leaf Port Channel (PC) configuration.</a></strong><br />
<strong><a href="#section6">4. Leaf Virtual Port Channel (vPC) configuration.</a></strong></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section1">1. The End-to-End Workflow for Cisco ACI Access Policy Deployment.</span></strong></p>

<p>Cisco Application Centric Infrastructure (ACI) redefines network management with its policy-driven automation, moving away from traditional device-by-device configurations. At the heart of this transformative approach lies Access Policies, which dictate how physical network devices connect to the ACI fabric and how virtual and physical workloads access network services.</p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section2">1.1. Interface Policies Configuration Workflow.</span></strong></p>

<ul>
  <li>
    <p><strong>VLAN Pools</strong> A VLAN Pool defines a range (or multiple ranges) of VLAN IDs that are permitted for use on leaf switch ports for EPG encapsulation. It doesn’t provision the VLANs themselves but rather reserves them for use.</p>
  </li>
  <li><strong>Domains and Domain Types</strong> Purpose: Domains act as a bridge, linking VLAN Pools with policies and the infrastructure. They define the scope where a particular VLAN pool can be utilized and by what type of entity.
    <ol>
      <li>
        <p>Physical Domains integrate bare-metal servers, physical appliances (e.g., firewalls, load balancers, non-virtualized servers), or other physical devices directly connected to ACI leaf ports.</p>
      </li>
      <li>
        <p>Fibre Channel Domains are primarily used for integrating Fibre Channel over Ethernet (FCoE) storage networks with the ACI fabric. This allows the ACI fabric to act as a converged network that carries both IP and FCoE traffic.</p>
      </li>
      <li>
        <p>External L3 Domains connect the ACI fabric to external Layer 3 (routed) networks or traditional routers outside the ACI fabric (Used by L3OUT).</p>
      </li>
      <li>
        <p>External Bridge Domains connect the ACI fabric to external Layer 2 (bridged) networks or switches outside the ACI fabric. This is used when you need to extend Layer 2 segments from the ACI fabric to external devices without routing (Used by L2OUT).</p>
      </li>
    </ol>
  </li>
  <li><strong>Attachable Access Entity Profile (AAEP)</strong> The AAEP effectively defines which VLANs are allowed on an interface or group of interfaces. It does this by referencing Domains, which in turn reference VLAN Pools. If we compare with old fashion switch configuration it should be
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> switchport trunk allowed vlan
</code></pre></div>    </div>
    <p><strong>Common Scenarios for AAEP Scope/Design:</strong></p>
    <ol>
      <li>
        <p>One-to-one with Domains: Often, you’ll create an AAEP for each type of domain (Physical AAEP,   1.VMM AAEP, L3Out AAEP, L2Out AAEP, FCoE AAEP) to clearly separate their respective VLAN pools and interface associations.</p>
      </li>
      <li>
        <p>Functional Grouping: You might create AAEPs based on functional groups of ports, e.g., “ServerFarm_AAEP” that encompasses all necessary domains (physical and VMM) and VLANs for server connectivity.</p>
      </li>
    </ol>
  </li>
  <li><strong>Interface Policy Group (IPG)</strong> acts as a collection point for individual “interface policies.” These individual policies define specific Layer 1 or Layer 2 parameters for a port.</li>
</ul>

<p><strong>Key Interface Policy Group Policies are:</strong></p>
<ul>
  <li>Link Level Policy (speed, duplex, auto-negotiation).</li>
  <li>CDP (Cisco Discovery Protocol) Policy (enable/disable).</li>
  <li>LLDP (Link Layer Discovery Protocol) Policy (enable/disable).</li>
  <li>LACP Policy (for Port Channels: active, passive, on, port priority).</li>
  <li>Storm Control Policy control Broadcast, Unknown Unicast, and Multicast (BUM).</li>
  <li>MCP (Mis-Cabling Protocol) Policy.</li>
  <li>Spanning Tree related policies (BPDU Guard/Filter).</li>
  <li>
    <p>L2 Interface Polices (VLAN scope Global or Port Local).</p>
  </li>
  <li><strong>Interface Profile</strong></li>
</ul>

<p align="center">
<img src="/images/aci_access_polices/2-1.png" alt="ACI-AccessPolice" title="ACI-AccessPolice" /> 
</p>
<p>Figure 1. Representing Interface Policies Configuration Flow.</p>

<p><a href="/images/aci_access_polices/2-11.png" target="new" title="click here to see the full sized image"><img src="/images/aci_access_polices/2-11.png" alt="IntProPolices" /></a></p>

<p>Figure 2. Interface Policy Configuration within the Cisco APIC UI.</p>

<p><a href="/images/aci_access_polices/2-13.png" target="new" title="click here to see the full sized image"><img src="/images/aci_access_polices/2-13.png" alt="IntProPolices" /></a></p>

<p>Figure 2.1. Interface Policy Configuration within the Cisco APIC UI.</p>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section3">1.2. Leaf Switches Policies Configuration Workflow.</span></strong></p>

<ul>
  <li><strong>Leaf Switches Policy Groups</strong> they control the behavior of the entire leaf switch. This allows for consistent application of switch-level configurations to multiple leaf nodes selected by a Leaf Switch Profile.</li>
</ul>

<p><strong>Key Leaf Switches Policy Groups are:</strong></p>
<ul>
  <li>Spanning Tree</li>
  <li>Monitoring polices</li>
  <li>CoPP Leaf Policy</li>
  <li>NetFlow Node Policy</li>
  <li>BFD IPv4 and IPv6 Policy</li>
</ul>

<p align="center">  
<img src="/images/aci_access_polices/2-2.png" alt="ACI-AccessPolice" title="ACI-AccessPolice" /> 
</p>
<p>Figure 3. Representing Leaf Switch Policies Configuration Flow.</p>

<p><a href="/images/aci_access_polices/2-21.png" target="new" title="click here to see the full sized image"><img src="/images/aci_access_polices/2-21.png" alt="LeafSWIntPro" /></a></p>

<p>Figure 4. Leaf Switch Policies Configuration within the Cisco APIC UI.</p>

<p align="center">
<img src="/images/aci_access_polices/2.png" alt="ACI-AccessPolice" title="ACI-AccessPolice" /> 
</p>

<p>Figure 5. Full configuration flow for interface and leaf switch polices.</p>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section4">2. Leaf Access Port configuration.</span></strong></p>

<p><a href="/images/aci_access_polices/3-1.png" target="new" title="click here to see the full sized image"><img src="/images/aci_access_polices/3-1.png" alt="AccessPort" /></a></p>

<p>Figure 6. Implementation of an Interface Policy Group within the Cisco APIC user interface (UI).</p>

<p>After creating the Leaf Interface profile and attaching it to the Leaf Switch profile, we add specific ports under the Leaf Interface profile and select the appropriate Interface Policy Group. This approach provides reusability and simplifies automation. For Particular case we created Leaf Access Port Policy Group like below and attached to port eth1/1 inside specific Leaf Interface Profile.</p>

<p><a href="/images/aci_access_polices/3-2.png" target="new" title="click here to see the full sized image"><img src="/images/aci_access_polices/3-2.png" alt="AccessPort" /></a></p>

<p>Figure 7. Representing creation of Access port policy group and attaching to specific Leaf Interface profile.</p>

<p align="center">
<img src="/images/aci_access_polices/3.png" alt="ACI-AccessPolice" title="ACI-AccessPolice" /> 
</p>

<p>Figure 8. End to end workflow to setup access port up and running on leaf switch.</p>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section5">3. Leaf Port Channel (PC) configuration.</span></strong></p>

<p>The foundational workflow for configuring a Port Channel in Cisco ACI closely mirrors that of a standard Access Port; however, a key distinction lies in the explicit inclusion and application of an LACP Policy within the associated Interface Policy Group, which is then specifically leveraged for the two designated physical ports on a single leaf switch to form the logical Port Channel.</p>

<p><a href="/images/aci_access_polices/4-1.png" target="new" title="click here to see the full sized image"><img src="/images/aci_access_polices/4-1.png" alt="PC" /></a></p>

<p>Figure 9. Representing creation of Port Channel policy group.</p>

<p align="center">
<img src="/images/aci_access_polices/4.png" alt="ACI-AccessPolice" title="ACI-AccessPolice" /> 
</p>

<p>Figure 10. End to end workflow to setup Port Channel (PC) up and running on leaf switch.</p>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section6">4. Leaf Virtual Port Channel (vPC) configuration.</span></strong></p>

<p>Virtual Port Channel (vPC) significantly enhances network resilience and operational uptime by logically combining two physical ports, each residing on a separate Cisco Nexus leaf switch, into a single, aggregated link. This innovative design provides superior high availability, as it creates active-active forwarding paths, ensuring that traffic can seamlessly continue to flow even if one of the leaf switches or an individual link experiences a failure.</p>

<p><a href="/images/aci_access_polices/6-1.png" target="new" title="click here to see the full sized image"><img src="/images/aci_access_polices/6-1.png" alt="VPCDom" /></a></p>

<p>Figure 11. Representing creation of Virtual Port Channel (vPC) policy group.</p>

<p align="center">
<img src="/images/aci_access_polices/5.png" alt="ACI-AccessPolice" title="ACI-AccessPolice" /> 
</p>
<p>Figure 12.End to end workflow to setup Virtual Port Channel (vPC) up and running on leaf switches.</p>

<p><a href="#top">Back to Top</a></p>]]></content><author><name></name></author><category term="Cisco ACI" /><summary type="html"><![CDATA[Understanding Cisco ACI Access Policies doesn’t have to be complicated. This practical guide simplifies the core concepts and clearly shows you what you need to know.]]></summary></entry><entry><title type="html">Designing with Cisco ACI</title><link href="http://0.0.0.0:4000/cisco%20aci/designing-with-aci/" rel="alternate" type="text/html" title="Designing with Cisco ACI" /><published>2025-03-30T00:00:00+00:00</published><updated>2025-03-30T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cisco%20aci/designing-with-aci</id><content type="html" xml:base="http://0.0.0.0:4000/cisco%20aci/designing-with-aci/"><![CDATA[<p align="center">
<img src="/images/designing_with_aci/1.gif" alt="ACI-top" title="ACI-top" /> 
</p>

<p>Welcome to “Designing with Cisco ACI,” where we will dive into the Cisco ACI principals of creating robust, scalable, and efficient network architectures using Cisco’s Application Centric Infrastructure (ACI).
<!-- excerpt --></p>

<p>
In today’s rapidly evolving IT landscape, the ability to design flexible and resilient networks is more critical than ever. This blog will provide you with ideas on what you can do with Cisco ACI fabric, covering various use cases and key design points. It will be especially valuable for those preparing to deploy Cisco ACI and who are still unsure which design to choose.
The following Cisco ACI design types will be the topics of discussion:  
</p>
<body id="top">  
</body>

<p><strong><a href="#section1">1. Cisco ACI Traditional Single Pod.</a></strong>  <br />
<strong><a href="#section2">2. Cisco ACI Remote leaf.</a></strong><br />
<strong><a href="#section3">3. Cisco ACI Mini.</a></strong><br />
<strong><a href="#section4">4. Cisco ACI Multi-Pod Interconnect via IPN.</a></strong><br />
<strong><a href="#section5">5. Cisco ACI Multi-Pod Interconnect Spine back to back.</a></strong><br />
<strong><a href="#section6">6. Cisco ACI Multi-Site.</a></strong><br />
<strong><a href="#section7">7. Multi-Pod - Multi-Site hybrid deployment.</a></strong></p>

<p><strong>So let’s start!</strong></p>

<p><strong><span style="color:#074080; font-size:18.0pt;" id="section1">1. Cisco ACI Traditional Single Pod.</span></strong></p>
<p align="center">
<img src="/images/designing_with_aci/single-pod.png" alt="ACI-SinglePod" title="ACI-SinglePod" /> 
</p>
<p>Figure 1. Cisco ACI Single Pod Fabric.</p>

<p><strong>Main Characteristics:</strong></p>

<ul>
  <li><strong>Architecture:</strong> Consists of a single APIC (Application Policy Infrastructure Controller) cluster managing one ACI fabric.</li>
  <li><strong>Management:</strong> Centralized management with a single point of control.</li>
  <li><strong>Scalability:</strong> Limited to the capacity of one fabric.</li>
  <li><strong>Latency:</strong> Lower latency within the single fabric.</li>
  <li><strong>Failure Domain:</strong> Single failure domain, meaning any issue can affect the entire fabric.</li>
</ul>

<p><strong>Key Differences between Cisco ACI Single and Multi Pod Fabric:</strong></p>
<ul>
  <li><strong>Scalability:</strong> Multi-Pod offers greater scalability by allowing multiple Pods to be managed under one APIC cluster.</li>
  <li><strong>Resiliency:</strong> Multi-Pod improves resiliency by isolating failure domains.</li>
  <li><strong>Latency:</strong> Single Pod has lower latency within the fabric, while Multi-Pod may have slightly higher latency due to inter-Pod communication</li>
</ul>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt" id="section2">2. Cisco ACI Remote leaf.</span></strong></p>
<p align="center">
<img src="/images/designing_with_aci/remote-leaf.png" alt="ACI-Remote-leaf" title="ACI-Remote-leaf" /> 
</p>
<p>Figure 2. Cisco ACI Extending fabric with remote leaf to remote location.</p>

<p><strong>Main Characteristics:</strong></p>
<ul>
  <li><strong>VxLAN Tunnels:</strong> Unicast traffic is forwarded through VXLAN tunnels over Layer 3. This ensures that traffic between the remote leaf and the main fabric is encapsulated and securely transmitted.</li>
  <li><strong>Head End Replication (HER):</strong> For Layer 2 broadcast, unknown unicast, and multicast (BUM) traffic, Head End Replication (HER) tunnels are used. This method avoids the need for multicast in the WAN.</li>
  <li><strong>Policy Consistency:</strong> All policies defined in the main data center are consistently applied to the remote leaf switches, ensuring uniform policy enforcement across the entire fabric.</li>
  <li><strong>Local Switching:</strong> Traffic between endpoints connected to the remote leaf is switched locally, while traffic requiring spine proxy services is forwarded to the main fabric.</li>
</ul>

<p><strong>Inter-Pod Network Requirements for Remote Leaf deployment:</strong></p>
<ul>
  <li>L3 network should support 50 Bytes overhead for VxLAN if we have default MTU 1500 Bytes.</li>
  <li>The latency between the ACI main DC and the remote Leaf should be less than 300 msec Round Trip Time (RTT)</li>
  <li>A minimum of 100 Mbps bandwidth in the IP Network. Starting from Cisco ACI Release 4.2.4, the requirement has been lowered to 10 Mbps of bandwidth.</li>
  <li>The remote leaf solution requires the /32 Tunnel End Point (TEP) IP addresses of the remote leaf switches and main data center leaf/spine switches to be advertised across the main data center and remote leaf switches without summarization.</li>
</ul>

<p><strong>Use Cases:</strong><br />
<strong>Satellite/Co-Location Data Centers:</strong>  <br />
Enterprises or service providers often have a main data center and several smaller satellite data centers distributed across multiple locations.<br />
<strong>Disaster Recovery Sites:</strong>  <br />
Organizations need to maintain disaster recovery sites to ensure business continuity in case of a failure at the primary site.<br />
<strong>Branch Offices:</strong><br />
Companies with multiple branch offices require consistent network policies and centralized management.<br />
<strong>Edge Computing:</strong><br />
With the rise of edge computing, data processing is moved closer to the source of data generation example telco 5G.</p>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt" id="section3">3. Cisco ACI Mini.</span></strong></p>
<p align="center">
<img src="/images/designing_with_aci/aci-mini.png" alt="ACI-Remote-leaf" title="ACI-Remote-leaf" /> 
</p>
<p>Figure 3. Cisco ACI mini - connecting virtual APICs to fabric.</p>

<p><strong>Main Characteristics</strong></p>
<ul>
  <li><strong>APIC Cluster:</strong>
Consists of one physical APIC and two virtual APICs (vAPICs) running in virtual machines.
Reduces the physical footprint and cost of the APIC cluster, making it suitable for environments with limited rack space or budget.</li>
  <li><strong>Deployment Scenarios:</strong>
Ideal for colocation facilities, single-room data centers, or any scenario where full-scale ACI installations are impractical.</li>
  <li><strong>Management and Policy:</strong>
Provides centralized management and policy enforcement similar to full-scale ACI deployments.
Does not support features like Multi-Pod, Virtual Pod, or Remote Leaf switches.</li>
  <li><strong>Integration:</strong>
Multi-Site Support: Can be integrated with Cisco ACI Multi-Site for broader deployment scenarios.</li>
  <li><strong>Time Synchronization:</strong>
Requires the physical APIC and the ESXi server hosting the virtual APICs to be time-synced with the same NTP server to avoid synchronization issues.</li>
</ul>

<p><strong>Use Cases:</strong></p>
<ul>
  <li><strong>Colocation Facilities:</strong>
Businesses that use colocation facilities to host their IT infrastructure.</li>
  <li><strong>Single-Room Data Centers:</strong>
Small businesses or branch offices with a single-room data center.</li>
  <li><strong>Development and Test Environments:</strong>
Organizations that need a dedicated environment for development and testing.</li>
  <li><strong>Edge Computing:</strong>
Deployments at the edge of the network, closer to data sources.</li>
</ul>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt" id="section4">4. Cisco ACI Multi-Pod Interconnect via IPN.</span></strong></p>
<p align="center">
<img src="/images/designing_with_aci/multi-pod.png" alt="ACI-MultiPod" title="ACI-MultiPod" /> 
</p>
<p>Figure 4. Cisco ACI Multi-Pod Interconnected via Inter-Pod Network.</p>

<p><strong>Main Characteristics</strong></p>
<ul>
  <li><strong>Centralized Management:</strong>
Managed by a single APIC cluster, providing a unified management and policy domain across all Pods1.</li>
  <li><strong>Fault Isolation:</strong>
Control plane protocols (e.g., IS-IS, COOP, MP-BGP) operate independently within each Pod, enhancing fault isolation.</li>
  <li><strong>Scalability:</strong>
Supports the interconnection of multiple Pods, allowing for greater scalability and flexibility in data center design.</li>
  <li><strong>Inter-Pod Network (IPN):</strong>
Pods are connected via a Layer 3 Inter-Pod Network (IPN), which uses VXLAN encapsulation for data plane traffic and MP-BGP EVPN for control plane communication.</li>
  <li><strong>End-to-End Policy Enforcement:</strong>
Ensures consistent policy enforcement across all Pods, maintaining uniform security and network policies.</li>
  <li><strong>Resiliency:</strong>
Isolated failure domains within each Pod reduce the impact of failures, enhancing overall resiliency.</li>
  <li><strong>Geographical Flexibility:</strong>
Pods can be located in different physical data centers, including geographically dispersed locations, with up to 50 ms RTT latency.</li>
</ul>

<p><strong>Inter-Pod Network (IPN) Requirements:</strong></p>
<ul>
  <li><strong>OSPF:</strong> Required for routing between Pods.</li>
  <li><strong>PIM Bi-Dir:</strong> Used for handling unknown unicast, multicast, and broadcast traffic.</li>
  <li><strong>MTU Size:</strong> The IPN must support jumbo frames with an MTU of 9150 to accommodate VXLAN encapsulated traffic.
In newer APIC version there is possibility to perform tunning of the MTU size of all the control plane packets generated by ACI nodes (leaf and spines), including inter-Pod MP-BGP control plane traffic.</li>
  <li><strong>DHCP Relay Functionality:</strong> Needed to allow Spine and Leaf nodes to be discovered across the IPN.</li>
  <li><strong>Prioritization:</strong> Ensures that APIC-to-APIC communication is prioritized over the IPN.</li>
  <li><strong>Interface Support:</strong> 10/40/100G</li>
  <li><strong>VLAN 4:</strong> Used for 802.1Q encapsulation between the spine and IPN devices.</li>
  <li><strong>VRF Context:</strong> Recommended to isolate IPN traffic from other services, ensuring stable IPN connectivity.</li>
</ul>

<p><strong>Use Cases:</strong></p>
<ul>
  <li><strong>Disaster Recovery and Business Continuity:</strong>
Organizations need to ensure business continuity and disaster recovery across multiple data centers.</li>
  <li><strong>Geographically Dispersed Data Centers:</strong>
Enterprises with data centers in different geographical locations require a unified network fabric.</li>
  <li><strong>Active/Active Data Centers:</strong>
Businesses need to deploy applications across multiple active data centers for load balancing and high availability.</li>
  <li><strong>Scalability and Flexibility:</strong>
Large enterprises or service providers need to scale their data center infrastructure.</li>
</ul>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt" id="section5">5. Cisco ACI Multi-Pod Interconnect Spine back to back.</span></strong></p>
<p align="center">
<img src="/images/designing_with_aci/multi-pod-b2b.png" alt="ACI-MultiPod" title="ACI-MultiPod" /> 
</p>
<p>Figure 5. Cisco ACI Multi-Pod, pods connected directly via Spines.</p>

<p>The characteristics and use cases for Cisco ACI Multi-Pod with spine back-to-back interconnect are essentially the same as those for Cisco ACI Multi-Pod interconnected via an Inter-Pod Network (IPN). The primary distinction is the absence of an IPN. However, it is crucial to ensure direct connectivity between remote spines.</p>

<p><strong>Advantages:</strong></p>
<ul>
  <li><strong>Simplified Network Design:</strong>
Eliminates the need for an Inter-Pod Network (IPN), simplifying the overall network design and reducing complexity.</li>
  <li><strong>Reduced Latency:</strong><br />
Provides direct connectivity between remote spines, which can reduce latency compared to traversing an IPN.</li>
  <li><strong>Cost Efficiency:</strong><br />
Reduces the need for additional IPN devices and infrastructure, leading to cost savings.</li>
  <li><strong>Improved Performance:</strong><br />
Direct spine-to-spine connections can optimize traffic flow and improve overall network performance.</li>
  <li><strong>Enhanced Resiliency:</strong><br />
Offers redundant paths between Pods, enhancing network resiliency and fault tolerance.</li>
</ul>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt" id="section6">6. Cisco ACI Multi-Site.</span></strong></p>
<p align="center">
<img src="/images/designing_with_aci/multi-site.png" alt="ACI-MultiSite" title="ACI-MultiSite" /> 
</p>
<p>Figure 6. Cisco ACI Multi-Site managed by Nexus Dashboard Orchestrator.</p>

<p><strong>Key Differences between Multi Site and Multi Pod deployment:</strong></p>
<ul>
  <li><strong>Management:</strong> Multi-Pod uses a single APIC cluster, while Multi-Site uses multiple APIC clusters managed by NDO.</li>
  <li><strong>Latency:</strong> Multi-Pod typically has lower latency between pods, whereas Multi-Site can handle higher latency across sites.</li>
  <li><strong>Scalability:</strong> Multi-Site is better suited for large-scale, geographically dispersed deployments.</li>
</ul>

<p><strong>Main Characteristics:</strong></p>
<ul>
  <li><strong>Geographical Flexibility:</strong>
Supports the interconnection of geographically dispersed data centers, extending Layer 2 and Layer 3 connectivity between sites.</li>
  <li><strong>Unified Policy Management:</strong>
Utilizes the Nexus Dashboard Orchestrator (formerly Multi-Site Orchestrator) to manage and enforce policies across multiple sites.</li>
  <li><strong>Fault Isolation:</strong>
Each site operates with its own APIC cluster, ensuring fault isolation and independent control planes.</li>
  <li><strong>Scalability:</strong>
Allows for the integration of multiple ACI fabrics, providing scalability for large and distributed environments.</li>
  <li><strong>Consistent Policy Enforcement:</strong>
Ensures consistent policy enforcement across all interconnected sites, maintaining uniform security and network policies.</li>
  <li><strong>Data Plane Communication:</strong>
VXLAN and MP-BGP EVPN: Uses VXLAN for data-plane communication and MP-BGP EVPN for control-plane communication between sites.</li>
</ul>

<p><strong>Inter-Site Network (ISN) Requirements:</strong>
ACI fabrics are connected via ISN routed network which will be used for VxLAN communications between endpoints which they belong to different fabrics.
I would like to highlight that there is no latency limit between the different ACI fabrics part of the same Multi-Site domain.<br />
The only latency considerations are:</p>

<ul>
  <li>Up to 150 ms, RTT is the latency supported between the Nexus Dashboard cluster nodes where the Orchestrator service is enabled.</li>
  <li>Up to 500 msec RTT is the latency between each Nexus Dashboard Orchestrator node and the APIC controller nodes that are added to the Multi-Site domain. This means that the Multi-Site architecture has been designed from the ground up to be able to manage ACI fabrics that can be geographically dispersed around the world.</li>
  <li>No Need for Multicast.</li>
  <li>MTU between ISN routers and spines should be increases to minimum 1600 Bytes, in this case should be changed default MTU size 9000 Bytes in global configuration.</li>
  <li>IP connectivity can be achieved using OSPF, BGP or static routing.</li>
</ul>

<p><strong>Use Cases:</strong><br />
<strong>Centralized Data Center with Separate Availability Zones:</strong><br />
Allows for the creation of distinct availability zones within a centralized data center, enhancing fault isolation and resource management.<br />
<strong>Disaster Recovery:</strong><br />
Facilitates disaster recovery by enabling data replication and failover between sites, ensuring business continuity in case of site failures.<br />
<strong>Geographically Distributed Data Centers:</strong><br />
Supports the management of data centers spread across different cities, countries, or continents from a single pane of glass, simplifying provisioning, monitoring, and management.<br />
<strong>Stretched Bridge Domain with Layer 2 Broadcast Extension:</strong><br />
Extends Layer 2 domains across sites, allowing for live VM migration and active/active high availability.<br />
<strong>Stretched VRF with Inter-Site Contracts:</strong><br />
Enables consistent policy enforcement across sites by stretching VRFs and applying inter-site contracts.<br />
<strong>Shared Services with Stretched Provider EPGs:</strong><br />
Allows shared services to be accessed across multiple sites using stretched Endpoint Groups (EPGs).</p>

<p><a href="#top">Back to Top</a></p>

<p><strong><span style="color:#074080; font-size:18.0pt" id="section7">7. Multi-Pod - Multi-Site hybrid deployment.</span></strong></p>

<p><a href="/images/designing_with_aci/multi-pod-multi-site.png" target="new" title="click here to see the full sized image"><img src="/images/designing_with_aci/multi-pod-multi-site.png" alt="your alt description here" /></a>
Figure 7. Extending Cisco ACI Multi-Pod to Multi-Site.</p>

<p>Using Cisco ACI Multi-Site and Multi-Pod as a single fabric offers several benefits, enhancing the overall network architecture. Here are some key advantages:</p>

<ul>
  <li><strong>Centralized Management:</strong><br />
Both Multi-Site and Multi-Pod architectures allow for centralized management through a single APIC cluster, simplifying policy enforcement and configuration.</li>
  <li><strong>Scalability:</strong><br />
Multi-Pod enables the expansion of a single ACI fabric across multiple locations without the need for separate fabrics, making it easier to scale the network.<br />
Multi-Site allows for the interconnection of multiple ACI fabrics, providing scalability across geographically dispersed data centers.</li>
  <li><strong>Resiliency and Fault Isolation:</strong><br />
Multi-Pod architecture offers full resiliency at the network level across pods while maintaining a single fabric, ensuring minimal impact from failures.<br />
Multi-Site architecture isolates failure domains between sites, enhancing overall network resiliency.</li>
  <li><strong>Disaster Recovery and Business Continuity:</strong><br />
Both architectures support disaster recovery by enabling data replication and failover between sites or pods, ensuring business continuity in case of site failures.</li>
  <li><strong>Consistent Policy Enforcement:</strong> 
Policies and configurations can be consistently applied across all sites and pods, ensuring uniform security and compliance.</li>
  <li><strong>Operational Efficiency:</strong>  <br />
Centralized management and automation reduce administrative overhead and operational complexity, making it easier to manage large-scale networks.</li>
</ul>

<p><a href="#top">Back to Top</a></p>]]></content><author><name></name></author><category term="Cisco ACI" /><summary type="html"><![CDATA[Welcome to “Designing with Cisco ACI,” where we will dive into the Cisco ACI principals of creating robust, scalable, and efficient network architectures using Cisco’s Application Centric Infrastructure (ACI).]]></summary></entry><entry><title type="html">Unlocking the Power of Cisco ACI 6.0</title><link href="http://0.0.0.0:4000/cisco%20aci/aci-6-new-features/" rel="alternate" type="text/html" title="Unlocking the Power of Cisco ACI 6.0" /><published>2024-09-27T00:00:00+00:00</published><updated>2024-09-27T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cisco%20aci/aci-6-new-features</id><content type="html" xml:base="http://0.0.0.0:4000/cisco%20aci/aci-6-new-features/"><![CDATA[<p align="center">
<img src="/images/aci_6_new_features/1.png" alt="ACI_60" title="ACI_60" />   
</p>
<p style="text-align: justify;">
The Cisco ACI 6.0 release has been available for a couple of months now. Let’s check out what it brings to network engineers and IT professionals. Experience a new era of enhanced performance, security, and automation capabilities. In this blog, we’ll dive into the key enhancements of Cisco ACI 6.0 and explore how they can empower your network infrastructure.
</p>

<p><strong><span style="color:#074080">Main three features and enhancements in Cisco ACI 6.0.2 and 6.0.3</span></strong></p>

<ol>
  <li>Virtual APIC on VMware ESXi and Cloud</li>
  <li>VMM Integration with Nutanix AHV</li>
  <li>Nexus Dashboard - Connectivity analysis</li>
</ol>

<p><strong><span style="color:#074080">1. Virtual APIC  6.0(2)F</span></strong></p>

<ul>
  <li>No physical APICs.</li>
  <li>No configuration limitations you can administer it like a regular APICs.</li>
  <li>Scale is pretty much same as for physical APIC.</li>
  <li>Limitations not able to run applications such as ELAM, policy explorer.</li>
  <li>APICs can be connected to leaf switches directly.</li>
  <li>Or they can be connected remotely via L3 network.
During the initial setup, the wizard will ask you to choose how the APIC is connected to the network. Figure 1 below is an example.</li>
  <li>Virtual APIC Cluster Fully functional APIC that can manage up to 200 switches.</li>
  <li>6.0(2)F Cloud hosted APIC Cluster can only be deployed within AWS Cloud.</li>
</ul>
<p align="center">
<img src="/images/aci_6_new_features/2.png" alt="ACI 60" title="ACI 60" /> 
</p>
<p><strong>Figure 1</strong> ACI setup wizard</p>

<p><strong><span style="color:#074080">2. Integration with Nutanix AHV hypervisor  6.0(3)F</span></strong></p>
<ul>
  <li>APICs connect and authenticate to Prism Manager.</li>
  <li>Integration of Nutanix Acropolis Hypervisor (AHV) with Cisco Application Centric Infrastructure (ACI).</li>
  <li>This integration provides virtual and physical network automation and VM endpoints visibility in Cisco ACI.</li>
  <li>End to end network automation and end to end visibility.</li>
</ul>

<p><strong><span style="color:#074080">3. Nexus Dashboard - Connectivity Analysis 6.0(2)F</span></strong></p>

<ul>
  <li>Troubleshooting between two End points, src and dst IP addresses or src and dst MAC addresses.</li>
  <li>Visibility of what happening between control and data plane.</li>
  <li>Verification of control and data plane.</li>
  <li>Traces all possible forwarding paths between source and destinations.</li>
  <li>Identifies if there are problems in the path and what is causing issues.</li>
  <li>Helps to highlight real root cause, perform forwarding checks.</li>
</ul>

<p><strong><span style="color:#074080">4.Interface policy - simplify configuration</span></strong></p>
<ul>
  <li>This type of configuration is optional there is no need to change if you already master polices configuration. On that way you will keep everything under your control.</li>
  <li>This is more for new guys which is starting to use ACI Fabric.</li>
  <li>Guys with experience will not have so many benefits from this feature.</li>
  <li>It is compatible with previous manual configuration of polices.</li>
  <li>Not recommended for brown field deployments, I would like to recommend only for green field deployment.</li>
</ul>

<p><strong><span style="color:#074080">5.Weight-Based Symmetric PBR</span></strong></p>

<p>In 6.0 it was introduced Weight-based PBR it means that we can send more traffic to specific node.
Use case:<br />
•	On specific node we have connected Load Balancer.<br />
•	On specific node we have more powerful firewall in the group.<br />
•	Introducing new firewall, we can redirect small amount of traffic to new firewall for test purposes.</p>

<p><strong><span style="color:#074080">6.Stretched L3out SVI across remote leaves.</span></strong></p>

<ul>
  <li>Supported for stretching a non-vPC L3Out SVI across remote leaf switches
Use cases:</li>
  <li>Mobile packet core.</li>
  <li>Containerized workloads special at the Edge.</li>
  <li>Cloud native network functions CNF.</li>
</ul>

<p><strong><span style="color:#074080">7. Multi-Site support for vzAny PBR and L3Out-to-L3Out PBR use cases.</span></strong>
From ACI 6.0(4)F and NDO 4.2(3) following uses cases are supported:</p>
<ul>
  <li>vzAny-to-vzAny (Intra VRF).</li>
  <li>vzAny-to-EPG (Intra VRF).</li>
  <li>vzAny-to-L3out (Intra VRF).</li>
  <li>L3out-to-L3out ((Intra VRF) and inter-VRF).</li>
  <li>Standard EPG-to-EPG subnet under consumer still is required.
If you have plan to implement vzAny inside multi-site design, please check all details on link below:
<a href="https://www.cisco.com/c/en/us/td/docs/dcn/ndo/4x/configuration/cisco-nexus-dashboard-orchestrator-configuration-guide-aci-421/ndo-configuration-aci-use-case-vzany-pbr-42x.html" target="_blank">Cisco Nexus Dashboard Orchestrator Configuration Guide for ACI Fabrics</a>.</li>
</ul>

<p><strong><span style="color:#074080">8. Main Routing improvements Multi-Site support for vzAny PBR and L3Out-to-L3Out PBR use cases.</span></strong></p>

<ul>
  <li>BGP AS - Remove private AS from AS_PATH.</li>
  <li>Support for BFD on Secondary subnet IPv4/IPv6.</li>
  <li>BGP Additional Paths - Receive multiple paths for the same prefix without the new paths replacing any previous paths.</li>
</ul>

<p><strong><span style="color:#074080">9. Operating and Administering improvements.</span></strong></p>

<p>ACI ver.6.0(1)F</p>
<ul>
  <li>Simplifying interface configuration.</li>
  <li>Secure Erase (RMA).</li>
</ul>

<p>ACI ver.6.0(2)F</p>
<ul>
  <li>Auto firmware update for Cisco APIC on discovery - APIC will be automatically upgrade on version same as fabric.</li>
  <li>Installing switch SMU without reload.</li>
  <li>Troubleshooting of QoS Polices.</li>
</ul>

<p>ACI ver.6.0(3)F</p>
<ul>
  <li>Reduce of time of APIC upgrade.</li>
  <li>Transaction-based Logging.</li>
  <li>Memory-based Automatic Switch Image Selection 32/64 bit.</li>
</ul>

<p>ACI ver.6.0(4)F</p>
<ul>
  <li>Streamlined navigation and editing of the stat collection threshold from a TCA fault.</li>
</ul>

<p><strong><span style="color:#074080">10. Main Security improvements</span></strong></p>

<ul>
  <li>Cisco Nexus 9K switch secure erase.</li>
  <li>Support for a user group map rule for SAML and OAuth2.</li>
  <li>Support for TLS ver 1.3</li>
  <li>First hop security FHS support for VMM.</li>
  <li>TACACS external logging for switches.</li>
  <li>RSA Key 307 and 4096 support on X.509 certificates.</li>
</ul>

<p><strong><span style="color:#074080">11. Integrations.</span></strong></p>

<ul>
  <li>Enhanced LLDP TLVs for Azure Stack HCI.</li>
  <li>Policy mode for Cisco ACI and VMware NSX-T integration.</li>
  <li>VMM Domain support for VMware vSphere 8.0.</li>
</ul>

<p><strong><span style="color:#074080">12. Scale improvements.</span></strong></p>

<p align="center">
<img src="/images/aci_6_new_features/3.png" alt="ACI 60" title="ACI 60" /> 
</p>

<p align="center">
<img src="/images/aci_6_new_features/4.png" alt="ACI 60" title="ACI 60" /> 
</p>

<p align="center">
<img src="/images/aci_6_new_features/5.png" alt="ACI 60" title="ACI 60" /> 
</p>

<p>References: <br />
<a href="https://www.cisco.com/c/en/us/td/docs/dcn/aci/apic/6x/release-notes/cisco-apic-release-notes-611.html" target="_blank">Cisco Application Policy Infrastructure Controller</a> <br />
<a href="https://www.cisco.com/c/en/us/td/docs/dcn/aci/apic/all/cisco-aci-releases-changes-in-behavior.html" target="_blank">Cisco ACI Releases Changes in Behavior</a></p>]]></content><author><name></name></author><category term="Cisco ACI" /><summary type="html"><![CDATA[The Cisco ACI 6.0 release has been available for a couple of months now. Let’s check out what it brings to network engineers and IT professionals. Experience a new era of enhanced performance, security, and automation capabilities. In this blog, we’ll dive into the key enhancements of Cisco ACI 6.0 and explore how they can empower your network infrastructure.]]></summary></entry><entry><title type="html">Mastering Cisco ACI Physical Design</title><link href="http://0.0.0.0:4000/cisco%20aci/physical-design/" rel="alternate" type="text/html" title="Mastering Cisco ACI Physical Design" /><published>2024-07-07T00:00:00+00:00</published><updated>2024-07-07T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cisco%20aci/physical-design</id><content type="html" xml:base="http://0.0.0.0:4000/cisco%20aci/physical-design/"><![CDATA[<p align="center">
<img src="/images/phy_design/2.png" alt="Switch SFP Cables" title="ACI Phy Design" /> 
</p>
<p style="text-align: justify;">
Let's dive into the world of Cisco ACI physical design. After all, we all know that without hardware, there is no cloud or networking.
</p>

<p style="text-align: justify;">
Dive into the world of Clos Topology, a concept born from a 1952 paper by Charles Clos, a researcher at Bell Laboratories. His revolutionary ideas on multistage telephone switching systems have transcended time, now serving as the foundation for building highly scalable data centers with cost-effective switches.
The Cisco ACI fabric is based on either a two-tier (spine and leaf switch) or three-tier (spine switch, tier-1 leaf switch, and tier-2 leaf switch) architecture.
In a Cisco Application Centric Infrastructure (ACI), each of the leaf, Fabric Extenders and spine switches has different functions.
</p>

<p><strong><span style="color:#074080">Leaf Switches</span></strong></p>

<ul>
  <li>Leaf switches are the ingress/egress points for traffic into and out of an ACI fabric.</li>
  <li>They connect directly to servers and other devices, facilitating rapid data transfer.</li>
  <li>Each leaf switch is linked to every spine switch, creating a mesh of high-bandwidth connections.</li>
  <li>This setup ensures that any data packet is only a couple of hops away from its destination, significantly reducing latency.</li>
</ul>

<p><strong><span style="color:#074080">Fabric Extenders (FEX)</span></strong></p>

<p>Fabric Extenders (FEX) in Cisco’s Application Centric Infrastructure (ACI) have several key functions:</p>
<ul>
  <li>Port Extension - FEX extends the ports of a Nexus switch, acting as a remote line card. This allows for the use of more cost-effective devices with a reduced set of features that are connected to the Leaf Switches.</li>
  <li>Agile Connectivity - FEX offers agile connectivity for rack and blade servers and for converged fabric deployments. It delivers innovation to the data center, reduces total cost of ownership, and gains architectural flexibility.</li>
  <li>Configuration and Management - All management is done on the parent switch; there is no console/vty access on the FEX. This simplifies the management of the network infrastructure.</li>
  <li>Cisco ACI leaf switch models with -G or -Tin the product name, such as Cisco Nexus N9K-C9348GC-FXP, N9K-C93108TC-FX, N9K-C93108TC-FX-24, N9K-C93108TC-EX, N9K-C93108TC-EX-24, N9K-C93216TC-FX2, and N9K-93108TC-FX3P.</li>
</ul>

<p>Discover the steps to configure Fabric Extenders (FEX) in Cisco ACI in Cisco comprehensive guide available <a href="https://www.cisco.com/c/en/us/support/docs/cloud-systems-management/application-policy-infrastructure-controller-apic/200529-Configure-a-Fabric-Extender-with-Applica.html" target="_blank">here</a>.</p>

<p><strong><span style="color:#074080">Spine Switches</span></strong></p>

<p>Spine switches are Clos intermediary switches that have a number of key functions:</p>

<ul>
  <li>They are responsible for high-capacity data transmission across the network.</li>
  <li>They ensure that data packets travel efficiently from one leaf switch to another.</li>
  <li>The spine layer is crucial for maintaining the performance of the network as it scales, providing a robust framework that can handle increasing data demands without bottlenecks.</li>
  <li>Spine switches interconnect all leaf switches in a full-mesh topology.</li>
</ul>

<p>Together, they create an architecture that is highly standardized across deployments. This design allows for a highly scalable and efficient network infrastructure.</p>

<h5 id="two-tier-or-multi-tier-design"><strong><span style="color:#074080">Two-Tier or Multi-Tier Design?</span></strong></h5>

<p style="text-align: justify;">
Below Figure 1 illustrates the necessary components and cabling for an two-tier ACI fabric. It’s important to note that no cables should be connected between ACI leaf switches, as this is characteristic of Clos network design. Similarly, cross-cabling ACI spines will lead to the disabling of the cross-connected ports. The topology depicts a full mesh of cabling between the spine and leaf layers. 
However, it’s important to note that while the Spine-Leaf architecture can be implemented without ACI, with the switches working in NX-OS mode. I would like to highlight that ASICs are different for Leaf and Spine switches, so a switch like the Nexus N9K-93180YC-EX cannot run as Spine switches.  
</p>

<p align="center">
<img class="center" style="float" width="350" height="350" src="/images/phy_design/aci-PhysicalDesign.jpg" alt="phy-design" title="PHY" />  
</p>

<p><strong>Figure 1</strong> Two-Tier Topology Spine-Leaf classic interconnect between leaf and spine switches</p>

<p>In summary, I would like to recommend to check switch compatibility on the links below:</p>

<ul>
<li><a href="https://www.cisco.com/c/dam/en/us/products/switches/nexus-9000-series-switches/nexus-9300-10GbaseT-switches-comparison.html" target="_blank">Cisco Nexus 9300 1/10GBaseT Switches</a></li>
<li><a href="https://www.cisco.com/c/en/us/products/switches/nexus-9000-series-switches/nexus-9300-10ge-fiber-switches-comparison.html" target="_blank">Cisco Nexus 9300 1/10/25/40/50/100GE Fiber Switches</a></li>
<li><a href="https://www.cisco.com/c/dam/en/us/products/switches/nexus-9000-series-switches/nexus-9300-40GE-switches-comparison.html" target="_blank">Cisco Nexus 9300 40/100 GE Switches</a></li>
<li><a href="https://www.cisco.com/c/en/us/products/switches/nexus-9000-series-switches/nexus-9300-400-ge-switches.html" target="_blank">Cisco Nexus 9300 400 GE Switches</a></li>
<li><a href="https://www.cisco.com/c/en/us/products/collateral/switches/nexus-9000-series-switches/datasheet-c78-731792.html" target="_blank"> Cisco Nexus 9300 ACI Fixed Spine Switches Data Sheet</a></li>
</ul>

<p>If you plan to use Cisco Nexus modular switches for Cisco ACI fabric technical details you can find on below links:</p>

<ul>
  <li><a href="https://www.cisco.com/c/en/us/products/collateral/switches/nexus-9000-series-switches/datasheet-c78-729404.html" target="_blank"> Cisco Nexus 9500 Series Switches Data Sheet</a></li>
  <li><a href="https://www.cisco.com/c/en/us/products/switches/nexus-9000-series-switches/nexus-9500-chassis-comparison.html" target="_blank"> Nexus 9500 Chassis Comparison</a></li>
  <li><a href="https://www.cisco.com/c/en/us/products/switches/nexus-9000-series-switches/nexus-9800-models-comparison.html" target="_blank"> Nexus 9800 Compare Models</a></li>
  <li><a href="https://www.cisco.com/c/en/us/products/switches/nexus-9000-series-switches/nexus-9800-line-cards-comparison.html" target="_blank"> Nexus 9800 Line Cards comparison</a></li>
</ul>

<p style="text-align: justify;">
Starting from Cisco ACI 4.1, the fabric also supports a multi-tier (three-tiers) topology. This includes two tiers of leaf switches, allowing for vertical expansion of the Cisco ACI fabric. This is particularly useful for migrating a traditional three-tier architecture (core-aggregation-access), a common design model for many enterprise networks.
The ACI Multi-Tier is a beneficial solution for data centers aiming to reduce inter-row cabling and have low-bandwidth needs for top-of-rack switches. In this setup, Tier 1 leaf switches are placed at the end of each row, while Tier 2 leaf switches are positioned at the top of each rack. This strategy is particularly effective in managing cables and meeting bandwidth requirements.
</p>

<p align="center">
<img class="center" style="float" width="450" height="400" src="/images/phy_design/aci-PhysicalDesign 2.jpg" alt="phy-design" title="PHY" />  
</p>

<p><strong>Figure 2</strong> Multi -Tier Topology Spine- Tier 1 Leaf and Tier 2 Leaf</p>

<p align="center">
<img class="center" style="float" width="400" height="300" src="/images/phy_design/aci-Sec PhysicalDesign 2.jpg" alt="phy-design" title="PHY" />  
</p>

<p><strong>Figure 3</strong> Multi -Tier Topology Spine- Tier 1 Leaf and Tier 2 Leaf option between two data center buildings</p>

<p>The topology requirements may include various models of spine and leaf switches, please check below:</p>

<ul>
  <li>
    <p><strong>Spine:</strong> EX/FX/C/GX/GX2 spines (For example, Cisco Nexus 9332C, 9316D-GX, 9348D-GX2A, 9408 and 9500 with EX/FX/GX linecards).</p>
  </li>
  <li>
    <p><strong>Tier-1</strong> Leaf: EX/FX/FX2/FX3/GX/GX2 except Cisco Nexus 93180LC-EX.</p>
  </li>
  <li>
    <p><strong>Tier-2</strong> All EX/FX/FX2/FX3/GX/GX2.</p>
  </li>
</ul>

<p>In the scenario where server access is at 1G and there is not need for high bandwidth, it’s a viable choice to use the Cisco Nexus 93180FX3 or Nexus 9364C-GX as the tier-1 leaf and the Cisco Nexus 9348FXP as the tier-2 leaf.</p>

<p>Before you start to build and design a multi-tier topology in a Cisco ACI environment, please take into account the following:</p>

<p><strong><span style="color:#074080">1.Fabric Port Configuration:</span></strong> All inter-switch connections must be configured as fabric ports. This includes the connections between Tier-2 leaf switch fabric ports and Tier-1 leaf switch fabric ports.</p>

<p><strong><span style="color:#074080">2.Tier-2 Leaf Switch Connectivity:</span></strong> A Tier-2 leaf switch has the capability to connect to more than two Tier-1 leaf switches. This is in contrast to a traditional double-sided vPC design, which only supports two upstream switches. The maximum number of ECMP links supported by a Tier-2 leaf switch to a Tier-1 leaf switch is 18.</p>

<p><strong><span style="color:#074080">3.Endpoint Group (EPG) and Layer 3 Outside (L3Out) Connections:</span></strong> EPGs, L3Outs, Cisco APICs, or Fabric Extenders (FEX) can be connected to either Tier-1 or Tier-2 leaf switches.</p>

<p><strong><span style="color:#074080">4.Tier-1 Leaf Switch Connections:</span></strong> Tier-1 leaf switches can have both hosts and Tier-2 leaf switches connected to them.</p>

<p><strong><span style="color:#074080">5.Switch Tier Transition:</span></strong> Transitioning from a Tier-1 to a Tier-2 leaf switch (and vice versa) requires decommissioning and recommissioning the switch.</p>

<p><strong><span style="color:#074080">6.Compatibility with Cisco ACI Multi-Pod and Multi-Site:</span></strong> Multi-tier architectures are compatible with both Cisco ACI Multi-Pod and Cisco ACI Multi-Site.</p>

<p><strong><span style="color:#074080">7.Tier-2 Leaf Switch Connectivity Limitations:</span></strong> Tier-2 leaf switches cannot be connected to remote leaf switches (Tier-1 leaf switches).</p>

<p><strong><span style="color:#074080">8.Scale Considerations:</span></strong> The combined number of Tier-1 and Tier-2 leaf switches must not exceed the maximum number of leaf switches validated for a given release (400 per pod; 500 per Cisco ACI Multi-Pod as of Cisco ACI release 6.0(1)).</p>

<p>Reference for <a href="https://www.cisco.com/c/en/us/products/switches/nexus-9000-series-switches/nexus-9800-line-cards-comparison.html" target="_blank"> Cisco ACI Multi-tier Architecture</a>.</p>

<p><strong><em><span style="color:#074080">Fabric Port</span></em></strong></p>

<p style="text-align: justify;">
In Cisco's Application Centric Infrastructure (ACI), a fabric port is a type of port that is used for uplink connectivity within the fabric. These ports are typically used to connect different switches within the ACI fabric.
For example, in a multi-tier topology, all switch-to-switch links must be configured as fabric ports. This includes the connections between Tier-2 leaf switch fabric ports and Tier-1 leaf switch fabric ports.
</p>

<p><strong><span style="color:#074080">Fiber optics modules</span></strong></p>

<p style="text-align: justify;">  
Don't forget that based on specific switch model and uplinks design you will need to choose proper optical modules.
All details about compatibility matrix for Cisco Optics and devices you can find on below links:
</p>
<ul>
  <li><a href="https://tmgmatrix.cisco.com/">Cisco Optics-to-Device Compatibility Matrix</a></li>
  <li><a href="https://tmgmatrix.cisco.com/iop">Cisco Optics-to-Optics Interoperability Matrix</a></li>
</ul>
<p style="text-align: justify;"> 
If you struggling or you have some doubts about compatibility between switches and optical modules SFP SFP+ QSFPs I would like to recommend:
</p>

<p>
<iframe width="420" height="315" src="https://www.youtube.com/embed/5I19ePqzNhc?controls=0">
</iframe>
</p>

<p>
<iframe width="420" height="315" src="https://www.youtube.com/embed/FFhNGxmniR4?controls=0">
</iframe>  
</p>

<p><strong><span style="color:#074080">Taxonomy for Cisco Nexus 9000 Series Part Numbers</span></strong></p>

<p style="text-align: justify;">
For a comprehensive understanding and decoding of all Nexus Part Numbers names, kindly refer to Figure 4.
</p>

<p align="center">
<img class="center" style="float" width="800" height="400" src="/images/phy_design/taxonomy_nk9.png" alt="taxonomynk9" title="PHY" />  
</p>

<p><strong>Figure 4</strong> Cheet-sheet for decoding Cisco Nexus 9000 Series Part Numbers</p>]]></content><author><name></name></author><category term="Cisco ACI" /><summary type="html"><![CDATA[Let's dive into the world of Cisco ACI physical design. After all, we all know that without hardware, there is no cloud or networking.]]></summary></entry><entry><title type="html">Cisco ACI Fabric - APIC Day 1 configuration</title><link href="http://0.0.0.0:4000/cisco%20aci/aci-apic-initial-setup/" rel="alternate" type="text/html" title="Cisco ACI Fabric - APIC Day 1 configuration" /><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cisco%20aci/aci-apic-initial-setup</id><content type="html" xml:base="http://0.0.0.0:4000/cisco%20aci/aci-apic-initial-setup/"><![CDATA[<p align="center">
<img src="/images/apic_day_0_config/apic.jpg" alt="Cisco ACI APIC" title="Cisco APIC interconnects" />  
</p>
<p style="text-align: justify;">
This time I will go through configuration steps for provisioning switches on Cisco ACI Fabric, NTP, DNS and BGP Route Reflectors. Steps described in this post are applicable for Cisco ACI Fabric and Cisco ACI simulator. 
</p>

<p><strong>0.</strong> In your favorite browser address bar add <strong><em>https://oob.ip.address</em></strong>. Use IP address what you set during APIC initial configuration.</p>

<p><img class="center" width="800" height="400" src="/images/acisim_init_conf/login_screen2.jpg" alt="APIC Login" title="APIC Login" /></p>

<p><strong>Cisco ACI Fabric APIC ver 5.2 supported browsers:</strong></p>
<ul>
  <li>Chrome version 59 (at minimum)</li>
  <li>Firefox version 54 (at minimum)</li>
  <li>Internet Explorer version 11 (at minimum)</li>
  <li>Safari version 10 (at minimum)</li>
</ul>

<p><strong>1.</strong> After what you will login it will appear Cisco APIC welcome screen, looks nice. Just press <strong>Let’s go</strong> as this white button suggesting.
<img class="center" width="800" height="400" src="/images/apic_day_0_config/1.jpg" alt="APIC Welcome Screen" title="APIC Welcome Screen" /></p>

<p><strong>2.</strong> One more screen from APIC this one is important as it will provided you information about:</p>
<ul>
  <li>New features for installed APIC version of APIC.</li>
  <li>Updated scalability numbers.</li>
  <li>Very good resources and knowledge base for ACI fabric and APIC clusters.<br />
After what you reviewed all interesting things pres <strong>Start ACI Fabric Setup</strong> button.</li>
</ul>

<p><img class="center" width="800" height="470" src="/images/apic_day_0_config/2.jpg" alt="APIC Release notes" title="APIC Release notes" /></p>

<p><strong>3.</strong> Below we can see the most important sections which should be configured before any service configuration on ACI.</p>

<p>I would like to suggest following order:</p>
<ol>
  <li>Add leaf and spine to Cisco ACI Fabric.</li>
  <li>NTP server configuration.</li>
  <li>DNS server configuration.</li>
  <li>BGP RR configuration for Spine.</li>
</ol>

<p><img class="center" width="800" height="450" src="/images/apic_day_0_config/3.jpg" alt="APIC Basic service setup" title="APIC Basic service setup" /></p>

<p><strong>4.</strong> Add first leaf to Cisco ACI Fabric, for serial number we will use TEP-1-102, 101 and 103 as this is Cisco ACI simulator on real Cisco ACI Fabric you will have serial numbers in following format F1234A768. For node ID choose unique number, once when you tied node ID to serial number you will not be able to change on the fly. If you would like to change you will need to do decommissioning and commissioning with new node ID. Switch name you can change anytime if you will not like assigned during initial switch provisioning.</p>

<p><img class="center" width="600" height="s 200" src="/images/apic_day_0_config/4.jpg" alt="APIC spine leaf membership" title="APIC spine leaf membership" /></p>

<p><strong>5.</strong> Here you should pay attention  and check is it switch in <strong>Registering</strong> phase, if you notice some other messages please stop and perform tshoot.</p>

<p><img class="center" width="800" height="400" src="/images/apic_day_0_config/5.jpg" alt="APIC add leaf to membership" title="APIC add leaf to membership" /></p>

<p><strong>6.</strong> Once when is done we should see that switch is successfully <strong>Register</strong> as we can see below for leaf with node ID 101.</p>

<p><img class="center" width="800" height="350" src="/images/apic_day_0_config/6.jpg" alt="APIC add leaf to membership" title="APIC add leaf to membership" /></p>

<p><strong>7.</strong> Same you should repeat for all Spine switches too, below you can see <strong>Registering</strong> of spine with node ID 103.</p>

<p><img class="center" width="800" height="350" src="/images/apic_day_0_config/7.jpg" alt="APIC add spine to membership" title="APIC add spine to membership" /></p>

<p><strong>8.</strong> And Spine with node ID 103 has been successfully <strong>Register</strong>.</p>

<p><img class="center" width="800" height="350" src="/images/apic_day_0_config/8.jpg" alt="APIC add spine to membership" title="APIC add spine to membership" /></p>

<p><strong>9.</strong> Based on workflow from point #3 I will proceed with NTP configuration. You can choose time format from <strong>local</strong> or <strong>utc</strong>. Use <strong>plus</strong> sign for adding one or more NTP servers, one of them you can set like preferable.</p>

<p><img class="center" width="800" height="400" src="/images/apic_day_0_config/9.jpg" alt="APIC NTP Configuration" title="APIC NTP Configuration" /></p>

<p><strong>10.</strong> In the lab for NTP server I use public Google NTP server just for test.
<img class="center" width="800" height="450" src="/images/apic_day_0_config/10.jpg" alt="APIC NTP Configuration" title="APIC NTP Configuration" /></p>

<p><strong>11.</strong> Here I will add Google DNS server just for test. Usually you will use internal DNS. Don’t forget to add DNS records for leaf, spines and APICs.</p>

<p><img class="center" width="800" height="400" src="/images/apic_day_0_config/11.jpg" alt="APIC DNS Configuration" title="APIC DNS Configuration" /></p>

<p><strong>12.</strong> Here we can check status about what we done already. Below you can notice that still we have to configure BGP route reflectors.  To configure BGP just press button <strong>Begin</strong> which will redirect you to configuration page.</p>

<p><img class="center" width="800" height="430" src="/images/apic_day_0_config/12.jpg" alt="APIC setup overview" title="APIC setup overview" /></p>

<p><strong>13.</strong> Press <strong>plus</strong> sign and add minimu two spines per Cisco ACI fabric site.</p>

<p><img class="center" width="800" height="400" src="/images/apic_day_0_config/13.jpg" alt="APIC BGP RR config" title="APIC BGP RR config" /></p>

<p><strong>14.</strong> From drop down choose node id of your spine. As I am using Cisco ACI Simulator I have only one spine with node ID 103. For test I am using default AS number.</p>

<p><img class="center" width="600" height="250" src="/images/apic_day_0_config/14.jpg" alt="APIC BGP RR config" title="APIC BGP RR config" /></p>

<p><strong>15.</strong> Below you can see that spine with node ID 103 was successful registered like RR in Pod 1.</p>

<p><img class="center" width="800" height="300" src="/images/apic_day_0_config/15.jpg" alt="APIC BGP RR config" title="APIC BGP RR config" /></p>

<p><strong>16.</strong> Short overview all looks good as Proxy won’t be used.</p>

<p><img class="center" width="800" height="400" src="/images/apic_day_0_config/16.jpg" alt="APIC setup overview" title="APIC setup overview" /></p>

<p><strong>17.</strong> Summary page, last step for configuration setup.
<img class="center" width="800" height="430" src="/images/apic_day_0_config/17.jpg" alt="APIC setup summary" title="APIC setup summary" /></p>

<p><strong>18.</strong> I would like to recommend you to check <strong>Dashboard</strong> sections like below on print screen</p>
<ul>
  <li>Switches should be <strong>Healthy</strong>.</li>
  <li>Controllers should be <strong>In service</strong> and <strong>FullyFit</strong>.</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Note
This is only zero day configuration for Cisco APIC. For MultiPod, Multisite, 
Remote Leaf and other Cisco ACI Fabric designs should be configured other parameters
and network objects.
</code></pre></div></div>
<p><img class="center" width="800" height="400" src="/images/apic_day_0_config/24.jpg" alt="" title="" /><br />
<br />
<br />
<strong>Bonus steps</strong> for those how like to have clean ACI fabric without lot of system notifications.<br />
Next few steps will describe how we can disable  messages from Cisco Smart Software Manage, this is mostly applicable for lab deployments and ACI Simulator as after 90 days should be reconfigured.</p>

<p><strong>19.</strong> From APIC notification center please choose Cisco Smart Software Manager (CSSM).</p>

<p><img class="center" width="800" height="400" src="/images/apic_day_0_config/18.jpg" alt="APIC notifications" title="APIC notification" /></p>

<p><strong>20.</strong> Press on fault notification F3057.</p>

<p><img class="center" width="800" height="400" src="/images/apic_day_0_config/19.jpg" alt="APIC notification" title="APIC notification" /></p>

<p><strong>21.</strong> As below on print screen click on <strong>Configure Network Settings</strong>.</p>

<p><img class="center" width="800" height="350" src="/images/apic_day_0_config/20.jpg" alt="APIC notification" title="APIC notification" /></p>

<p><strong>22.</strong> From drop down menu choose <strong>Offline</strong> click on OK and you are ready.</p>

<p><img class="center" width="500" height="400" src="/images/apic_day_0_config/21.jpg" alt="APIC notification" title="APIC notification" /></p>

<p>I have walked through most important parameters for Cisco ACI Fabric. Now you can start to play with Cisco ACI Fabric network objects or you can proceed with fine tunning for the production environment. 😎</p>]]></content><author><name></name></author><category term="Cisco ACI" /><summary type="html"><![CDATA[This time I will go through configuration steps for provisioning switches on Cisco ACI Fabric, NTP, DNS and BGP Route Reflectors. Steps described in this post are applicable for Cisco ACI Fabric and Cisco ACI simulator.]]></summary></entry><entry><title type="html">Cisco ACI Simulator APIC Initial setup</title><link href="http://0.0.0.0:4000/cisco%20aci/acisim-init-conf/" rel="alternate" type="text/html" title="Cisco ACI Simulator APIC Initial setup" /><published>2022-11-10T00:00:00+00:00</published><updated>2022-11-10T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cisco%20aci/acisim-init-conf</id><content type="html" xml:base="http://0.0.0.0:4000/cisco%20aci/acisim-init-conf/"><![CDATA[<p align="center">
<img src="/images/acisim_init_conf/subject.jpg" alt="Cisco ACI Simulator" title="Cisco ACI Initial Configuration" />  
</p>
<p style="text-align: justify;">
If you already read my last post <a href="/aci/acisim-on-kvm/index.html" target="_blank">How to run Cisco ACI Simulator on KVM host</a> you know that now we need to do first step and go through Cisco APIC initial configuration setup. If you need help this post is right place for you. So let' go and see what we need to do.
</p>

<p><img class="center" style="float" width="400" height="400" src="/images/acisim_init_conf/net_diagram.jpg" alt="ACISIM Diagram" title="ACISIM internal connectivity" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Note:
Cisco ACI simulator doesn't support data plane and you won't be able to connect any  
other VM which can represent End Point. Simple you will not be able to issue ping  
command and send ICMP request and see any ICMP reply or use any other protocol for testing purpose.
</code></pre></div></div>

<p>1.VM is booting
<img class="center" width="800" height="400" src="/images/acisim_init_conf/1.jpg" alt="acisim booting" title="acisim booting" /></p>

<p>2.If you have recommended HW resource use large topology otherwise your choice is “N”.
<img class="center" width="800" height="280" src="/images/acisim_init_conf/2.jpg" alt="Acisim topology size" title="Acisim topology size" /></p>

<p>3.After few seconds it will appear this nice logo.
<img class="center" width="800" height="280" src="/images/acisim_init_conf/3.jpg" alt="APIC Welcome screen" title="APIC Welcome screen" /></p>

<p>4.For ACISIM I would like to recommend to accept all default parameters. All parameters you can change but in that case there is no guaranteed that Cisco ACI simulator will work properly.
<img class="center" width="800" height="s 350" src="/images/acisim_init_conf/7.jpg" alt="APIC ACISIM setup" title="APIC ACISIM setup" /></p>

<p>5.One of parameters what actually you should change is default OOB subnet and gateway. Choose IP address/Subnet mask from your OOB lab network and set in this step. Later you will use this IP address for access APIC UI via HTTPS.
<img class="center" width="600" height="100" src="/images/acisim_init_conf/8.jpg" alt="APIC ACISIM OOB" title="APIC ACISIM OOB" /></p>

<p>6.Please set admin password and don’t forget, later you will need for login to APIC UI 😉
<img class="center" width="600" height="100" src="/images/acisim_init_conf/9.jpg" alt="APIC ACISIM admin pass" title="APIC ACISIM admin pass" /></p>

<p>7.Here you have chance to change any configuration parameter what you set during initial configuration.
If you are self confident and you put password on the sticky note on your desk you just press “ENTER”
<img class="center" width="600" height="120" src="/images/acisim_init_conf/10.jpg" alt="APIC ACISIM modification" title="APIC ACISIM modification" /></p>

<p>8.In your favorite browser type https://oob.ip.address what you set during initial configuration.
When APIC login page appear for User ID use <strong>admin</strong> and for password use note from your sticky note from your the desk.
<img class="center" width="800" height="400" src="/images/acisim_init_conf/login_screen1.jpg" alt="Login" title="Login" /></p>

<p>Final Part 2 coming soon! Are you enjoyed reading? If yes please subscribe <strong><a href="/subscribe/index.html" target="_blank">Here</a></strong></p>

<head>
<style>
a:link {
  text-decoration: underline;
}

a:visited {
  text-decoration: none;
}

a:hover {
  text-decoration: none;
}

a:active {
  text-decoration: none;
}
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
</style>
</head>]]></content><author><name></name></author><category term="Cisco ACI" /><summary type="html"><![CDATA[If you already read my last post How to run Cisco ACI Simulator on KVM host you know that now we need to do first step and go through Cisco APIC initial configuration setup. If you need help this post is right place for you. So let' go and see what we need to do.]]></summary></entry><entry><title type="html">How to run Cisco ACI Simulator on KVM host</title><link href="http://0.0.0.0:4000/cisco%20aci/acisim-on-kvm/" rel="alternate" type="text/html" title="How to run Cisco ACI Simulator on KVM host" /><published>2022-06-13T00:00:00+00:00</published><updated>2022-06-13T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cisco%20aci/acisim-on-kvm</id><content type="html" xml:base="http://0.0.0.0:4000/cisco%20aci/acisim-on-kvm/"><![CDATA[<p align="center">
<img src="/images/acisim_on_kvm/subject.jpg" alt="Cisco ACI Simulator" title="Cisco ACI Simulator Spine leaf and APIC" />  
</p>
<p style="text-align: justify;">
As officially ACI Simulator is not supported on KVM host and I didn't have ESXi in my lab I have decided to do experiment. 
Moreover I have played with amount of hardware resource necessary for this experiment. If you would like to know result of this experiment, you can proceed and read rest of the post.
</p>

<p style="text-align: justify;">
At the beginning lets say couple of words about Cisco ACI Simulator or ACISIM. So this is Cisco virtual machine, inside which we can find simulation of two leaf switches, one spine and one Cisco Application Policy Infrastructure Controller (APIC). Actually this is simulation of Data Center leaf spine topology with SDN controller. 
</p>

<p><strong><span style="color:#074080">Official Cisco ACI Simulator Virtual Machine HW requirements:</span></strong></p>

<p><img style="center" src="/images/acisim_on_kvm/hw_resurces.jpg" alt="HW requirements" title="Official HW requirements" /></p>
<p style="text-align: justify;">
To make things more interesting I have reduced HW resources just from the beginning of experiment.&#128521;
</p>

<p><strong><span style="color:#074080">And I created setup as below:</span></strong></p>

<p><img style="float" width="350" height="100" src="/images/acisim_on_kvm/hw_resurces2.jpg" alt="HW requirements" title="HW requirements tested in the lab" /></p>

<p style="text-align: justify;">
So, are you ready to spin ACI Simulator with less than minimum HW resource for release 5.2(4d)? If yes, let's go and see what will happen in the end.&#128526;
</p>
<p><strong><span style="color:#074080">Step 1. Download ACI Simulator</span></strong></p>

<p>Go to <a href="https://software.cisco.com/download/home/286283149/type/286283168/release/5.2(4d)" target="_blank">Cisco Download Page</a>, please don’t forget that you should have valid Cisco account, otherwise no luck.</p>

<p>Download all <strong>eight</strong> pieces:</p>
<ul>
<li>acisim-5.2-4d_part1.ova</li>
<li>acisim-5.2-4d_part2.ova</li>
<li>acisim-5.2-4d_part3.ova</li>
<li>acisim-5.2-4d_part4.ova</li>
<li>acisim-5.2-4d_part5.ova</li>
<li>acisim-5.2-4d_part6.ova</li>
<li>acisim-5.2-4d_part7.ova</li>
<li>acisim-5.2-4d_part8.ova</li>
</ul>
<p><strong><span style="color:#074080">Step 2. Concatenate downloaded parts and create QCOW2 image from VMDK</span></strong></p>

<p>Use Linux <strong>cat</strong> command to concatenate all 8 parts.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat acisim-5.2-4d_part1.ova \ 
    acisim-5.2-4d_part2.ova \
    acisim-5.2-4d_part3.ova \
    acisim-5.2-4d_part4.ova \
    acisim-5.2-4d_part5.ova \
    acisim-5.2-4d_part6.ova \
    acisim-5.2-4d_part7.ova \
    acisim-5.2-4d_part8.ova &gt; acisim.ova
</code></pre></div></div>

<p>Optional check MD5 of acisim.ova VM once when concatenations is done.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>md5sum -c aci.md5
</code></pre></div></div>

<p>Now we have virtual machine with vmdk vHD but we need qocow2 image. So what’s next, we need to untar acisim.ova and we have to convert acisim-5.2-4d-disk1.vmdk to acisim-5.2-4d-disk1.qcow2.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tar -xvf aci.ova
</code></pre></div></div>
<p>voila finally we have acisim-5.2-4d-disk1.vmdk let’s run one more command and we will have acisim-5.2-4d-disk1.qcow2</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>qemu-img convert -O qcow2 acisim-5.2-4d-disk1.vmdk path/to/images_folder/acisim-5.2-4d-disk1.qcow2
</code></pre></div></div>

<p><strong><span style="color:#074080">Step 3. Run ACISIM VM on Ubuntu with KVM hypervisor</span></strong></p>

<p style="text-align: justify;">
Run virt-manger application from desktop UI or If you are using headless server first ssh to server (don't forget to enable x11 forwarding).
</p>
<ul>
<li>Once when you run <b>virt-manager command</b> it will appear creation VM setup dialog</li>
</ul>

<p><img style="center" src="/images/acisim_on_kvm/Untitled0.jpg" alt="Virt-Manager1" title="Open VM setup dialog" /></p>

<ul>
<li>Please choose like below on print screens.</li>
</ul>

<p><img style="float" width="350" height="275" src="/images/acisim_on_kvm/Untitled.jpg" alt="Virt-Manager2" title="Create new VM" />  <img style="float" width="350" height="275" src="/images/acisim_on_kvm/Untitled 2.jpg" alt="Virt-Manager3" title="Choose qcow2 image" /></p>

<ul>
<li>Here you will need to point to folder where you saved acisim.qcow2 image.</li>
</ul>

<p><img style="float" width="350" height="275" src="/images/acisim_on_kvm/Untitled 3.jpg" alt="Virt-Manager4" title="Choose qcow2 image" /> <img style="float" width="350" height="275" src="/images/acisim_on_kvm/Untitled 4.jpg" alt="Virt-Manager5" title="Customize VM parameters" /></p>

<ul>
<li>Let use only 8 vCPUs and 32G of RAM, this is far from recommended by Cisco. </li>
</ul>

<p><img style="float" width="350" height="275" src="/images/acisim_on_kvm/Untitled 5.jpg" alt="Virt-Manager6" title="Increase vCPU" /> <img style="float" width="350" height="275" src="/images/acisim_on_kvm/Untitled 6.jpg" alt="Virt-Manager7" title="Increase Memory" /></p>

<ul>
<li>Mandatory, please select network adapter type <b>e1000</b> otherwise later you will not be able to connect to your ACISIM via HTTPS or SSH.</li>
<li>To start VM creation press "Begin installation".</li>
</ul>

<p><img style="float" width="350" height="275" src="/images/acisim_on_kvm/Untitled 7.jpg" alt="Virt-Manager8" title="Set network" /> <img style="float" width="350" height="275" src="/images/acisim_on_kvm/Untitled 9.jpg" alt="Virt-Manager9" title="Begin installation" /></p>

<ul>
<li>Between all VMs choose acisim and run.</li>
</ul>

<p><img style="center" src="/images/acisim_on_kvm/Untitled 10.jpg" alt="Virt-Manager10" title="Select VM and Run" /></p>

<ul>
<li>Press OPEN button to access acisim VM console.</li>
</ul>

<p><img style="center" src="/images/acisim_on_kvm/Untitled 11.jpg" alt="Virt-Manager11" title="Open Console" /></p>

<ul>
<li>After 2-3 min (depend about power of your host) it will appear APIC initial screen.</li>
</ul>

<p><img style="center" src="/images/acisim_on_kvm/Untitled 12.jpg" alt="Virt-Manager12" title="APIC initial config" /></p>

<ul>
<li>Below we can see that we are running Cisco ACI simulator on KVM host and successful start of initial configuration on Cisco APIC.</li>
</ul>

<p><img style="center" src="/images/acisim_on_kvm/Untitled 13.jpg" alt="Virt-Manager13" title="APIC initial config" /></p>

<p><strong><span style="color:#074080">Short Outcome:</span></strong></p>
<ul>
<li>If we have insufficient HW resource already in the lab we can reduce number of vCPUs and amount of memory
for ACISIM VM.</li> 
<li>In this Blog post I have used 8x vCPS and 32 GB of RAM below this it will not work.</li>
<li>We can use Cisco ACI simulator on KVM host, even if it is not officially supported by vendor.</li>
</ul>
<p><strong><span style="color:#074080">Enjoy your ACI simulator and happy testing!!!</span></strong></p>]]></content><author><name></name></author><category term="Cisco ACI" /><summary type="html"><![CDATA[As officially ACI Simulator is not supported on KVM host and I didn't have ESXi in my lab I have decided to do experiment. Moreover I have played with amount of hardware resource necessary for this experiment. If you would like to know result of this experiment, you can proceed and read rest of the post.]]></summary></entry><entry><title type="html">Five easy steps to configure static VxLAN Part 4</title><link href="http://0.0.0.0:4000/cumulus%20linux/static-vxlan-part4/" rel="alternate" type="text/html" title="Five easy steps to configure static VxLAN Part 4" /><published>2021-12-24T00:00:00+00:00</published><updated>2021-12-24T00:00:00+00:00</updated><id>http://0.0.0.0:4000/cumulus%20linux/static-vxlan-part4</id><content type="html" xml:base="http://0.0.0.0:4000/cumulus%20linux/static-vxlan-part4/"><![CDATA[<style>
p1 {
  font-size: 11px;
}
p2 {
  font-size: 12px;
}
</style>

<p align="center">
<img src="/images/static_vxlan/vxlan_tunnel4.jpg" alt="Overlay VxLAN Cumulus Linux" title="VxLAN between Cumulus VX and Juniper vMX" />  
</p>
<p style="text-align: justify;">
Part 4 final countdown with static VxLAN. In this post I will cover the case when we need to extend L2 domain between Data centers. According to this VxLAN tunnel will be established between two DC gateways and DC gateways will represent VTEPs.
</p>

<p style="text-align: justify;">
Data center gateways will be simulated with Cumulus Linux VX vSwitch and Juniper vMX virtual router. This time I will skip installation and configuration of the following:  
<ul>
<li>namespaces for simulation of VM1 and VM3</li>
<li>Download VPCS and configuration</li>
<li>Configuration of VxLAN tunnels from srv and srv1 to Cumulus VX switch as it will be the same like in Part 3.</li>
</ul>
All details related to the above configuration and deployment you can find in blogposts <a href="/vxlan/static-vxlan-part2/index.html">Part 2</a> and <a href="/vxlan/static-vxlan-part3/index.html">Part 3</a>.
</p>

<p align="center"> 
  <img src="/images/static_vxlan/net_diag4.jpg" alt="VxLAN, VTEP, Overlay Network" title="VxLAN Network diagram" /> 
</p>

<p><strong>Virtual router, switch and linux host machines:</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">VM name</th>
      <th style="text-align: left">OS Version</th>
      <th style="text-align: left">Extra installed SW</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">srv</td>
      <td style="text-align: left">Ubuntu 20.04</td>
      <td style="text-align: left">bridge-utils</td>
    </tr>
    <tr>
      <td style="text-align: left">srv1</td>
      <td style="text-align: left">Ubuntu 20.04</td>
      <td style="text-align: left">bridge-utils</td>
    </tr>
    <tr>
      <td style="text-align: left">virtual switch</td>
      <td style="text-align: left">Cumulus Linux 4.4.1 VX (Nvidia)</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: left">virtual router</td>
      <td style="text-align: left">Juniper vMX  JUNOS 18.4R1.8</td>
      <td style="text-align: left"> </td>
    </tr>
  </tbody>
</table>

<p><strong><span style="color:#074080">Step 1. Configure interfaces on dcgw1 and dcgw2</span></strong></p>

<p>On dcgw1 (Cumulus Linux) configure interface for OSPF peering with dc-interconnect router</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add interface swp3 ip address 172.21.100.1/30
</code></pre></div></div>
<p>add loopback which will be used for VTEP termination point.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add loopback lo ip address 192.168.1.4/32
</code></pre></div></div>
<p>Above we should repeat for dcgw2 (Juniper vMX)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set interfaces xe-0/0/4 unit 0 family inet address 172.21.101.1/30  
</code></pre></div></div>
<p>add loopback which will be used for VTEP termination point.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set interfaces lo0 unit 5 family inet address 192.168.1.5/32  
</code></pre></div></div>
<p><strong><span style="color:#074080">Step 2. Configure dc-interconnect L3 switch interfaces(Cumulus Linux VX)</span></strong></p>

<p>Add loopback</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add loopback lo ip address 2.2.2.2/32
</code></pre></div></div>
<p>Add interface for peering with dcgw1</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add interface swp3 ip address 172.21.100.2/30
</code></pre></div></div>
<p>Add interface for peering with dcgw2</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add interface swp1 ip address 172.21.101.2/30
</code></pre></div></div>
<p><strong><span style="color:#074080">Step 3. Configuration of  OSPF peering</span></strong></p>

<p><u>Apply on dcgw1.</u></p>

<p>Set router ID.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add ospf router-id 192.168.1.4
</code></pre></div></div>
<p>Add specific networks to OSPF.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add ospf network 192.168.1.4/32 area 1
net add ospf network 172.21.100.0/30 area 1
</code></pre></div></div>
<p>Change network type on link between dcgw1 and dc-interconnect.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add interface swp3 ospf network point-to-point
</code></pre></div></div>
<p>On unnecessary interfaces stop sending OSPF messages.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add ospf passive-interface swp1
net add ospf passive-interface swp2
net add ospf passive-interface swp4
net add ospf passive-interface swp5
net add ospf passive-interface swp6
net add ospf passive-interface swp7
</code></pre></div></div>
<p><u>Apply on dc-interconnect.</u></p>

<p>Set router ID.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add ospf router-id 2.2.2.2
</code></pre></div></div>
<p>Add specific networks to OSPF.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add ospf network 2.2.2.2/32 area 1
net add ospf network 172.21.100.0/30 area 1
net add ospf network 172.21.101.0/30 area 1
</code></pre></div></div>
<p>Change network type on links between dc-interconnect dcgw1 and dcgw2.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add interface swp3 ospf network point-to-point
net add interface swp1 ospf network point-to-point
</code></pre></div></div>
<p>On unnecessary interfaces stop sending OSPF messages.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add ospf passive-interface swp2
net add ospf passive-interface swp4
net add ospf passive-interface swp5
net add ospf passive-interface swp6
net add ospf passive-interface swp67
</code></pre></div></div>
<p><u>Apply on dcgw2.</u></p>

<p>Set router ID.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set routing-options router-id 192.168.1.5
</code></pre></div></div>
<p>Change network type on links between dc-interconnect and dcgw2 and enable OSPF on xe-0/0/4 (172.21.101.0/30).</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set protocols ospf area 0.0.0.1 interface xe-0/0/4.0 interface-type p2p
</code></pre></div></div>
<p>Enable OSPF on loopback</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set protocols ospf area 0.0.0.1 interface lo0.5
</code></pre></div></div>
<p>Check OSPF routes on dc-interconnect L3 switch.</p>

<p><img style="center" src="/images/static_vxlan/ospf_check_dcintcon.jpg" alt="Show OSPF routes" title="OSPF Routing" /></p>

<p><strong><span style="color:#074080">Step 4. Add static VxLAN tunnels between dcgw1 and dcgw2</span></strong><br />
<u>Apply on dcgw1.</u><br />
Create VxLAN tunnel interface.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add vxlan vni-103 vxlan id 103
</code></pre></div></div>
<p>Add vni-103 to specific bridge.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add bridge br-22 ports vni-103
</code></pre></div></div>
<p>Configure Loopback primary address for VTEP source.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add vxlan vni-103 vxlan local-tunnelip 192.168.1.4
</code></pre></div></div>
<p>Configure remote VTEP on dcgw2.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>net add vxlan vni-103 vxlan remoteip 192.168.1.5
</code></pre></div></div>
<p><u>Apply on dcgw2.</u><br />
Create bridge domain.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set bridge-domains br-vl22 vlan-id 22
set bridge-domains br-vl22 vxlan vni 103
</code></pre></div></div>
<p>Enable “ingress-node-replication” this will handle incoming broadcast, unknown unicast, or multicast (BUM) traffic.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set bridge-domains br-vl22 vxlan ingress-node-replication
</code></pre></div></div>
<p>Configure Loopback lo0.5 for VTEP source.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set switch-options vtep-source-interface lo0.5
</code></pre></div></div>
<p>Configure remote VTEP on dcgw1.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set switch-options remote-vtep-list 192.168.1.4
</code></pre></div></div>
<p><strong><span style="color:#074080">Step 5. Connectivity check </span></strong></p>

<p>Run PING command from VM1, VM3 and DB server (VPCS)
<img style="center" src="/images/static_vxlan/vm1_vm3_ping_db_part4.jpg" alt="Ping from namespace" title="Connectivity check with PING command" /></p>

<p><u>Issue ping from VM3 (namespace)</u></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo ip netns exec vm3 ping 192.168.22.4 -- &gt; remote IP on DB server.
</code></pre></div></div>
<p><img style="float" width="650" height="150" src="/images/static_vxlan/vm3_ping_db_part4.jpg" alt=" PING ICMP remote DB Server" title="VM3 Ping remote DB server" /></p>

<p><img style="float" width="750" height="250" src="/images/static_vxlan/vm3_ping_db_part4_v2.jpg" alt="L2 frame, VxLAN, wireshark, packet capture, pcap" title="Wireshark VxLAN packet capture" /></p>

<p><u>Issue ping from VM1 (namespace)</u></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo ip netns exec vm1 ping 192.168.22.4 -- &gt; remote IP on DB server.
</code></pre></div></div>
<p><img style="float" width="650" height="150" src="/images/static_vxlan/vm1_ping_db_part4.jpg" alt=" PING ICMP remote DB Server" title="VM3 Ping remote DB server" /></p>

<p><img style="float" width="750" height="250" src="/images/static_vxlan/vm1_ping_db_part4_v2.jpg" alt="L2 frame, VxLAN, wireshark, packet capture, pcap" title="Wireshark VxLAN packet capture" /></p>

<p>From the capture you can see that two VxLAN tunnels were used for interconnection with Cumulus VX virtual switch. Both tunnels landed on to one bridge on Cumulus VX virtual switch. That way we achieve communication between two VMs and DB server (VPCS).</p>

<p><u>Issue ping from DB server (VPCS) to check connectivity with VM1 and VM3</u></p>

<p><img style="center" width="750" height="250" src="/images/static_vxlan/db_srv_ping_vm3_pcap_part3.jpg" alt="Ping from VPCS" title="Connectivity check with PING command" /></p>

<p>Based on the ping and packet captures we can conclude that we have L2 connectivity between two data centers. It means that we build overlay network on top of classical L3 network connectivity.</p>

<p><br />
<strong><a href="/vxlan/static_vxlan_part1/index.html" target="_blank">Part 1 Static VxLAN between Ubuntu hosts</a></strong><br />
<strong><a href="/vxlan/static-vxlan-part2/index.html" target="_blank">Part 2 Static VxLAN between Ubuntu and Cumulus VX vSwicth</a></strong><br />
<strong><a href="/vxlan/static-vxlan-part3/index.html" target="_blank">Part 3 Static VxLAN between Ubuntu Hosts and Cumulus VX vSwitch DC Gateway</a></strong><br />
<br /></p>]]></content><author><name></name></author><category term="Cumulus Linux" /><summary type="html"><![CDATA[Part 4 final countdown with static VxLAN. In this post I will cover the case when we need to extend L2 domain between Data centers. According to this VxLAN tunnel will be established between two DC gateways and DC gateways will represent VTEPs.]]></summary></entry></feed>